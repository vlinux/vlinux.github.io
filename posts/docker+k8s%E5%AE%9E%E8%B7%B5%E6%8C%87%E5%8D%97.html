<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1"><link rel="icon" href="/favicon.svg"><script>const prefersDark=window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches,colorSchemeSetting=localStorage.getItem("vueuse-color-scheme")||"auto";("dark"===colorSchemeSetting||prefersDark&&"light"!==colorSchemeSetting)&&document.documentElement.classList.toggle("dark",!0)</script><script>const locale=localStorage.getItem("valaxy-locale")||"zh-CN";document.documentElement.setAttribute("lang",locale)</script><script type="module" async="" crossorigin="" src="/assets/app-24c8f26b.js"></script><style>@charset "UTF-8";.back-to-top{position:fixed;right:-1.5rem;bottom:1rem;z-index:var(--yun-z-go-up-btn);opacity:0;pointer-events:none;color:var(--va-c-primary);transform:translate(0) rotate(270deg);transition:transform var(--va-transition-duration),opacity var(--va-transition-duration-fast)!important}.progress-circle{transition:.3s stroke-dashoffset;transform:rotate(-90deg);transform-origin:50% 50%}.progress-circle-container{position:absolute}.menu-btn{display:inline-flex;position:fixed;left:.8rem;top:.6rem;line-height:1;z-index:var(--yun-z-menu-btn);cursor:pointer}.sidebar .links{display:flex;justify-content:center}.sidebar .link-item{display:inline-flex}.sidebar .link-item .icon{width:2rem;height:2rem}.links-of-author{display:flex;flex-wrap:wrap;justify-content:center}.links-of-author .icon{width:1.5rem;height:1.5rem}.links-of-author-item{line-height:1;font-size:.9rem}.site-nav{display:flex;justify-content:center;overflow:hidden;line-height:1.5;white-space:nowrap;text-align:center;margin-top:1rem}.site-link-item{display:flex;padding:0 15px;align-items:center;border-left:1px solid var(--va-c-gray);flex-direction:column;color:var(--va-c-text)}.site-link-item:first-child,.site-link-item:last-child{line-height:1;padding:0}.site-link-item:first-child{border-left:none;border-right:1px solid var(--va-c-gray)}.site-link-item:last-child{border-left:1px solid var(--va-c-gray)}.site-link-item:nth-child(2){border:none}.site-link-item .count{color:var(--va-c-text);font-family:var(--va-font-sans);display:block;text-align:center;font-size:1rem}.site-link-item .icon{width:1.5rem;height:1.5rem}.site-link-item .icon:hover{color:var(--va-c-primary-light)}.sidebar-panel{padding:.5rem}.site-author-avatar{display:inline-block;line-height:0;position:relative}.site-author-avatar img{height:96px;width:96px;max-width:100%;margin:0;padding:4px;background-color:#fff;box-shadow:0 0 10px #0003;transition:.4s}.site-author-avatar img:hover{box-shadow:0 0 30px rgba(var(--va-c-primary-rgb),.2)}.site-author-name{margin-top:0;margin-bottom:1rem;line-height:1.5}.site-author-status{position:absolute;height:1.8rem;width:1.8rem;bottom:0;right:0;line-height:1.8rem;border-radius:50%;box-shadow:0 1px 2px #0003;background-color:var(--va-c-bg-light);border:1px solid rgba(255,255,255,.1)}.site-name{color:var(--va-c-text);font-family:var(--va-font-serif);font-weight:900}.site-subtitle{color:var(--va-c-gray);display:block}.site-description{color:var(--va-c-text);font-size:.8rem}.va-bg{position:fixed;width:100%;height:100%;z-index:-1;background-image:var(--va-bg-img);background-size:cover;background-position:center;background-repeat:no-repeat;background-attachment:fixed;animation-name:bgFadeIn;animation-duration:2s;opacity:var(--va-bg-img-opacity,1)}@supports (-webkit-touch-callout:none){.va-bg{background-attachment:scroll}}@keyframes bgFadeIn{0%{opacity:0}to{opacity:var(--va-bg-img-opacity,1)}}canvas.fireworks{position:fixed;left:0;top:0;z-index:1;pointer-events:none}*,:after,:before{box-sizing:border-box;border-width:0;border-style:solid;border-color:#e5e7eb}html{line-height:1.5;-webkit-text-size-adjust:100%;-moz-tab-size:4;tab-size:4;font-family:ui-sans-serif,system-ui,-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,Noto Sans,sans-serif,"Apple Color Emoji","Segoe UI Emoji",Segoe UI Symbol,"Noto Color Emoji"}body{margin:0;line-height:inherit}hr{height:0;color:inherit;border-top-width:1px}h1,h2,h3,h4{font-size:inherit;font-weight:inherit}a{color:inherit;text-decoration:inherit}strong{font-weight:bolder}code,pre{font-family:ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace;font-size:1em}table{text-indent:0;border-color:inherit;border-collapse:collapse}button{font-family:inherit;font-size:100%;font-weight:inherit;line-height:inherit;color:inherit;margin:0;padding:0}button{text-transform:none}[type=button],button{-webkit-appearance:button;background-color:transparent;background-image:none}blockquote,h1,h2,h3,h4,hr,p,pre{margin:0}ol,ul{list-style:none;margin:0;padding:0}button{cursor:pointer}canvas,img,svg{display:block;vertical-align:middle}img{max-width:100%;height:auto}body{counter-reset:katexEqnNo mmlEqnNo}html{-webkit-tap-highlight-color:transparent}a{color:var(--va-c-link);font-weight:500}*{outline:0}hr{opacity:.2;margin:1rem}.va-card{background-color:var(--va-c-bg-light)}.flex-center{display:flex;justify-content:center;align-items:center}.inline-flex-center{display:inline-flex;justify-content:center;align-items:center}#app,body,html{margin:0;padding:0;line-height:2}body{background-color:var(--va-c-bg)}a{cursor:pointer}@media screen and (max-width:640px){.markdown-body div[class*=language-]{margin:0 var(--va-code-mobile-margin-x,-1rem)}}@media (min-width:640px){.markdown-body div[class*=language-]{border-radius:6px;margin:16px 0}}@media (max-width:639px){.markdown-body li div[class*=language-]{border-radius:6px 0 0 6px}}.markdown-body code{font-size:.85em}.markdown-body div[class*=language-]{position:relative;background-color:var(--va-code-block-bg);overflow-x:auto}.markdown-body div[class*=language-] code{padding:0 24px;line-height:var(--va-code-line-height);font-size:var(--va-code-font-size);color:var(--va-code-block-color);transition:color .5s;width:fit-content;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none}.markdown-body div[class*=language-] pre{position:relative;z-index:1;margin:0;padding:1rem 0;background:0 0;overflow-x:auto}.markdown-body div[class*=language-] pre code{display:block}.markdown-body [class*=language-]>span.copy{position:absolute;top:8px;right:8px;z-index:2;display:block;justify-content:center;align-items:center;border-radius:4px;width:40px;height:40px;background-color:var(--va-code-block-bg);opacity:0;cursor:pointer;background-image:var(--va-icon-copy);background-position:50%;background-size:20px;background-repeat:no-repeat;transition:opacity .25s}.markdown-body [class*=language-]:hover>span.copy{opacity:1}.markdown-body [class*=language-]>span.copy:hover{background-color:var(--va-code-copy-code-hover-bg)}.markdown-body [class*=language-]:before{position:absolute;top:6px;right:12px;z-index:2;font-size:12px;font-weight:500;color:var(--va-c-text-dark-3);transition:color .5s,opacity .5s}.markdown-body [class*=language-]:hover:before{opacity:0}:root{--va-c-text-warning:#544500}.vt-hamburger{display:flex;justify-content:center;align-items:center}.vt-hamburger:hover .vt-hamburger-top{transform:translate(-5.5px)}.vt-hamburger:hover .vt-hamburger-middle{transform:translate(0)}.vt-hamburger:hover .vt-hamburger-bottom{transform:translate(-11px)}.vt-hamburger-container{position:relative;width:22px;height:20px;overflow:hidden}.vt-hamburger-bottom,.vt-hamburger-middle,.vt-hamburger-top{left:0;position:absolute;width:22px;height:2px;background-color:var(--va-c-primary);transition:top .25s,background-color .5s,transform .25s}.vt-hamburger-top{top:0;transform:translate(0)}.vt-hamburger-middle{top:9px;transform:translate(-11px)}.vt-hamburger-bottom{top:18px;transform:translate(-5.5px)}.sidebar{position:fixed;overflow-y:auto;top:0;bottom:0;left:0;width:calc(100vw - 64px);max-width:var(--va-sidebar-width);background-image:var(--yun-sidebar-bg-img);background-color:var(--yun-sidebar-bg-color);background-size:contain;background-repeat:no-repeat;background-position:bottom 1rem center;text-align:center;z-index:var(--yun-z-sidebar);transform:translate(-100%);transition:box-shadow var(--va-transition-duration),background-color var(--va-transition-duration),opacity .25s,transform var(--va-transition-duration) cubic-bezier(.19,1,.22,1)!important}h1:focus .header-anchor,h1:hover .header-anchor,h2:focus .header-anchor,h2:hover .header-anchor,h3:focus .header-anchor,h3:hover .header-anchor,h4:focus .header-anchor,h4:hover .header-anchor{visibility:visible;opacity:1}a.header-anchor{float:left;margin-top:.125em;margin-left:-.87em;padding-right:.23em;font-size:.85em;visibility:hidden;opacity:0;transition:opacity var(--va-transition-duration)}a.header-anchor:before{content:none}a.header-anchor:focus,a.header-anchor:hover{text-decoration:none}:root{--va-aside-width:256px;--va-sidebar-width:300px;--va-border-width:1px;--va-font-serif:"Noto Serif SC",STZhongsong,STKaiti,KaiTi,Roboto,serif;--va-font-sans:Inter,-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen,Ubuntu,Cantarell,"Fira Sans","Droid Sans","Helvetica Neue",sans-serif;--va-font-mono:Menlo,Monaco,Consolas,"Courier New",monospace;--va-transition-duration-fast:.2s;--va-transition-duration:.4s;--va-transition-duration-slow:.6s;--va-transition:all var(--va-transition-duration-fast) ease-in-out}:root{--va-c-white:#ffffff;--va-c-black:#1a1a1a;--va-c-gray:#8e8e8e;--va-c-danger:#db2828;--va-c-warning:#f2711c;--va-c-text-light-1:#213547;--va-c-text-light-2:rgba(60, 60, 60, .7);--va-c-text-light-3:rgba(60, 60, 60, .33);--va-c-text-light-4:rgba(60, 60, 60, .18);--va-c-text-dark-1:rgba(255, 255, 255, .87);--va-c-text-dark-2:rgba(235, 235, 235, .6);--va-c-text-dark-3:rgba(235, 235, 235, .38);--va-c-text-dark-4:rgba(235, 235, 235, .18);--va-c-primary-light:#359eff;--va-c-primary-lighter:#81c2ff;--va-c-primary-dark:#006bce;--va-c-primary:#0078E7}:root{color-scheme:light;--va-c-brand:#0078E7;--va-border-color:#222;--va-c-bg:white;--va-c-bg-light:white;--va-c-bg-dark:#fafafa;--va-c-bg-opacity:rgba(255, 255, 255, .8);--va-c-bg-soft:#f9f9f9;--va-c-bg-alt:#f9f9f9;--va-c-bg-mute:#f1f1f1;--va-c-text:#333;--va-c-text-light:#555;--va-c-text-lighter:#666;--va-c-text-dark:#111;--va-c-primary-rgb:0,120,231;--va-c-link:var(--va-c-primary-dark);--va-c-divider:rgba(60, 60, 60, .2)}:root{--va-code-line-height:1.7;--va-code-font-size:.875em;--va-code-block-color:var(--va-c-text-dark-1);--va-code-block-bg:#282c34;--va-code-line-highlight-color:rgba(0, 0, 0, .5);--va-code-line-number-color:var(--va-c-text-dark-3);--va-code-copy-code-hover-bg:rgba(255, 255, 255, .05);--va-code-copy-code-active-text:var(--va-c-text-dark-2)}:root{--va-icon-copy:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' fill='none' height='20' width='20' stroke='rgba(128,128,128,1)' stroke-width='2' class='h-6 w-6' viewBox='0 0 24 24'%3E%3Cpath stroke-linecap='round' stroke-linejoin='round' d='M9 5H7a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V7a2 2 0 0 0-2-2h-2M9 5a2 2 0 0 0 2 2h2a2 2 0 0 0 2-2M9 5a2 2 0 0 1 2-2h2a2 2 0 0 1 2 2'/%3E%3C/svg%3E");--va-icon-copied:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' fill='none' height='20' width='20' stroke='rgba(128,128,128,1)' stroke-width='2' class='h-6 w-6' viewBox='0 0 24 24'%3E%3Cpath stroke-linecap='round' stroke-linejoin='round' d='M9 5H7a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V7a2 2 0 0 0-2-2h-2M9 5a2 2 0 0 0 2 2h2a2 2 0 0 0 2-2M9 5a2 2 0 0 1 2-2h2a2 2 0 0 1 2 2m-6 9 2 2 4-4'/%3E%3C/svg%3E")}.post-category{color:var(--va-c-text)}.post-tag{white-space:nowrap;color:var(--yun-tag-color)}.post-tag:hover{color:var(--va-c-primary)}html{overflow-y:scroll}.yun-main{padding-left:var(--va-sidebar-width);transition:padding-left var(--va-transition-duration);box-sizing:border-box}.yun-icon-btn{cursor:pointer;display:inline-flex;align-items:center;justify-content:center;border:none;width:3rem;height:3rem;border-radius:50%;transition:background-color var(--va-transition-duration)}.yun-icon-btn div{font-size:1.2rem}.yun-icon-btn:hover{background-color:rgba(var(--va-c-primary-rgb),.08)}.yun-icon-btn:active{background-color:rgba(var(--va-c-primary-rgb),.16)}:root{--smc-font-sans:Raleway,-apple-system,"PingFang SC","Microsoft YaHei",Arial,sans-serif;--smc-font-serif:"Songti SC","Noto Serif SC",STZhongsong,STKaiti,KaiTi,Roboto,serif;--smc-font-mono:Menlo,Monaco,Consolas,"Courier New",monospace}:root{--smc-c-primary-light:#4eaaff;--smc-c-primary-lighter:#9bcfff;--smc-c-primary:#0078E7;--smc-theme-name:yun;--smc-line-height:1.8;--smc-c-primary-rgb:0,120,231;--smc-c-text:#24292e;--smc-c-text-light:#555;--smc-c-text-lighter:#666;--smc-header-bottom-color:#eaecef;--smc-border-color:var(--smc-c-primary-light);--smc-code-bg-color:#f6f8fa;--smc-link-color:#005eb4}.markdown-body{-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%;background:var(--smc-bg-color);color:var(--smc-c-text);font-family:var(--smc-font-sans);font-size:1rem;line-height:var(--smc-line-height);overflow-wrap:break-word}.markdown-body *{box-sizing:border-box}.markdown-body a{background-color:transparent}.markdown-body a:active,.markdown-body a:hover{outline-width:0}.markdown-body hr{background-color:var(--smc-c-primary,#333);height:2px;margin:1.5rem 0}.markdown-body blockquote{margin:1rem 0;padding:0 1rem;border-left:.25em solid var(--smc-border-color)}.markdown-body code,.markdown-body pre{font-family:Source Code Pro,Consolas,Monaco,SFMono-Regular,Ubuntu Mono,Menlo,monospace}.markdown-body code{padding:3px 6px;font-size:.85rem;color:var(--smc-c-text-light);background:var(--smc-code-bg-color);border-radius:3px}.markdown-body pre{margin-top:0;margin-bottom:0;overflow-wrap:normal;padding:1rem;overflow:auto;background-color:var(--smc-code-bg-color);border-radius:3px}.markdown-body pre>code{font-size:.85rem;white-space:pre}.markdown-body pre code{display:block;padding:0;margin:0;overflow:visible;line-height:inherit;word-break:normal;background-color:transparent;border:0}.markdown-body img{display:block;margin:1rem auto;max-width:92%;max-height:600px;border-radius:.2rem;transition:.4s;--tw-shadow:0 1px 3px 0 rgb(0 0 0/.1),0 1px 2px -1px rgb(0 0 0/.1);--tw-shadow-colored:0 1px 3px 0 var(--tw-shadow-color),0 1px 2px -1px var(--tw-shadow-color);-webkit-box-shadow:var(--tw-ring-offset-shadow,0 0 transparent),var(--tw-ring-shadow,0 0 transparent),var(--tw-shadow);box-shadow:var(--tw-ring-offset-shadow,0 0 transparent),var(--tw-ring-shadow,0 0 transparent),var(--tw-shadow)}.markdown-body img:hover{--tw-shadow:0 4px 6px -1px rgb(0 0 0/.1),0 2px 4px -2px rgb(0 0 0/.1);--tw-shadow-colored:0 4px 6px -1px var(--tw-shadow-color),0 2px 4px -2px var(--tw-shadow-color)}.markdown-body img:before{content:"「 LOADING ERROR 」"}@media screen and (min-width:1600px){.markdown-body img{max-width:800px}}.markdown-body a{color:var(--smc-c-primary);text-decoration:none;border-bottom:1px solid transparent;transition:all .2s ease-in-out}.markdown-body a:hover{color:var(--smc-link-color);border-bottom:1px solid var(--smc-link-color)}.markdown-body ol,.markdown-body ul{padding-left:2em}.markdown-body ol ul,.markdown-body ul ul{margin-top:0;margin-bottom:0}.markdown-body li{overflow-wrap:break-all;margin-top:.25em}.markdown-body li>p{margin-top:16px}.markdown-body table{width:100%;overflow:auto;border-spacing:0;border-collapse:collapse}.markdown-body table td,.markdown-body table th{padding:6px 13px;border:1px solid var(--smc-c-primary-light,#999)}.markdown-body table thead th{font-weight:600}.markdown-body table>tbody>tr:hover{background-color:rgba(var(--smc-c-primary-rgb),.1)}.markdown-body strong{font-family:var(--smc-font-serif);font-weight:900}.markdown-body p{margin-top:1rem;margin-bottom:1rem;overflow-x:auto;overflow-y:hidden}.markdown-body h1,.markdown-body h2,.markdown-body h3,.markdown-body h4{margin-top:1.5rem;margin-bottom:1rem;font-weight:300;line-height:1.5}.markdown-body h1{font-size:2.5rem;border-bottom:1px solid var(--smc-header-bottom-color)}.markdown-body h2{font-size:2.2rem;border-bottom:1px solid var(--smc-header-bottom-color)}.markdown-body h3{font-size:1.9rem}.markdown-body h4{font-size:1.6rem}@media screen and (max-width:768px){.markdown-body h1{font-size:2rem}.markdown-body h2{font-size:1.8rem}.markdown-body h3{font-size:1.6rem}.markdown-body h4{font-size:1.4rem}}.markdown-body{--smc-font-family:var(--va-font-sans);--c-toc-link:var(--va-c-text-light)}.markdown-body{word-wrap:break-word}.markdown-body h1,.markdown-body h2,.markdown-body h3,.markdown-body h4{font-family:var(--va-font-serif);font-weight:900}.markdown-body ul{list-style:initial}.markdown-body ul li>p{margin-bottom:0}.markdown-body ol li{list-style:decimal}.markdown-body img{margin:auto}.markdown-body p{overflow:unset}:root{--yun-post-card-max-width:900px;--yun-c-cloud:white;--yun-z-toc-btn:7;--yun-z-cloud:7;--yun-z-go-down:9;--yun-z-backdrop:9;--yun-z-sidebar:10;--yun-z-fireworks:11;--yun-z-menu-btn:20;--yun-z-go-up-btn:20;--yun-z-search-popup:30;--yun-z-search-btn:31;--va-z-overlay:var(--yun-z-backdrop)}:root{--yun-bg-img:url(https://cdn.yunyoujun.cn/img/bg/stars-timing-0-blur-30px.jpg);--yun-sidebar-bg-color:var(--va-c-bg-light);--yun-sidebar-bg-img:url(https://cdn.yunyoujun.cn/img/bg/alpha-stars-timing-1.webp)}:root{--va-font-serif:"Noto Serif SC",STZhongsong,STKaiti,KaiTi,Roboto,serif;--va-font-sans:Inter,-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen,Ubuntu,Cantarell,"Fira Sans","Droid Sans","Helvetica Neue",sans-serif;--va-font-mono:Menlo,Monaco,Consolas,"Courier New",monospace}*,:after,:before{--un-rotate:0;--un-rotate-x:0;--un-rotate-y:0;--un-rotate-z:0;--un-scale-x:1;--un-scale-y:1;--un-scale-z:1;--un-skew-x:0;--un-skew-y:0;--un-translate-x:0;--un-translate-y:0;--un-translate-z:0;--un-scroll-snap-strictness:proximity;--un-border-spacing-x:0;--un-border-spacing-y:0;--un-ring-offset-shadow:0 0 rgba(0,0,0,0);--un-ring-shadow:0 0 rgba(0,0,0,0);--un-shadow:0 0 rgba(0,0,0,0);--un-ring-offset-width:0px;--un-ring-offset-color:#fff;--un-ring-width:0px;--un-ring-color:rgba(147,197,253,.5)}.i-ri-alipay-line{--un-icon:url("data:image/svg+xml;utf8,%3Csvg viewBox='0 0 24 24' width='1.2em' height='1.2em' xmlns='http://www.w3.org/2000/svg' %3E%3Cpath fill='currentColor' d='M18.408 16.79c-2.173-.95-3.72-1.646-4.64-2.086c-1.4 1.696-2.872 2.72-5.08 2.72S5 16.064 5.176 14.392c.12-1.096.872-2.888 4.128-2.576c1.72.16 2.504.48 3.912.944c.36-.664.664-1.4.888-2.176H7.88v-.616h3.072V8.864H7.2v-.68h3.752V6.592s.032-.248.312-.248H12.8v1.848h4v.68h-4v1.104h3.264a12.41 12.41 0 0 1-1.32 3.32c.51.182 2.097.676 4.76 1.482a8 8 0 1 0-1.096 2.012ZM12 22C6.477 22 2 17.523 2 12S6.477 2 12 2s10 4.477 10 10s-4.477 10-10 10Zm-3.568-5.632c1.44 0 2.824-.872 3.96-2.352c-1.608-.776-2.944-1.16-4.44-1.16c-1.304 0-1.984.8-2.104 1.416c-.12.616.248 2.096 2.584 2.096Z'/%3E%3C/svg%3E");-webkit-mask:var(--un-icon) no-repeat;mask:var(--un-icon) no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;background-color:currentColor;color:inherit;width:1.2em;height:1.2em}[i-ri-archive-line=""]{--un-icon:url("data:image/svg+xml;utf8,%3Csvg viewBox='0 0 24 24' width='1.2em' height='1.2em' xmlns='http://www.w3.org/2000/svg' %3E%3Cpath fill='currentColor' d='M3 10H2V4.003C2 3.449 2.455 3 2.992 3h18.016A.99.99 0 0 1 22 4.003V10h-1v10.002a.996.996 0 0 1-.993.998H3.993A.996.996 0 0 1 3 20.002V10Zm16 0H5v9h14v-9ZM4 5v3h16V5H4Zm5 7h6v2H9v-2Z'/%3E%3C/svg%3E");-webkit-mask:var(--un-icon) no-repeat;mask:var(--un-icon) no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;background-color:currentColor;color:inherit;width:1.2em;height:1.2em}[i-ri-arrow-left-s-line=""]{--un-icon:url("data:image/svg+xml;utf8,%3Csvg viewBox='0 0 24 24' width='1.2em' height='1.2em' xmlns='http://www.w3.org/2000/svg' %3E%3Cpath fill='currentColor' d='m10.828 12l4.95 4.95l-1.414 1.415L8 12l6.364-6.364l1.414 1.414l-4.95 4.95Z'/%3E%3C/svg%3E");-webkit-mask:var(--un-icon) no-repeat;mask:var(--un-icon) no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;background-color:currentColor;color:inherit;width:1.2em;height:1.2em}[i-ri-arrow-right-s-line=""]{--un-icon:url("data:image/svg+xml;utf8,%3Csvg viewBox='0 0 24 24' width='1.2em' height='1.2em' xmlns='http://www.w3.org/2000/svg' %3E%3Cpath fill='currentColor' d='m13.171 12l-4.95-4.95l1.415-1.413L16 12l-6.364 6.364l-1.414-1.415l4.95-4.95Z'/%3E%3C/svg%3E");-webkit-mask:var(--un-icon) no-repeat;mask:var(--un-icon) no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;background-color:currentColor;color:inherit;width:1.2em;height:1.2em}[i-ri-arrow-up-s-line=""]{--un-icon:url("data:image/svg+xml;utf8,%3Csvg viewBox='0 0 24 24' width='1.2em' height='1.2em' xmlns='http://www.w3.org/2000/svg' %3E%3Cpath fill='currentColor' d='m12 10.828l-4.95 4.95l-1.414-1.414L12 8l6.364 6.364l-1.415 1.414l-4.95-4.95Z'/%3E%3C/svg%3E");-webkit-mask:var(--un-icon) no-repeat;mask:var(--un-icon) no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;background-color:currentColor;color:inherit;width:1.2em;height:1.2em}[i-ri-calendar-2-line=""]{--un-icon:url("data:image/svg+xml;utf8,%3Csvg viewBox='0 0 24 24' width='1.2em' height='1.2em' xmlns='http://www.w3.org/2000/svg' %3E%3Cpath fill='currentColor' d='M9 1v2h6V1h2v2h4a1 1 0 0 1 1 1v16a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V4a1 1 0 0 1 1-1h4V1h2Zm11 10H4v8h16v-8ZM8 13v2H6v-2h2Zm5 0v2h-2v-2h2Zm5 0v2h-2v-2h2ZM7 5H4v4h16V5h-3v2h-2V5H9v2H7V5Z'/%3E%3C/svg%3E");-webkit-mask:var(--un-icon) no-repeat;mask:var(--un-icon) no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;background-color:currentColor;color:inherit;width:1.2em;height:1.2em}[i-ri-calendar-line=""]{--un-icon:url("data:image/svg+xml;utf8,%3Csvg viewBox='0 0 24 24' width='1.2em' height='1.2em' xmlns='http://www.w3.org/2000/svg' %3E%3Cpath fill='currentColor' d='M9 1v2h6V1h2v2h4a1 1 0 0 1 1 1v16a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V4a1 1 0 0 1 1-1h4V1h2Zm11 10H4v8h16v-8ZM7 5H4v4h16V5h-3v2h-2V5H9v2H7V5Z'/%3E%3C/svg%3E");-webkit-mask:var(--un-icon) no-repeat;mask:var(--un-icon) no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;background-color:currentColor;color:inherit;width:1.2em;height:1.2em}.i-ri-clipboard-line{--un-icon:url("data:image/svg+xml;utf8,%3Csvg viewBox='0 0 24 24' width='1.2em' height='1.2em' xmlns='http://www.w3.org/2000/svg' %3E%3Cpath fill='currentColor' d='M7 4V2h10v2h3.007c.548 0 .993.445.993.993v16.014a.994.994 0 0 1-.993.993H3.993A.993.993 0 0 1 3 21.007V4.993C3 4.445 3.445 4 3.993 4H7Zm0 2H5v14h14V6h-2v2H7V6Zm2-2v2h6V4H9Z'/%3E%3C/svg%3E");-webkit-mask:var(--un-icon) no-repeat;mask:var(--un-icon) no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;background-color:currentColor;color:inherit;width:1.2em;height:1.2em}.i-ri-cloud-line{--un-icon:url("data:image/svg+xml;utf8,%3Csvg viewBox='0 0 24 24' width='1.2em' height='1.2em' xmlns='http://www.w3.org/2000/svg' %3E%3Cpath fill='currentColor' d='M12 2a7 7 0 0 1 6.992 7.339A6 6 0 0 1 17 21H7A6 6 0 0 1 5.008 9.339A7 7 0 0 1 12 2Zm0 2a5 5 0 0 0-4.994 5.243l.07 1.488l-1.404.494A4.002 4.002 0 0 0 7 19h10a4 4 0 1 0-3.796-5.265l-1.898-.633A6.003 6.003 0 0 1 17 9a5 5 0 0 0-5-5Z'/%3E%3C/svg%3E");-webkit-mask:var(--un-icon) no-repeat;mask:var(--un-icon) no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;background-color:currentColor;color:inherit;width:1.2em;height:1.2em}.i-ri-download-cloud-2-fill{--un-icon:url("data:image/svg+xml;utf8,%3Csvg viewBox='0 0 24 24' width='1.2em' height='1.2em' xmlns='http://www.w3.org/2000/svg' %3E%3Cpath fill='currentColor' d='M13 13v5.585l1.828-1.828l1.415 1.415L12 22.414l-4.243-4.242l1.415-1.415L11 18.585V13h2ZM12 2a7.001 7.001 0 0 1 6.954 6.194A5.5 5.5 0 0 1 18 18.978V17a6 6 0 0 0-11.996-.225L6 17v1.978a5.5 5.5 0 0 1-.954-10.784A7 7 0 0 1 12 2Z'/%3E%3C/svg%3E");-webkit-mask:var(--un-icon) no-repeat;mask:var(--un-icon) no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;background-color:currentColor;color:inherit;width:1.2em;height:1.2em}[i-ri-file-list-line=""]{--un-icon:url("data:image/svg+xml;utf8,%3Csvg viewBox='0 0 24 24' width='1.2em' height='1.2em' xmlns='http://www.w3.org/2000/svg' %3E%3Cpath fill='currentColor' d='M20 22H4a1 1 0 0 1-1-1V3a1 1 0 0 1 1-1h16a1 1 0 0 1 1 1v18a1 1 0 0 1-1 1Zm-1-2V4H5v16h14ZM8 7h8v2H8V7Zm0 4h8v2H8v-2Zm0 4h8v2H8v-2Z'/%3E%3C/svg%3E");-webkit-mask:var(--un-icon) no-repeat;mask:var(--un-icon) no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;background-color:currentColor;color:inherit;width:1.2em;height:1.2em}[i-ri-file-word-line=""]{--un-icon:url("data:image/svg+xml;utf8,%3Csvg viewBox='0 0 24 24' width='1.2em' height='1.2em' xmlns='http://www.w3.org/2000/svg' %3E%3Cpath fill='currentColor' d='M16 8v8h-2l-2-2l-2 2H8V8h2v5l2-2l2 2V8h1V4H5v16h14V8h-3ZM3 2.992C3 2.444 3.447 2 3.998 2H16l5 5v13.992A1 1 0 0 1 20.007 22H3.993A1 1 0 0 1 3 21.008V2.992Z'/%3E%3C/svg%3E");-webkit-mask:var(--un-icon) no-repeat;mask:var(--un-icon) no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;background-color:currentColor;color:inherit;width:1.2em;height:1.2em}[i-ri-folder-2-line=""]{--un-icon:url("data:image/svg+xml;utf8,%3Csvg viewBox='0 0 24 24' width='1.2em' height='1.2em' xmlns='http://www.w3.org/2000/svg' %3E%3Cpath fill='currentColor' d='M12.414 5H21a1 1 0 0 1 1 1v14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V4a1 1 0 0 1 1-1h7.414l2 2ZM20 11H4v8h16v-8Zm0-2V7h-8.414l-2-2H4v4h16Z'/%3E%3C/svg%3E");-webkit-mask:var(--un-icon) no-repeat;mask:var(--un-icon) no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;background-color:currentColor;color:inherit;width:1.2em;height:1.2em}.i-ri-gamepad-line{--un-icon:url("data:image/svg+xml;utf8,%3Csvg viewBox='0 0 24 24' width='1.2em' height='1.2em' xmlns='http://www.w3.org/2000/svg' %3E%3Cpath fill='currentColor' d='M17 4a6 6 0 0 1 6 6v4a6 6 0 0 1-6 6H7a6 6 0 0 1-6-6v-4a6 6 0 0 1 6-6h10Zm0 2H7a4 4 0 0 0-3.995 3.8L3 10v4a4 4 0 0 0 3.8 3.995L7 18h10a4 4 0 0 0 3.995-3.8L21 14v-4a4 4 0 0 0-3.8-3.995L17 6Zm-7 3v2h2v2H9.999L10 15H8l-.001-2H6v-2h2V9h2Zm8 4v2h-2v-2h2Zm-2-4v2h-2V9h2Z'/%3E%3C/svg%3E");-webkit-mask:var(--un-icon) no-repeat;mask:var(--un-icon) no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;background-color:currentColor;color:inherit;width:1.2em;height:1.2em}.i-ri-genderless-line{--un-icon:url("data:image/svg+xml;utf8,%3Csvg viewBox='0 0 24 24' width='1.2em' height='1.2em' xmlns='http://www.w3.org/2000/svg' %3E%3Cpath fill='currentColor' d='M13 7.066A7.501 7.501 0 0 1 12 22a7.5 7.5 0 0 1-1-14.934V1h2v6.066ZM12 20a5.5 5.5 0 1 0 0-11a5.5 5.5 0 0 0 0 11Z'/%3E%3C/svg%3E");-webkit-mask:var(--un-icon) no-repeat;mask:var(--un-icon) no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;background-color:currentColor;color:inherit;width:1.2em;height:1.2em}.i-ri-github-line{--un-icon:url("data:image/svg+xml;utf8,%3Csvg viewBox='0 0 24 24' width='1.2em' height='1.2em' xmlns='http://www.w3.org/2000/svg' %3E%3Cpath fill='currentColor' d='M5.884 18.653c-.3-.2-.558-.456-.86-.816a50.59 50.59 0 0 1-.466-.579c-.463-.575-.755-.841-1.056-.95a1 1 0 1 1 .675-1.882c.752.27 1.261.735 1.947 1.588c-.094-.117.34.427.433.539c.19.227.33.365.44.438c.204.137.588.196 1.15.14c.024-.382.094-.753.202-1.096c-2.968-.725-4.648-2.64-4.648-6.396c0-1.238.37-2.355 1.058-3.291c-.218-.894-.185-1.975.302-3.192a1 1 0 0 1 .63-.583c.081-.024.127-.034.208-.047c.803-.123 1.937.17 3.415 1.097a11.731 11.731 0 0 1 2.687-.308c.912 0 1.819.103 2.684.308c1.477-.933 2.614-1.227 3.422-1.097c.085.014.158.032.218.051a1 1 0 0 1 .616.58c.487 1.215.52 2.296.302 3.19c.691.936 1.058 2.045 1.058 3.292c0 3.758-1.674 5.666-4.642 6.393c.125.415.19.878.19 1.38c0 .664-.002 1.299-.007 2.01c0 .19-.002.394-.005.706a1 1 0 0 1-.018 1.957c-1.14.228-1.984-.532-1.984-1.524l.002-.447l.005-.705c.005-.707.008-1.338.008-1.997c0-.697-.184-1.152-.426-1.361c-.661-.57-.326-1.654.541-1.751c2.966-.334 4.336-1.483 4.336-4.66c0-.955-.312-1.745-.913-2.405a1 1 0 0 1-.189-1.044c.166-.415.236-.957.095-1.614l-.01.002c-.491.14-1.11.44-1.858.95a1 1 0 0 1-.833.135a9.626 9.626 0 0 0-2.592-.35c-.89 0-1.772.12-2.592.35a1 1 0 0 1-.829-.133c-.753-.507-1.374-.807-1.87-.947c-.143.653-.072 1.194.093 1.607a1 1 0 0 1-.189 1.044c-.597.656-.913 1.459-.913 2.404c0 3.172 1.371 4.33 4.322 4.66c.865.098 1.202 1.178.545 1.749c-.193.167-.43.732-.43 1.364v3.149c0 .986-.834 1.726-1.96 1.529a1 1 0 0 1-.04-1.963v-.99c-.91.062-1.661-.087-2.254-.484Z'/%3E%3C/svg%3E");-webkit-mask:var(--un-icon) no-repeat;mask:var(--un-icon) no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;background-color:currentColor;color:inherit;width:1.2em;height:1.2em}[i-ri-heart-line=""]{--un-icon:url("data:image/svg+xml;utf8,%3Csvg viewBox='0 0 24 24' width='1.2em' height='1.2em' xmlns='http://www.w3.org/2000/svg' %3E%3Cpath fill='currentColor' d='M12.001 4.529a5.998 5.998 0 0 1 8.242.228a6 6 0 0 1 .236 8.236l-8.48 8.492l-8.478-8.492a6 6 0 0 1 8.48-8.464Zm6.826 1.641a3.998 3.998 0 0 0-5.49-.153l-1.335 1.198l-1.336-1.197a4 4 0 0 0-5.686 5.605L12 18.654l7.02-7.03a4 4 0 0 0-.193-5.454Z'/%3E%3C/svg%3E");-webkit-mask:var(--un-icon) no-repeat;mask:var(--un-icon) no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;background-color:currentColor;color:inherit;width:1.2em;height:1.2em}[i-ri-home-4-line=""]{--un-icon:url("data:image/svg+xml;utf8,%3Csvg viewBox='0 0 24 24' width='1.2em' height='1.2em' xmlns='http://www.w3.org/2000/svg' %3E%3Cpath fill='currentColor' d='M19 21H5a1 1 0 0 1-1-1v-9H1l10.327-9.388a1 1 0 0 1 1.346 0L23 11h-3v9a1 1 0 0 1-1 1Zm-6-2h5V9.158l-6-5.455l-6 5.455V19h5v-6h2v6Z'/%3E%3C/svg%3E");-webkit-mask:var(--un-icon) no-repeat;mask:var(--un-icon) no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;background-color:currentColor;color:inherit;width:1.2em;height:1.2em}.i-ri-mail-line{--un-icon:url("data:image/svg+xml;utf8,%3Csvg viewBox='0 0 24 24' width='1.2em' height='1.2em' xmlns='http://www.w3.org/2000/svg' %3E%3Cpath fill='currentColor' d='M3 3h18a1 1 0 0 1 1 1v16a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V4a1 1 0 0 1 1-1Zm17 4.238l-7.928 7.1L4 7.216V19h16V7.238ZM4.511 5l7.55 6.662L19.502 5H4.511Z'/%3E%3C/svg%3E");-webkit-mask:var(--un-icon) no-repeat;mask:var(--un-icon) no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;background-color:currentColor;color:inherit;width:1.2em;height:1.2em}.i-ri-money-cny-circle-line{--un-icon:url("data:image/svg+xml;utf8,%3Csvg viewBox='0 0 24 24' width='1.2em' height='1.2em' xmlns='http://www.w3.org/2000/svg' %3E%3Cpath fill='currentColor' d='M12.005 22.003c-5.523 0-10-4.477-10-10s4.477-10 10-10s10 4.477 10 10s-4.477 10-10 10Zm0-2a8 8 0 1 0 0-16a8 8 0 0 0 0 16Zm1-7h3v2h-3v2h-2v-2h-3v-2h3v-1h-3v-2h2.586L8.469 7.882l1.415-1.415l2.12 2.122l2.122-2.122l1.414 1.415l-2.12 2.12h2.585v2h-3v1Z'/%3E%3C/svg%3E");-webkit-mask:var(--un-icon) no-repeat;mask:var(--un-icon) no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;background-color:currentColor;color:inherit;width:1.2em;height:1.2em}[i-ri-price-tag-3-line=""]{--un-icon:url("data:image/svg+xml;utf8,%3Csvg viewBox='0 0 24 24' width='1.2em' height='1.2em' xmlns='http://www.w3.org/2000/svg' %3E%3Cpath fill='currentColor' d='m10.904 2.1l9.9 1.414l1.414 9.9l-9.193 9.192a1 1 0 0 1-1.414 0l-9.9-9.9a1 1 0 0 1 0-1.413L10.905 2.1Zm.707 2.121L3.833 12l8.485 8.485l7.779-7.778l-1.061-7.425l-7.425-1.06Zm2.122 6.364a2 2 0 1 1 2.828-2.828a2 2 0 0 1-2.828 2.828Z'/%3E%3C/svg%3E");-webkit-mask:var(--un-icon) no-repeat;mask:var(--un-icon) no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;background-color:currentColor;color:inherit;width:1.2em;height:1.2em}.i-ri-qq-line{--un-icon:url("data:image/svg+xml;utf8,%3Csvg viewBox='0 0 24 24' width='1.2em' height='1.2em' xmlns='http://www.w3.org/2000/svg' %3E%3Cpath fill='currentColor' d='m17.536 12.514l-.696-1.796c0-.021.01-.375.01-.558C16.85 7.088 15.447 4 12 4c-3.446 0-4.848 3.088-4.848 6.16c0 .183.009.537.01.557l-.696 1.797c-.19.514-.38 1.05-.517 1.51c-.657 2.189-.444 3.095-.282 3.115c.348.043 1.354-1.648 1.354-1.648c0 .98.487 2.258 1.542 3.18c-.394.127-.878.32-1.188.557c-.28.214-.245.431-.194.52c.22.385 3.79.245 4.82.125c1.03.12 4.599.26 4.82-.126c.05-.088.085-.305-.194-.519c-.311-.237-.795-.43-1.19-.556c1.055-.923 1.542-2.202 1.542-3.181c0 0 1.007 1.691 1.355 1.648c.162-.02.378-.928-.283-3.116a26.91 26.91 0 0 0-.516-1.509Zm1.021 8.227c-.373.652-.833.892-1.438 1.057a4.91 4.91 0 0 1-.794.138c-.44.045-.986.065-1.613.064a33.217 33.217 0 0 1-2.71-.116c-.692.065-1.785.114-2.71.116a16.048 16.048 0 0 1-1.614-.064a4.917 4.917 0 0 1-.793-.138c-.605-.164-1.065-.405-1.44-1.059a2.274 2.274 0 0 1-.239-1.652c-.592-.132-1.001-.482-1.279-.911a2.43 2.43 0 0 1-.309-.71a4.027 4.027 0 0 1-.116-1.106c.013-.785.187-1.762.532-2.912c.14-.466.327-1.008.567-1.655l.554-1.43a15.362 15.362 0 0 1-.002-.203C5.153 5.605 7.589 2 12 2c4.413 0 6.848 3.605 6.848 8.16l-.001.203l.553 1.43l.01.026c.225.606.413 1.153.556 1.626c.348 1.15.522 2.128.535 2.916c.007.407-.03.776-.118 1.108c-.066.246-.161.48-.31.708c-.276.427-.684.776-1.277.91c.13.554.055 1.14-.24 1.654Z'/%3E%3C/svg%3E");-webkit-mask:var(--un-icon) no-repeat;mask:var(--un-icon) no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;background-color:currentColor;color:inherit;width:1.2em;height:1.2em}.i-ri-speaker-fill{--un-icon:url("data:image/svg+xml;utf8,%3Csvg viewBox='0 0 24 24' width='1.2em' height='1.2em' xmlns='http://www.w3.org/2000/svg' %3E%3Cpath fill='currentColor' d='M4 2h16a1 1 0 0 1 1 1v18a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V3a1 1 0 0 1 1-1Zm8 18a5 5 0 1 0 0-10a5 5 0 0 0 0 10Zm0-12a1.5 1.5 0 1 0 0-3a1.5 1.5 0 0 0 0 3Zm0 10a3 3 0 1 1 0-6a3 3 0 0 1 0 6Z'/%3E%3C/svg%3E");-webkit-mask:var(--un-icon) no-repeat;mask:var(--un-icon) no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;background-color:currentColor;color:inherit;width:1.2em;height:1.2em}[i-ri-timer-line=""]{--un-icon:url("data:image/svg+xml;utf8,%3Csvg viewBox='0 0 24 24' width='1.2em' height='1.2em' xmlns='http://www.w3.org/2000/svg' %3E%3Cpath fill='currentColor' d='m17.618 5.968l1.453-1.453l1.414 1.414l-1.453 1.453A9 9 0 1 1 12 4c2.125 0 4.078.736 5.618 1.968ZM12 20a7 7 0 1 0 0-14a7 7 0 0 0 0 14ZM11 8h2v6h-2V8ZM8 1h8v2H8V1Z'/%3E%3C/svg%3E");-webkit-mask:var(--un-icon) no-repeat;mask:var(--un-icon) no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;background-color:currentColor;color:inherit;width:1.2em;height:1.2em}[i-ri-translate=""]{--un-icon:url("data:image/svg+xml;utf8,%3Csvg viewBox='0 0 24 24' width='1.2em' height='1.2em' xmlns='http://www.w3.org/2000/svg' %3E%3Cpath fill='currentColor' d='M5 15v2a2 2 0 0 0 1.85 1.994L7 19h3v2H7a4 4 0 0 1-4-4v-2h2Zm13-5l4.4 11h-2.155l-1.201-3h-4.09l-1.199 3h-2.154L16 10h2Zm-1 2.885L15.753 16h2.492L17 12.885ZM8 2v2h4v7H8v3H6v-3H2V4h4V2h2Zm9 1a4 4 0 0 1 4 4v2h-2V7a2 2 0 0 0-2-2h-3V3h3ZM6 6H4v3h2V6Zm4 0H8v3h2V6Z'/%3E%3C/svg%3E");-webkit-mask:var(--un-icon) no-repeat;mask:var(--un-icon) no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;background-color:currentColor;color:inherit;width:1.2em;height:1.2em}.i-ri-wechat-2-line{--un-icon:url("data:image/svg+xml;utf8,%3Csvg viewBox='0 0 24 24' width='1.2em' height='1.2em' xmlns='http://www.w3.org/2000/svg' %3E%3Cpath fill='currentColor' d='M8.667 11.512a1.276 1.276 0 0 1-1.285-1.286c0-.718.568-1.286 1.285-1.286c.718 0 1.285.568 1.285 1.286c0 .717-.567 1.286-1.285 1.286Zm6.667 0a1.276 1.276 0 0 1-1.285-1.286c0-.718.568-1.286 1.285-1.286s1.285.568 1.285 1.286c0 .717-.568 1.286-1.285 1.286Zm-8.511 7.704l.715-.437a4 4 0 0 1 2.706-.536c.211.033.385.059.52.077c.406.053.819.08 1.237.08c4.42 0 7.9-3.022 7.9-6.6c0-3.577-3.48-6.6-7.9-6.6c-4.421 0-7.9 3.023-7.9 6.6c0 1.366.5 2.673 1.431 3.781c.049.058.12.137.215.235a4 4 0 0 1 1.1 3.102l-.024.298Zm-.63 2.726a1 1 0 0 1-1.527-.93l.189-2.26a2 2 0 0 0-.55-1.551a6.935 6.935 0 0 1-.303-.332C2.806 15.447 2.1 13.695 2.1 11.8c0-4.75 4.432-8.6 9.9-8.6c5.467 0 9.9 3.85 9.9 8.6s-4.433 8.6-9.9 8.6c-.51 0-1.01-.033-1.5-.098c-.152-.02-.342-.048-.568-.084a2 2 0 0 0-1.353.269l-2.387 1.456Z'/%3E%3C/svg%3E");-webkit-mask:var(--un-icon) no-repeat;mask:var(--un-icon) no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;background-color:currentColor;color:inherit;width:1.2em;height:1.2em}.i-ri-wechat-pay-line{--un-icon:url("data:image/svg+xml;utf8,%3Csvg viewBox='0 0 24 24' width='1.2em' height='1.2em' xmlns='http://www.w3.org/2000/svg' %3E%3Cpath fill='currentColor' d='m19.146 8.993l-9.799 5.608l-.07.046a.647.647 0 0 1-.3.069a.655.655 0 0 1-.58-.345l-.046-.092l-1.831-3.95c-.023-.046-.023-.092-.023-.138c0-.183.139-.321.324-.321c.07 0 .139.023.209.069l2.155 1.515c.162.092.347.161.556.161a.938.938 0 0 0 .348-.069l8.274-3.648C16.935 6.273 14.635 5.2 12.001 5.2c-4.421 0-7.9 3.023-7.9 6.6c0 1.366.5 2.673 1.431 3.781c.049.058.12.137.215.235a4 4 0 0 1 1.1 3.102l-.024.298l.715-.437a4 4 0 0 1 2.706-.536c.211.033.385.059.52.077c.406.053.819.08 1.237.08c4.42 0 7.9-3.022 7.9-6.6c0-.996-.27-1.95-.755-2.807ZM6.193 21.943a1 1 0 0 1-1.527-.931l.189-2.26a2 2 0 0 0-.55-1.551a6.935 6.935 0 0 1-.303-.332C2.806 15.447 2.1 13.695 2.1 11.8c0-4.75 4.432-8.6 9.9-8.6c5.467 0 9.9 3.85 9.9 8.6s-4.433 8.6-9.9 8.6c-.51 0-1.01-.033-1.5-.098c-.152-.02-.342-.048-.568-.084a2 2 0 0 0-1.353.269l-2.387 1.456Z'/%3E%3C/svg%3E");-webkit-mask:var(--un-icon) no-repeat;mask:var(--un-icon) no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;background-color:currentColor;color:inherit;width:1.2em;height:1.2em}.i-ri-women-line{--un-icon:url("data:image/svg+xml;utf8,%3Csvg viewBox='0 0 24 24' width='1.2em' height='1.2em' xmlns='http://www.w3.org/2000/svg' %3E%3Cpath fill='currentColor' d='M11 15.934A7.501 7.501 0 0 1 12 1a7.5 7.5 0 0 1 1 14.934V18h5v2h-5v4h-2v-4H6v-2h5v-2.066ZM12 14a5.5 5.5 0 1 0 0-11a5.5 5.5 0 0 0 0 11Z'/%3E%3C/svg%3E");-webkit-mask:var(--un-icon) no-repeat;mask:var(--un-icon) no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;background-color:currentColor;color:inherit;width:1.2em;height:1.2em}[i~=ri-sun-line]{--un-icon:url("data:image/svg+xml;utf8,%3Csvg viewBox='0 0 24 24' width='1.2em' height='1.2em' xmlns='http://www.w3.org/2000/svg' %3E%3Cpath fill='currentColor' d='M12 18a6 6 0 1 1 0-12a6 6 0 0 1 0 12Zm0-2a4 4 0 1 0 0-8a4 4 0 0 0 0 8ZM11 1h2v3h-2V1Zm0 19h2v3h-2v-3ZM3.515 4.929l1.414-1.414L7.05 5.636L5.636 7.05L3.515 4.93ZM16.95 18.364l1.414-1.414l2.121 2.121l-1.414 1.414l-2.121-2.121Zm2.121-14.85l1.414 1.415l-2.121 2.121l-1.414-1.414l2.121-2.121ZM5.636 16.95l1.414 1.414l-2.121 2.121l-1.414-1.414l2.121-2.121ZM23 11v2h-3v-2h3ZM4 11v2H1v-2h3Z'/%3E%3C/svg%3E");-webkit-mask:var(--un-icon) no-repeat;mask:var(--un-icon) no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;background-color:currentColor;color:inherit;width:1.2em;height:1.2em}.yun-card{margin:auto;--un-shadow:var(--un-shadow-inset) 0 1px 3px 0 var(--un-shadow-color, rgba(0,0,0,.1)),var(--un-shadow-inset) 0 1px 2px -1px var(--un-shadow-color, rgba(0,0,0,.1));box-shadow:var(--un-ring-offset-shadow),var(--un-ring-shadow),var(--un-shadow);transition-property:color,background-color,border-color,outline-color,text-decoration-color,fill,stroke,opacity,box-shadow,transform,filter,backdrop-filter;transition-timing-function:cubic-bezier(.4,0,.2,1);transition-duration:.15s;transition-duration:var(--va-transition-duration)}.va-card{--un-shadow:var(--un-shadow-inset) 0 1px 3px 0 var(--un-shadow-color, rgba(0,0,0,.1)),var(--un-shadow-inset) 0 1px 2px -1px var(--un-shadow-color, rgba(0,0,0,.1));box-shadow:var(--un-ring-offset-shadow),var(--un-ring-shadow),var(--un-shadow)}.va-card:hover,.yun-card:hover{--un-shadow:var(--un-shadow-inset) 0 10px 15px -3px var(--un-shadow-color, rgba(0,0,0,.1)),var(--un-shadow-inset) 0 4px 6px -4px var(--un-shadow-color, rgba(0,0,0,.1));box-shadow:var(--un-ring-offset-shadow),var(--un-ring-shadow),var(--un-shadow)}@media (max-width:767.9px){.yun-main{padding-left:0}}.fixed{position:fixed}.relative{position:relative}[bottom~="19"]{bottom:4.75rem}[right~="2"]{right:.5rem}.z-1{z-index:1}.z-350{z-index:350}[m~="0"]{margin:0}[m~="2"]{margin:.5rem}[m~=y-4]{margin-top:1rem;margin-bottom:1rem}.my-1{margin-top:.25rem;margin-bottom:.25rem}[m~=x-1]{margin-left:.25rem;margin-right:.25rem}[mx~="2"],[m~=x-2]{margin-left:.5rem;margin-right:.5rem}[m~=y-2]{margin-top:.5rem;margin-bottom:.5rem}[mb~="4"]{margin-bottom:1rem}.mb-2,[m~=b-2]{margin-bottom:.5rem}[m~=l-1]{margin-left:.25rem}[m~=t-4]{margin-top:1rem}[mt-6=""],[m~=t-6]{margin-top:1.5rem}[m~=l-4]{margin-left:1rem}[m~=r-1]{margin-right:.25rem}[mt~="2"]{margin-top:.5rem}.block{display:block}.inline-block{display:inline-block}[h~="8"]{height:2rem}[w~="8"]{width:2rem}[w~=full]{width:100%}.flex,[flex~="~"]{display:flex}.inline-flex,[inline-flex=""]{display:inline-flex}.flex-grow,[flex~=grow]{flex-grow:1}.flex-col,[flex~=col]{flex-direction:column}.transform{transform:translate(var(--un-translate-x)) translateY(var(--un-translate-y)) translateZ(var(--un-translate-z)) rotate(var(--un-rotate)) rotateX(var(--un-rotate-x)) rotateY(var(--un-rotate-y)) rotate(var(--un-rotate-z)) skew(var(--un-skew-x)) skewY(var(--un-skew-y)) scaleX(var(--un-scale-x)) scaleY(var(--un-scale-y)) scaleZ(var(--un-scale-z))}@keyframes fade-in{0%{opacity:0}to{opacity:1}}@keyframes pulse{0%,to{opacity:1}50%{opacity:.5}}.animate-fade-in{animation:fade-in 1s linear 1}.animate-pulse{animation:pulse 2s cubic-bezier(.4,0,.6,1) infinite}.animate-iteration-1{animation-iteration-count:1}.items-center,[items~=center]{align-items:center}.justify-center,[justify~=center]{justify-content:center}.justify-around{justify-content:space-around}.gap-2{grid-gap:.5rem;gap:.5rem}[overflow~=auto]{overflow:auto}.truncate{overflow:hidden;text-overflow:ellipsis;white-space:nowrap}[border~="~"]{border-width:1px}[border~=rounded]{border-radius:.25rem}.rounded-full{border-radius:9999px}[stroke-width~="2"]{stroke-width:2px}.p-4,[p~="4"]{padding:1rem}[p~="1"]{padding:.25rem}[p~="2"]{padding:.5rem}[p~=x-4]{padding-left:1rem;padding-right:1rem}[py~="1"]{padding-top:.25rem;padding-bottom:.25rem}[p~=b-8]{padding-bottom:2rem}[p~=l-4]{padding-left:1rem}[text~=center]{text-align:center}[text~="2xl"]{font-size:1.5rem;line-height:2rem}[text-xl=""],[text~=xl]{font-size:1.25rem;line-height:1.75rem}[text~=xs]{font-size:.75rem;line-height:1rem}[text~=sm]{font-size:.875rem;line-height:1.25rem}[font~=black]{font-weight:900}.leading-none{line-height:1}.text-\$va-c-text-light{color:var(--va-c-text-light)}[text~=red-400]{--un-text-opacity:1;color:rgba(248,113,113,var(--un-text-opacity))}.hover\:shadow-md:hover{--un-shadow:var(--un-shadow-inset) 0 4px 6px -1px var(--un-shadow-color, rgba(0,0,0,.1)),var(--un-shadow-inset) 0 2px 4px -2px var(--un-shadow-color, rgba(0,0,0,.1));box-shadow:var(--un-ring-offset-shadow),var(--un-ring-shadow),var(--un-shadow)}.shadow{--un-shadow:var(--un-shadow-inset) 0 1px 3px 0 var(--un-shadow-color, rgba(0,0,0,.1)),var(--un-shadow-inset) 0 1px 2px -1px var(--un-shadow-color, rgba(0,0,0,.1));box-shadow:var(--un-ring-offset-shadow),var(--un-ring-shadow),var(--un-shadow)}.transition{transition-property:color,background-color,border-color,outline-color,text-decoration-color,fill,stroke,opacity,box-shadow,transform,filter,backdrop-filter;transition-timing-function:cubic-bezier(.4,0,.2,1);transition-duration:.15s}[font~=serif]{font-family:var(--va-font-serif)}@media (max-width:767.9px){.lt-md\:ml-0{margin-left:0}[p~="lt-md:0"]{padding:0}}@media (min-width:640px){.sm\:p-6{padding:1.5rem}.sm\:px-6{padding-left:1.5rem;padding-right:1.5rem}}@media (min-width:768px){.md\:hidden{display:none}.md\:translate-x-0{--un-translate-x:0;transform:translate(var(--un-translate-x)) translateY(var(--un-translate-y)) translateZ(var(--un-translate-z)) rotate(var(--un-rotate)) rotateX(var(--un-rotate-x)) rotateY(var(--un-rotate-y)) rotate(var(--un-rotate-z)) skew(var(--un-skew-x)) skewY(var(--un-skew-y)) scaleX(var(--un-scale-x)) scaleY(var(--un-scale-y)) scaleZ(var(--un-scale-z))}}@media (min-width:1024px){.lg\:px-12{padding-left:3rem;padding-right:3rem}}@media (min-width:1280px){.xl\:hidden{display:none}.xl\:px-16{padding-left:4rem;padding-right:4rem}}.post-copyright{font-size:.9rem;padding:.5rem 1rem;border-left:4px solid #ff5252;background-color:var(--va-c-bg-dark);list-style:none;word-break:break-all;position:relative;overflow:hidden}.post-copyright:after{pointer-events:none;position:absolute;color:#fff;background:url("data:image/svg+xml;charset=utf-8,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 496 512'%3E%3Cpath fill='gray' d='M245.8 214.9l-33.2 17.3c-9.4-19.6-25.2-20-27.4-20-22.2 0-33.3 14.6-33.3 43.9 0 23.5 9.2 43.8 33.3 43.8 14.4 0 24.6-7 30.5-21.3l30.6 15.5a73.2 73.2 0 01-65.1 39c-22.6 0-74-10.3-74-77 0-58.7 43-77 72.6-77 30.8-.1 52.7 11.9 66 35.8zm143 0l-32.7 17.3c-9.5-19.8-25.7-20-27.9-20-22.1 0-33.2 14.6-33.2 43.9 0 23.5 9.2 43.8 33.2 43.8 14.5 0 24.7-7 30.5-21.3l31 15.5c-2 3.8-21.3 39-65 39-22.7 0-74-9.9-74-77 0-58.7 43-77 72.6-77C354 179 376 191 389 214.8zM247.7 8C104.7 8 0 123 0 256c0 138.4 113.6 248 247.6 248C377.5 504 496 403 496 256 496 118 389.4 8 247.6 8zm.8 450.8c-112.5 0-203.7-93-203.7-202.8 0-105.5 85.5-203.3 203.8-203.3A201.7 201.7 0 01451.3 256c0 121.7-99.7 202.9-202.9 202.9z'/%3E%3C/svg%3E");content:" ";height:10rem;width:10rem;right:-2rem;top:-2rem;opacity:.1}.sponsor-button{background-color:#ffffff1a}.sponsor-button div{transform:scale(1.1);transition:transform var(--va-transition-duration) ease-in-out}.sponsor-button:hover{background-color:#ffffffe6}.sponsor-button:hover div{transform:scale(1.2)}.qrcode-container{overflow:hidden;height:0;transition:height var(--va-transition-duration) ease-in-out}.sponsor-description{color:var(--va-c-gray)}.sponsor-method-img{width:12rem;max-width:90%;aspect-ratio:1}.va-toc[data-v-e1350763]{text-align:left}.content[data-v-e1350763]{position:relative;padding-left:16px;font-size:14px;text-align:left}.outline-marker[data-v-e1350763]{position:absolute;top:32px;left:-2px;z-index:0;opacity:0;width:4px;height:18px;background-color:var(--va-c-brand);transition:top .25s cubic-bezier(0,1,.5,1),background-color .5s,opacity .25s;border-top-right-radius:2px;border-bottom-right-radius:2px}.outline-title[data-v-e1350763]{letter-spacing:.4px;line-height:28px;font-size:14px;font-weight:600}.visually-hidden[data-v-e1350763]{position:absolute;width:1px;height:1px;white-space:nowrap;clip:rect(0 0 0 0);clip-path:inset(50%);overflow:hidden}.yun-aside{position:fixed;right:0;top:0;bottom:0;width:var(--va-sidebar-width,300px);transform:translate(100%);transition:box-shadow var(--va-transition-duration),background-color var(--va-transition-duration),opacity .25s,transform var(--va-transition-duration) cubic-bezier(.19,1,.22,1)}@media screen and (min-width:1280px){.yun-aside{transform:translate(0)}}.toc-btn{color:var(--va-c-primary);background-color:#fff;z-index:var(--yun-z-toc-btn)}.post-nav{display:flex;justify-content:space-between;align-items:center}.post-nav-item{display:inline-flex;justify-content:center;align-items:center;color:var(--va-c-primary);outline:0;font-size:1.5rem;font-weight:700;text-transform:uppercase;height:3rem;transition:.4s}.post-nav-item:hover{background-color:rgba(var(--va-c-primary-rgb),.1);box-shadow:0 0 15px #0000001a}.post-nav-prev{padding:0 .6rem 0 .1rem}.post-nav-next{padding:0 .1rem 0 .6rem}.post-nav-next,.post-nav-prev{display:inline-flex;align-items:center;height:3rem;font-size:1rem}.post-nav-next .title,.post-nav-prev .title{overflow:hidden;max-width:10rem}.post-nav-next .icon,.post-nav-prev .icon{width:1.2rem;height:1.2rem}@media screen and (min-width:1024px){.post-nav-next .title,.post-nav-prev .title{max-width:18rem}}@media screen and (min-width:1280px){.content{max-width:calc(100vw - 2 * var(--va-sidebar-width) - 1rem - 8px)}}</style><link rel="preload" href="/assets/index-c2c57308.css" as="style"><link rel="modulepreload" crossorigin="" href="/assets/post-ff6bdc38.js"><link rel="preload" href="/assets/post-b195884f.css" as="style"><link rel="modulepreload" crossorigin="" href="/assets/Docker_K8s实践指南-25525e14.js"><link rel="modulepreload" crossorigin="" href="/assets/ValaxyMain.vue_vue_type_style_index_0_lang-e99d3f99.js"><link rel="preload" href="/assets/ValaxyMain-3beb7542.css" as="style"><title>Docker+K8s实践指南 - 运维之境</title><link rel="icon" href="/favicon.svg" type="image/svg+xml"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#ffffff"><meta name="generator" content="Valaxy 0.14.28"><meta name="description" content="那你终将成为别人的一条裤衩"><meta property="og:description" content="那你终将成为别人的一条裤衩"><meta property="og:locale" content="zh-CN"><meta property="og:site_name" content="运维之境"><meta property="og:title" content="Docker+K8s实践指南"><meta property="og:image" content="/favicon.svg"><meta property="og:url" content="https://www.vlinux.cn/"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@900&amp;display=swap"><link rel="preload" as="font" crossorigin="anonymous" href="/assets/KaTeX_AMS-Regular-0cdd387c.woff2"><link rel="preload" as="font" crossorigin="anonymous" href="/assets/KaTeX_Caligraphic-Bold-de7701e4.woff2"><link rel="preload" as="font" crossorigin="anonymous" href="/assets/KaTeX_Caligraphic-Regular-5d53e70a.woff2"><link rel="preload" as="font" crossorigin="anonymous" href="/assets/KaTeX_Fraktur-Bold-74444efd.woff2"><link rel="preload" as="font" crossorigin="anonymous" href="/assets/KaTeX_Fraktur-Regular-51814d27.woff2"><link rel="preload" as="font" crossorigin="anonymous" href="/assets/KaTeX_Main-Bold-0f60d1b8.woff2"><link rel="preload" as="font" crossorigin="anonymous" href="/assets/KaTeX_Main-BoldItalic-99cd42a3.woff2"><link rel="preload" as="font" crossorigin="anonymous" href="/assets/KaTeX_Main-Italic-97479ca6.woff2"><link rel="preload" as="font" crossorigin="anonymous" href="/assets/KaTeX_Main-Regular-c2342cd8.woff2"><link rel="preload" as="font" crossorigin="anonymous" href="/assets/KaTeX_Math-BoldItalic-dc47344d.woff2"><link rel="preload" as="font" crossorigin="anonymous" href="/assets/KaTeX_Math-Italic-7af58c5e.woff2"><link rel="preload" as="font" crossorigin="anonymous" href="/assets/KaTeX_SansSerif-Bold-e99ae511.woff2"><link rel="preload" as="font" crossorigin="anonymous" href="/assets/KaTeX_SansSerif-Italic-00b26ac8.woff2"><link rel="preload" as="font" crossorigin="anonymous" href="/assets/KaTeX_SansSerif-Regular-68e8c73e.woff2"><link rel="preload" as="font" crossorigin="anonymous" href="/assets/KaTeX_Script-Regular-036d4e95.woff2"><link rel="preload" as="font" crossorigin="anonymous" href="/assets/KaTeX_Size1-Regular-6b47c401.woff2"><link rel="preload" as="font" crossorigin="anonymous" href="/assets/KaTeX_Size2-Regular-d04c5421.woff2"><link rel="preload" as="font" crossorigin="anonymous" href="data:font/woff2;base64,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"><link rel="preload" as="font" crossorigin="anonymous" href="/assets/KaTeX_Size4-Regular-a4af7d41.woff2"><link rel="preload" as="font" crossorigin="anonymous" href="/assets/KaTeX_Typewriter-Regular-71d517d6.woff2"></head><body><div id="app" data-server-rendered="true"><!--[--><!--[--><canvas class="fireworks"></canvas><!--[--><div class="va-bg"></div><!--]--><!----><!--]--><!--[--><!--]--><!----><!--[--><!--[--><!----><button type="button" class="vt-hamburger menu-btn sidebar-toggle yun-icon-btn md:hidden" aria-label="mobile navigation" aria-expanded="false"><span class="vt-hamburger-container"><span class="vt-hamburger-top"></span><span class="vt-hamburger-middle"></span><span class="vt-hamburger-bottom"></span></span></button><aside class="md:translate-x-0 va-card transition sidebar"><!--[--><!--[--><!--[--><!----><div class=""><!--[--><div class="sidebar-panel"><div class="site-info" m="t-6"><a href="/about" class="site-author-avatar"><img class="rounded-full" src="https://cos.vlinux.cn/vlinux-logo/user.jpg" alt="avatar"><span class="site-author-status">🥺</span></a><div class="site-author-name"><a href="/about" class="">卷饼</a></div><a href="/about/site" class="site-name">运维之境</a><h4 class="site-subtitle block" text="xs">如果你太在意别人的话</h4><div class="site-description my-1">那你终将成为别人的一条裤衩</div></div><nav class="site-nav" text-xl="" mt-6=""><a href="/" class="site-link-item yun-icon-btn" title="首页"><div i-ri-home-4-line=""></div></a><a href="/archives/" class="site-link-item" title="归档"><div class="icon" i-ri-archive-line=""></div><span class="count">91</span></a><a href="/categories/" class="site-link-item" title="分类"><div class="icon" i-ri-folder-2-line=""></div><span class="count">16</span></a><a href="/tags/" class="site-link-item" title="标签"><div class="icon" i-ri-price-tag-3-line=""></div><span class="count">120</span></a><a href="/about" class="site-link-item yun-icon-btn" title="关于"><!--[--><div class="i-ri-clipboard-line"></div><!--]--></a></nav><hr m="t-4 b-2"><div class="links-of-author"><!--[--><a class="links-of-author-item yun-icon-btn" rel="noopener" href="https://wpa.qq.com/msgrd?v=3&amp;uin=38867033&amp;site=qq&amp;menu=yes&amp;jumpflag=1" title="QQ 38867033" target="_blank" style="color:#12b7f5"><div class="i-ri-qq-line icon"></div></a><a class="links-of-author-item yun-icon-btn" rel="noopener" href="https://github.com/vlinux" title="GitHub" target="_blank" style="color:#6e5494"><div class="i-ri-github-line icon"></div></a><a class="links-of-author-item yun-icon-btn" rel="noopener" href="https://repo.vlinux.cn/" title="杂物堆 Repo" target="_blank" style="color:#08c"><div class="i-ri-download-cloud-2-fill icon"></div></a><a class="links-of-author-item yun-icon-btn" rel="noopener" href="https://cos.vlinux.cn/www-vlinux-cn-blog-img/WechatIMG18.jpeg" title="微信公众号" target="_blank" style="color:#1aad19"><div class="i-ri-wechat-2-line icon"></div></a><a class="links-of-author-item yun-icon-btn" rel="noopener" href="https://www.vlinux.cn/music/" title="Music" target="_blank" style="color:#e6162d"><div class="i-ri-speaker-fill icon"></div></a><a class="links-of-author-item yun-icon-btn" rel="noopener" href="mailto:ilinux@88.com" title="E-Mail" target="_blank" style="color:#8e71c1"><div class="i-ri-mail-line icon"></div></a><a class="links-of-author-item yun-icon-btn" rel="noopener" href="https://www.vlinux.cn/play" title="Games" target="_blank" style="color:var(--va-c-text)"><div class="i-ri-gamepad-line icon"></div></a><!--]--></div><hr m="y-2"><div class="links"><!--[--><a href="/links/" class="link-item yun-icon-btn" title="我的小伙伴们" style="color:#1e90ff"><!--[--><div class="i-ri-genderless-line icon"></div><!--]--></a><a href="/bill" class="link-item yun-icon-btn" title="我的小账单" style="color:gold"><!--[--><div class="i-ri-money-cny-circle-line icon"></div><!--]--></a><a href="/girls/" class="link-item yun-icon-btn" title="喜欢的女孩子" style="color:#ff69b4"><!--[--><div class="i-ri-women-line icon"></div><!--]--></a><!--]--></div><br></div><div><button class="yun-icon-btn" title="切换深色模式" style="color:#f1cb64"><div i="ri-sun-line dark:ri-moon-line"></div></button><button class="yun-icon-btn" title="切换语言" style="color:var(--va-c-text)"><div i-ri-translate="" class="transition transform"></div></button></div><!--]--></div><!--]--><!--]--><!--]--></aside><!--]--><main class="yun-main lt-md:ml-0" flex="~"><div w="full" flex="~"><!--[--><div class="content" flex="~ col grow" w="full" p="l-4 lt-md:0"><div class="yun-card relative" m="0" style=""><!----><!----><!--[--><!--[--><header class="post-header mb-2" m="t-4"><h1 class="post-title flex-center" p="2" text="2xl center" font="serif black" style=""><!----><span inline-flex="" class="leading-none">Docker+K8s实践指南</span></h1></header><!--]--><!--[--><!--[--><!--[--><!--[--><!----><!----><!----><div class="post-meta" flex="~ col" justify="center" items="center" text="sm" py="1"><div class="post-time flex items-center"><span class="inline-flex-center" title="发表于"><div class="inline-block" i-ri-calendar-line=""></div><time m="l-1">2020-02-28</time></span><span class="inline-flex-center" title="更新于"><span m="x-2">-</span><div i-ri-calendar-2-line=""></div><time m="l-1">2022-12-15</time></span></div><div class="post-counter flex items-center" mt="2"><span class="inline-flex-center" title="本文字数"><div class="inline-block" i-ri-file-word-line=""></div><time m="l-1">73.1k</time></span><span class="inline-flex-center" title="阅读时长"><span m="x-2">-</span><div i-ri-timer-line=""></div><time m="l-1">286m</time></span></div></div><!--[--><!--]--><!--]--><div flex="~" text="sm" py="1"><!----><!----></div><div class="inline-flex" text="sm" py="1"><a href="/categories/?category=DevOps" class="post-category inline-flex-center"><div m="x-1" inline-flex="" i-ri-folder-2-line=""></div><span>DevOps</span></a><span mx="2">-</span><!--[--><a href="/tags/?tag=Prometheus" class="post-tag inline-flex-center" m="x-1"><div m="r-1" i-ri-price-tag-3-line=""></div><span>Prometheus</span></a><a href="/tags/?tag=Kubernetes" class="post-tag inline-flex-center" m="x-1"><div m="r-1" i-ri-price-tag-3-line=""></div><span>Kubernetes</span></a><!--]--></div><!--]--><!--]--><!--]--><div p="x-4 b-8" class="sm:px-6 lg:px-12 xl:px-16" w="full"><!--[--><article class="markdown-body"><!--[--><!----><!--[--><h2 id="写在前面的话" tabindex="-1">写在前面的话 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#写在前面的话" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h2><p>不得不承认中国的IT行业已经进入了云计算时代，越来越多的企业开始使用弹性的云服务来支撑自己的业务。云计算是机遇也是挑战，作为运维人员需要面临云计算时代企业对运维需求的萎缩，以及对运维技术能力要求的大幅度提高的困境。</p><p>如果说云计算对运维行业的影响是第一波浪潮，那我认为容器技术的发展对本行业的影响算是第二波浪潮，目前几乎所有的企业招聘运维工程师都要求熟悉KVM、OpenStack、Docker和Kubernetes技术，各大公有云平台也都上线了容器服务Kubernetes版（AWS、Azure（全球版）阿里云、腾讯云、华为云、Ucloud、京东云、天翼云（公测阶段）等），掌握这些技术已经是大势所需。</p><h2 id="参考文献" tabindex="-1">参考文献 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#参考文献" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h2><ul><li>Docker官方文档：<a target="_blank" rel="noreferrer" href="https://docs.docker.com/"><!--[-->https://docs.docker.com/<!--]--><!----></a></li><li>Kubernetes官方文档：<a target="_blank" rel="noreferrer" href="https://kubernetes.io/docs/home/"><!--[-->https://kubernetes.io/docs/home/<!--]--><!----></a></li><li>OpenStack官方文档：<a target="_blank" rel="noreferrer" href="https://docs.openstack.org/"><!--[-->https://docs.openstack.org/<!--]--><!----></a></li><li>Prometheus官方文档：<a target="_blank" rel="noreferrer" href="https://prometheus.io/docs/introduction/overview/"><!--[-->https://prometheus.io/docs/introduction/overview/<!--]--><!----></a></li></ul><h1 id="实验环境" tabindex="-1">实验环境 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#实验环境" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h1><p><strong>硬件设备准备</strong></p><ul><li>电脑一台，CPU支持VT，内存&gt;4G，可用磁盘空间大于80G。</li><li>安装VMware Workstation Pro虚拟机软件，用于创建虚拟机。</li><li>创建三台虚拟机，安装CentOS-7.7-x86_64系统。</li></ul><p><strong>实验环境详情</strong></p><table><thead><tr><th>主机名</th><th>IP地址（NAT）</th><th>描述</th></tr></thead><tbody><tr><td><a target="_blank" rel="noreferrer" href="http://linux-node1.linuxhot.com"><!--[-->linux-node1.linuxhot.com<!--]--><!----></a></td><td>eth0:192.168.56.11</td><td>1VCPU、2G内存、一块硬盘sda50G（动态）</td></tr><tr><td><a target="_blank" rel="noreferrer" href="http://linux-node2.linuxhot.com"><!--[-->linux-node2.linuxhot.com<!--]--><!----></a></td><td>eth0:192.168.56.12</td><td>1VCPU、2G内存、一块硬盘sda50G（动态）</td></tr><tr><td><a target="_blank" rel="noreferrer" href="http://linux-node3.linuxhot.com"><!--[-->linux-node3.linuxhot.com<!--]--><!----></a></td><td>eth0:192.168.56.13</td><td>1VCPU、2G内存、一块硬盘sda50G（动态）</td></tr><tr><td>备注</td><td>1.如何在安装的时候将网卡命名为eth0、eth1，请参考《附录一》。 2.其中linux-node3在学习Kubernetes中使用。</td><td></td></tr></tbody></table><p><strong>环境准备</strong></p><ul><li>安装操作系统CentOS-7.7-x86_64。</li><li>基本系统：1VCPU+2048M内存+50G（动态）硬盘。</li><li>网络选择：使用网络地址转换（NAT）。</li><li>软件包选择：Minimal Install。</li><li>关闭iptables和SELinux。</li><li>设置所有节点的主机名和IP地址，同时使用内部DNS或者/etc/hosts做好主机名解析。</li></ul><p><strong>实验小技巧</strong></p><p>[hide]</p><ul><li>建议初学者保持实验环境和本书一致，包括但不局限于IP地址，主机名，网卡名称等，可以帮你节约很多因为环境问题的排错时间。</li><li>做好虚拟机的快照，比如可以根据本书的不同章节，创建不同的快照，便于保留实验环境和在实验过程中进行环境的回滚。</li><li>请不要把关注点仅仅放在实验环节，一定要在理解的基础上完成实验，对于不明白的地方需要反复阅读，或者根据本书前言提到的途径获取技术支持。 [/hide]</li></ul><h1 id="附录一-实验环境准备" tabindex="-1">附录一 实验环境准备 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#附录一-实验环境准备" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h1><p><strong>环境准备案例：</strong></p><p>**界面位置：**文件新建虚拟机，打开新建虚拟机选项，</p><p><strong>创建虚拟机</strong>：请参考目录前实验环境章节，按要求创建虚拟机（创建步骤略）。</p><p><strong>操作系统安装：</strong></p><p>为了统一环境，保证实验的通用性，将网卡名称设置为eth*，不使用CentOS 7默认的网卡命名规则。所以需要在安装的时候，增加内核参数。</p><p>1)光标选择“Install CentOS 7”</p><p><img src="http://k8s.unixhot.com/media/c4bb194b0f60ef7a32a7a4a269b31094.png" alt="img"></p><p>2)点击Tab，打开kernel启动选项后，增加net.ifnames=0 biosdevname=0，如下图所示。</p><p><img src="http://k8s.unixhot.com/media/40432332febddc210781b55066a87252.png" alt="img"></p><p><strong>安装完毕后设置：</strong></p><p>1.设置网络。</p><p>如果你的默认NAT地址段不是192.168.56.0/24可以修改VMware Workstation的配置，点击编辑虚拟网络配置，然后进行配置。</p><p><img src="http://k8s.unixhot.com/media/b4ac2c94cc558e1150c8cc77ecd6ad13.png" alt="img"></p><p>2.设置IP地址，请配置静态IP地址。注意将UUID和MAC地址删除掉，便于进行虚拟机克隆。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@localhost ~]# vim /etc/sysconfig/network-scripts/ifcfg-eth0</span></span>
<span class="line"><span style="color:#a6accd">TYPE=Ethernet</span></span>
<span class="line"><span style="color:#a6accd">BOOTPROTO=static</span></span>
<span class="line"><span style="color:#a6accd">DEFROUTE=yes</span></span>
<span class="line"><span style="color:#a6accd">PEERDNS=no</span></span>
<span class="line"><span style="color:#a6accd">PEERROUTES=yes</span></span>
<span class="line"><span style="color:#a6accd">IPV4_FAILURE_FATAL=no</span></span>
<span class="line"><span style="color:#a6accd">NAME=eth0</span></span>
<span class="line"><span style="color:#a6accd">DEVICE=eth0</span></span>
<span class="line"><span style="color:#a6accd">ONBOOT=yes</span></span>
<span class="line"><span style="color:#a6accd">IPADDR=192.168.56.11</span></span>
<span class="line"><span style="color:#a6accd">NETMASK=255.255.255.0</span></span>
<span class="line"><span style="color:#a6accd">GATEWAY=192.168.56.2</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>重启网络服务</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@localhost ~]# systemctl restart network</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>3.关闭NetworkManager和防火墙开启自启动</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# systemctl disable firewalld</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# systemctl disable NetworkManager</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>4.设置主机名</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@localhost ~]# vi /etc/hostname</span></span>
<span class="line"><span style="color:#a6accd">linux-node1.unixhot.com</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>5.设置主机名解析</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# cat /etc/hosts</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4</span></span>
<span class="line"><span style="color:#a6accd">::1 localhost localhost.localdomain localhost6 localhost6.localdomain6</span></span>
<span class="line"><span style="color:#a6accd">192.168.56.11 linux-node1 linux-node1.unixhot.com</span></span>
<span class="line"><span style="color:#a6accd">192.168.56.12 linux-node2 linux-node2.unixhot.com</span></span>
<span class="line"><span style="color:#a6accd">192.168.56.13 linux-node3 linux-node3.unixhot.com</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>6.设置DNS解析</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@localhost ~]# vi /etc/resolv.conf</span></span>
<span class="line"><span style="color:#a6accd">nameserver 192.168.56.2</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>7.安装EPEL仓库和常用命令</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# rpm -ivh http://mirrors.aliyun.com/epel/epel-release-latest-7.noarch.rpm</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# yum install -y net-tools vim lrzsz tree screen lsof tcpdump nc mtr nmap</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>8.关闭并确认SELinux处于关闭状态</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# vim /etc/sysconfig/selinux</span></span>
<span class="line"><span style="color:#a6accd">SELINUX=disabled #修改为disabled</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>9.更新系统并重启</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# yum update -y &amp;&amp; reboot</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>10.克隆虚拟机</p><p>请关闭虚拟机，并克隆当前虚拟机linux-node1到linux-node2，建议选择“创建完整克隆”，而不是“创建链接克隆”。克隆完毕后请给linux-node2设置正确的IP地址和主机名。</p><p>11.给虚拟机做快照</p><p>分别给两台虚拟机做快照。以便于随时回到一个刚初始化完毕的系统中。可以有效的减少学习过程中的环境准备时间。</p><h1 id="第一部分-cobbler自动化安装实践" tabindex="-1">第一部分 Cobbler自动化安装实践 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#第一部分-cobbler自动化安装实践" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h1><h1 id="_1-自动化安装概述" tabindex="-1">1 自动化安装概述 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_1-自动化安装概述" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h1><p>对于运维工程师来说，服务器上架和操作系统的安装，是实施运维工作的开始。对于自动化运维来说，自动化安装是实施自动化运维的第一步。面对大量的服务器设备，我们需要掌握如何快速，并且自动的将新上架的服务器安装上对应的操作系统、或者给已经在运行的服务器重新安装系统。而且很重要的是，从第一步开始，我们就要创建自己内部的软件仓库，而这一切的工作，Cobbler都可以完成。 对于Linux来说，我们通常使用PXE+KickStart的方式进行操作系统的自动化安装。那么首先需要先了解一下KickStart。</p><h2 id="_1-1-pxe-kickstart介绍" tabindex="-1">1.1 PXE+Kickstart介绍 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_1-1-pxe-kickstart介绍" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h2><h3 id="_1-1-1-kickstart工作原理" tabindex="-1">1.1.1 KickStart工作原理 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_1-1-1-kickstart工作原理" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>KickStart是Linux的一种无人职守的安装方式。KickStart的工作原理是通过记录典型的安装过程中所需人工干预填写的各种参数，并生成一个名为 ks.cfg的文件；这样在后面安装过程中当出现要求填写参数的情况时，安装程序会首先去查找 KickStart生成的文件，当找到合适的参数时，就采用找到的参数，当没有找到合适的参数时，才需要安装者手工干预。 如果KickStart文件涵盖了安装过程中出现的所有需要填写的参数时，那么就完全不需要人工干预，安装程序会根据ks.cfg中设置的选项自动进行安装，并通过设置重启选项来重启系统，并结束安装。那么如何让服务器开机就开始自动安装操作系统呢，我们要借助开机启动选项，比如从PXE启动。</p><h3 id="_1-1-2-pxe工作原理" tabindex="-1">1.1.2 PXE工作原理 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_1-1-2-pxe工作原理" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>不管是服务器还是普通的PC机都支持多种启动方式，例如从硬盘、从U盘、从网络启动。而PXE是服务器开机启动的一种方式，也可以称之为网卡启动，因为进行PXE安装的必要条件是计算机上的网卡要支持PXE，即网卡中必须要有 PXE Client，不过不用担心基本上我们常见的服务器都支持这种方式。 首先我们需要了解下PXE的工作方式：PXE （Pre-boot Execution Environment）协议使计算机可以通过网络启动。PXE协议分为 Client 和 Server 端，PXE client 在网卡的 ROM 中，当计算机引导时，BIOS 把 PXE client 调入内存执行，由 PXE client 将放置在远端的文件通过网络下载到本地运行。 运行 PXE 协议需要设置 DHCP 服务器和 TFTP 服务器。DHCP 服务器用来给 PXE Client（将要安装系统的主机）分配一个 IP 地址，由于是给 PXE Client 分配 IP 地址，所以在配置 DHCP 服务器时需要增加相应的 PXE 设置。此外，在 PXE Client 的 ROM 中，已经存在了 TFTP Client。PXE Client 通过 TFTP 协议到 TFTP Server 上下载所需的文件。</p><h3 id="_1-1-3-pxe-kickstart工作流程" tabindex="-1">1.1.3 PXE+Kickstart工作流程 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_1-1-3-pxe-kickstart工作流程" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>下图显示了PXE+Kickstart的工作流程</p><p>1.PXE Client发送DHCP请求到DHCP Server 2.DHCP Server提供相关信息包括IP地址和TFTP服务器的位置，以及要下载的文件。 3.PXE Client请求TFTP Server。 4.TFTP Server响应请求并将文件发送给PXE Client 5.PXE Client去下载对应的Kickstart文件，并根据Kickstart文件中的内容开始进行安装 6.安装操作系统。</p><h2 id="_1-2-制作本地yum仓库" tabindex="-1">1.2 制作本地Yum仓库 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_1-2-制作本地yum仓库" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h2><p>下面我们就要准备先手动来通过PXE+Kickstart来实现自动化的系统安装，那么首先需要有一个YUM仓库，而且是本地的。下面我就拿CentOS 7为例：</p><h3 id="_1-2-1-准备安装源" tabindex="-1">1.2.1 准备安装源 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_1-2-1-准备安装源" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><ol><li><p>安装需要软件包 我们使用HTTP的方式作为YUM仓库的安装源，需要首先安装Apache，其中createrepo 是一个对rpm 文件进行索引建立的工具。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# yum install -y httpd createrepo</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# systemctl start httpd.service</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div></li><li><p>下载iso镜像</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# cd /usr/local/src</span></span>
<span class="line"><span style="color:#a6accd"># wget https://mirrors.aliyun.com/centos/7.4.1708/isos/x86_64/CentOS-7-x86_64-DVD-1708.iso</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div></li><li><p>创建yum仓库</p></li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# mount -o loop /usr/local/src/CentOS-7-x86_64-DVD-1511.iso /mnt/</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><blockquote><p>如果你使用VMWare Workstation创建虚拟机的时候设置了ISO镜像，可以不用下载，直接挂载使用。</p></blockquote><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# mount /dev/cdrom /mnt/</span></span>
<span class="line"><span style="color:#a6accd">mount: /dev/sr0 is write-protected, mounting read-only</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>在默认Apache的家目录下创建目录</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# mkdir /var/www/html/CentOS-7.4-x86_64</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# cp -a /mnt/* /var/www/html/CentOS-7.4-x86_64/</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li><p>准备Kickstart安装文件</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# cd /var/www/html/CentOS-7-x86_64/</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 CentOS-7-x86_64]# vim CentOS-7-x86_64.cfg</span></span>
<span class="line"><span style="color:#a6accd">#Kickstart Configurator by Jason Zhao</span></span>
<span class="line"><span style="color:#a6accd">#platform=x86, AMD64, or Intel EM64T</span></span>
<span class="line"><span style="color:#a6accd">#System  language</span></span>
<span class="line"><span style="color:#a6accd">lang en_US</span></span>
<span class="line"><span style="color:#a6accd">#System keyboard</span></span>
<span class="line"><span style="color:#a6accd">keyboard us</span></span>
<span class="line"><span style="color:#a6accd">#Sytem timezone</span></span>
<span class="line"><span style="color:#a6accd">timezone Asia/Shanghai</span></span>
<span class="line"><span style="color:#a6accd">#Root password</span></span>
<span class="line"><span style="color:#a6accd">rootpw --iscrypted $1$example$I.i3m26O7QYNja8p5Cj9.0</span></span>
<span class="line"><span style="color:#a6accd">#Use text mode install</span></span>
<span class="line"><span style="color:#a6accd">text</span></span>
<span class="line"><span style="color:#a6accd">#Install OS instead of upgrade</span></span>
<span class="line"><span style="color:#a6accd">install</span></span>
<span class="line"><span style="color:#a6accd">#Use NFS installation Media</span></span>
<span class="line"><span style="color:#a6accd">url --url=http://192.168.56.11/CentOS-7-x86_64</span></span>
<span class="line"><span style="color:#a6accd">#System bootloader configuration</span></span>
<span class="line"><span style="color:#a6accd">bootloader --location=mbr --driveorder=sda --append="net.ifnames=0 biosdevname=0"</span></span>
<span class="line"><span style="color:#a6accd">#Clear the Master Boot Record</span></span>
<span class="line"><span style="color:#a6accd">zerombr</span></span>
<span class="line"><span style="color:#a6accd">#Partition clearing information</span></span>
<span class="line"><span style="color:#a6accd">clearpart --all --initlabel </span></span>
<span class="line"><span style="color:#a6accd">#Disk partitioning information</span></span>
<span class="line"><span style="color:#a6accd">part /boot --fstype=xfs --size=1024</span></span>
<span class="line"><span style="color:#a6accd">part swap --asprimary --fstype="swap" --size=1024</span></span>
<span class="line"><span style="color:#a6accd">part / --fstype=xfs --size=1 --grow</span></span>
<span class="line"><span style="color:#a6accd">#System authorization infomation</span></span>
<span class="line"><span style="color:#a6accd">auth  --useshadow  --enablemd5 </span></span>
<span class="line"><span style="color:#a6accd">#Network information</span></span>
<span class="line"><span style="color:#a6accd">network --bootproto=dhcp --device=eth0 --onboot=on --activate</span></span>
<span class="line"><span style="color:#a6accd"># Reboot after installation</span></span>
<span class="line"><span style="color:#a6accd">reboot</span></span>
<span class="line"><span style="color:#a6accd">#Firewall configuration</span></span>
<span class="line"><span style="color:#a6accd">firewall --disabled </span></span>
<span class="line"><span style="color:#a6accd">#SELinux configuration</span></span>
<span class="line"><span style="color:#a6accd">selinux --disabled</span></span>
<span class="line"><span style="color:#a6accd">#Service configuration</span></span>
<span class="line"><span style="color:#a6accd">services --disabled=postfix</span></span>
<span class="line"><span style="color:#a6accd">#Do not configure XWindows</span></span>
<span class="line"><span style="color:#a6accd">skipx</span></span>
<span class="line"><span style="color:#a6accd">#Package install information</span></span>
<span class="line"><span style="color:#a6accd">%packages</span></span>
<span class="line"><span style="color:#a6accd">@ base</span></span>
<span class="line"><span style="color:#a6accd">@ core</span></span>
<span class="line"><span style="color:#a6accd">bash-completion</span></span>
<span class="line"><span style="color:#a6accd">sysstat</span></span>
<span class="line"><span style="color:#a6accd">ntp</span></span>
<span class="line"><span style="color:#a6accd">lrzsz</span></span>
<span class="line"><span style="color:#a6accd">openssl-devel</span></span>
<span class="line"><span style="color:#a6accd">zlib-devel</span></span>
<span class="line"><span style="color:#a6accd">OpenIPMI-tools</span></span>
<span class="line"><span style="color:#a6accd">screen</span></span>
<span class="line"><span style="color:#a6accd">%end</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div></li></ol><h3 id="_1-2-2-配置tftp-server" tabindex="-1">1.2.2 配置TFTP-Server <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_1-2-2-配置tftp-server" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>我们已经了解了PXE+Kickstart的原理，那么我们需要准备相关dhcp、tftp-server等。</p><ol><li><p>安装配置tftp-server</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# yum install -y tftp-server xinetd</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# vim /etc/xinetd.d/tftp</span></span>
<span class="line"><span style="color:#a6accd">disable                 = no（修改为no）</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div></li><li><p>准备安装需要的文件</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# yum install -y syslinux</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# mkdir /var/lib/tftpboot/pxelinux</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# cp /usr/share/syslinux/pxelinux.0 /var/lib/tftpboot/pxelinux</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# cp /mnt/isolinux/* /var/lib/tftpboot/pxelinux</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# cp /mnt/images/pxeboot/{vmlinuz,initrd.img} /var/lib/tftpboot/pxelinux/</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div></li></ol><p>在tftpboot下创建pxelinux.cfg目录</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# mkdir /var/lib/tftpboot/pxelinux/pxelinux.cfg </span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# cp /mnt/isolinux/isolinux.cfg /var/lib/tftpboot/pxelinux/pxelinux.cfg/default</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li>修改default配置，指定网站安装的kickstart文件</li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# vim /var/lib/tftpboot/pxelinux.cfg/default</span></span>
<span class="line"><span style="color:#a6accd">…省略…</span></span>
<span class="line"><span style="color:#a6accd">#在label linux上面增加自动化安装的配置</span></span>
<span class="line"><span style="color:#a6accd">label ks</span></span>
<span class="line"><span style="color:#a6accd">  menu label ^Auto Install CentOS 7</span></span>
<span class="line"><span style="color:#a6accd">  kernel vmlinuz</span></span>
<span class="line"><span style="color:#a6accd">  append initrd=initrd.img ip=dhcp inst.repo=http://192.168.56.11/CentOS-7-x86_64/ inst.ks=http://192.168.56.11/CentOS-7-x86_64/CentOS-7-x86_64.cfg net.ifnames=0 biosdevname=0</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li><p>配置DHCP Server</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# yum install -y dhcp</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# /bin/cp /usr/share/doc/dhcp-4.2.5/dhcpd.conf.example /etc/dhcp/dhcpd.conf</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# vim /etc/dhcp/dhcpd.conf</span></span>
<span class="line"><span style="color:#a6accd">option domain-name "devopsedu.com";</span></span>
<span class="line"><span style="color:#a6accd">option domain-name-servers 192.168.56.2;</span></span>
<span class="line"><span style="color:#a6accd">#请删除其他的subnet配置</span></span>
<span class="line"><span style="color:#a6accd">subnet 192.168.56.0 netmask 255.255.255.0 {</span></span>
<span class="line"><span style="color:#a6accd">     range dynamic-bootp 192.168.56.100 192.168.56.200;</span></span>
<span class="line"><span style="color:#a6accd">     option subnet-mask              255.255.255.0;  #设置子网掩码</span></span>
<span class="line"><span style="color:#a6accd">option routers                  192.168.56.2;    #设置网关</span></span>
<span class="line"><span style="color:#a6accd">     next-server                     192.168.56.11;   #设置TFTP-Server地址</span></span>
<span class="line"><span style="color:#a6accd">     filename                        "pxelinux/pxelinux.0";   #设置TFTP需要下载的文件</span></span>
<span class="line"><span style="color:#a6accd">}</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div></li><li><p>启动httpd、tftp和dhcpd服务</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# systemctl start xinetd.service dhcpd.service httpd.service</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div></li><li><p>检查服务是否启动</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# netstat -ntlp</span></span>
<span class="line"><span style="color:#a6accd">Active Internet connections (only servers)</span></span>
<span class="line"><span style="color:#a6accd">Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name    </span></span>
<span class="line"><span style="color:#a6accd">tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      871/sshd            </span></span>
<span class="line"><span style="color:#a6accd">tcp6       0      0 :::80                   :::*                    LISTEN      1321/httpd          </span></span>
<span class="line"><span style="color:#a6accd">tcp6       0      0 :::22                   :::*                    LISTEN      871/sshd</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div></li></ol><p>查看dhcpd和tftpd是否启动</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# netstat -nulp</span></span>
<span class="line"><span style="color:#a6accd">Active Internet connections (only servers)</span></span>
<span class="line"><span style="color:#a6accd">Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name    </span></span>
<span class="line"><span style="color:#a6accd">udp        0      0 0.0.0.0:67              0.0.0.0:*                           1487/dhcpd          </span></span>
<span class="line"><span style="color:#a6accd">udp        0      0 0.0.0.0:69              0.0.0.0:*                           1486/xinetd</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h3 id="_1-2-4-自动化安装和流程总结" tabindex="-1">1.2.4 自动化安装和流程总结 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_1-2-4-自动化安装和流程总结" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>现在就可以新创建一台虚拟机来进行测试了，记着选择网卡启动。DHCP后，就可以看到安装页面。我们刚才添加的Auto Install CentOS 也可以看到了。</p><p>下面我们可以来总结下PXE+KickStart的安装流程了： 需要经历以下的过程： 1.网卡上的PXE芯片有512字节，存放了DHCP和TFTP的客户端。 2.启动计算机选择网卡启动。 3.PXE上的DHCP客户端会向DHCP服务器，申请IP地址 4.DHCP服务器分配给它IP地址的同时通过以下字段，告诉pxe，TFTP的地址和它要下载的文件 next-server 192.168.56.11； filename "pxelinux.0"； 5.pxelinux.0告诉PXE要下载的配置文件是pxelinux.cfg目录下面的default 6.pxe下载并依据配置文件的内容下载启动必须的文件，并通过ks.cfg配置内容开始系统安装。</p><p><strong>我们都干了什么</strong> 所以我们看到设置一个网络环境可能涉及到许多步骤，才能为开始安装做好准备。您必须：</p><ul><li><p>配置服务，比如 DHCP、TFTP、DNS、HTTP、FTP 和 NFS</p></li><li><p>在 DHCP 和 TFTP 配置文件中填入各个客户端机器的信息</p></li><li><p>创建自动部署文件（比如 kickstart 和 autoinst）</p></li><li><p>将安装媒介解压缩到 HTTP/FTP/NFS 存储库中。 这个过程并不简单，甚至可以说复杂，我们要自己管理和创建不同的操作系统版本、手动管理DHCP等，那么如何能让我们有一个功能强大，更实用的自动化装机平台，开源社区给了我们答案就是Cobbler。 官方网站：<a target="_blank" rel="noreferrer" href="http://cobbler.github.io/"><!--[-->http://cobbler.github.io/<!--]--><!----></a></p></li></ul><h1 id="_2-cobbler快速入门" tabindex="-1">2 Cobbler快速入门 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_2-cobbler快速入门" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h1><p>使用 Cobbler，您无需进行人工干预即可安装机器。Cobbler设置一个PXE引导环境，并控制与安装相关的所有软件和环境准备，比如网络引导服务（DHCP 和 TFTP）与存储库镜像。当希望安装一台新机器时，Cobbler 可以：</p><ul><li>使用一个以前定义的模板来配置 DHCP 服务（如果启用了管理 DHCP）</li><li>将一个存储库（yum 或 rsync）建立镜像或解压缩一个媒介，以注册一个新操作系统</li><li>在 DHCP 配置文件中为需要安装的机器创建一个条目，并使用您指定的参数（IP 和 MAC 地址）</li><li>在 TFTFP 服务目录下创建适当的 PXE 文件</li><li>重新启动 DHCP 服务以反映更改</li><li>重新启动机器以开始安装（如果电源管理已启用）</li></ul><h2 id="_2-1-cobbler部署" tabindex="-1">2.1 Cobbler部署 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_2-1-cobbler部署" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h2><h3 id="_2-1-1-cobbler功能" tabindex="-1">2.1.1 Cobbler功能 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_2-1-1-cobbler功能" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>Cobbler 支持众多的发行版：Red Hat、Fedora、CentOS、Debian、Ubuntu 和 SuSE。当添加一个操作系统（通常通过使用 ISO 文件）时，Cobbler 知道如何解压缩合适的文件并调整网络服务，以正确引导机器。</p><p>Cobbler 可使用 kickstart 模板。基于 Red Hat 或 Fedora 的系统使用 kickstart 文件来自动化安装流程。通过使用模板，您就会拥有基本的 kickstart 模板，然后定义如何针对一种配置文件或机器配置而替换其中的变量。例如，一个模板可能包含两个变量 $domain 和 $machine_name。在 Cobbler 配置中，一个配置文件指定 <a target="_blank" rel="noreferrer" href="http://domain=mydomain.com"><!--[-->domain=mydomain.com<!--]--><!----></a>，并且每台使用该配置文件的机器在 machine_name 变量中指定其名称。该配置文件中的所有机器都使用相同的 kickstart 安装且针对 <a target="_blank" rel="noreferrer" href="http://domain=mydomain.com"><!--[-->domain=mydomain.com<!--]--><!----></a> 进行配置，但每台机器拥有其自己的机器名称。您仍然可以使用 kickstart 模板在不同的域中安装其他机器并使用不同的机器名称。</p><p>使用 koan 客户端，Cobbler 可从客户端配置虚拟机并重新安装系统。我不会讨论配置管理和 koan 特性，因为它们不属于本文的介绍范畴。但是，它们是值得研究的有用特性。</p><h3 id="_2-1-2-cobbler安装" tabindex="-1">2.1.2 Cobbler安装 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_2-1-2-cobbler安装" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><ol><li>安装EPEL源</li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# rpm -ivh http://mirrors.aliyun.com/epel/epel-release-latest-7.noarch.rpm</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ul><li>yum安装cobbler</li></ul><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# yum install -y httpd dhcp tftp python-ctypes cobbler cobbler-web pykickstart fence-agents xinetd debmirror</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li>启动并配置cobbler</li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# systemctl enable httpd cobblerd</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# systemctl start httpd cobblerd</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ul><li>下载Cobbler需要的启动文件</li></ul><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# cobbler get-loaders</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ul><li>检查Cobbler状态</li></ul><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# cobbler check</span></span>
<span class="line"><span style="color:#a6accd">The following are potential configuration items that you may want to fix:</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">1 : The 'server' field in /etc/cobbler/settings must be set to something other than localhost, or kickstarting features will not work.  This should be a resolvable hostname or IP for the boot server as reachable by all machines that will use it.</span></span>
<span class="line"><span style="color:#a6accd">2 : For PXE to be functional, the 'next_server' field in /etc/cobbler/settings must be set to something other than 127.0.0.1, and should match the IP of the boot server on the PXE network.</span></span>
<span class="line"><span style="color:#a6accd">3 : change 'disable' to 'no' in /etc/xinetd.d/tftp</span></span>
<span class="line"><span style="color:#a6accd">4 : enable and start rsyncd.service with systemctl</span></span>
<span class="line"><span style="color:#a6accd">5 : comment out 'dists' on /etc/debmirror.conf for proper debian support</span></span>
<span class="line"><span style="color:#a6accd">6 : comment out 'arches' on /etc/debmirror.conf for proper debian support</span></span>
<span class="line"><span style="color:#a6accd">7 : The default password used by the sample templates for newly installed machines (default_password_crypted in /etc/cobbler/settings) is still set to 'cobbler' and should be changed, try: "openssl passwd -1 -salt 'random-phrase-here' 'your-password-here'" to generate new one</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">Restart cobblerd and then run 'cobbler sync' to apply changes.</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>这段话的意思就是需要处理上面的所有问题，然后重启cobblerd服务，然后执行cobbler sync同步修改操作。</p><p><strong>将上面提到的7个问题进行修复</strong></p><ul><li>问题1和2解决</li></ul><p>均是需要修改cobbler的配置文件。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# vim /etc/cobbler/settings</span></span>
<span class="line"><span style="color:#a6accd">#修改以下两行即可，可以直接搜索127.0.0.1来定位</span></span>
<span class="line"><span style="color:#a6accd">server: 192.168.56.11   #设置cobbler server的IP地址</span></span>
<span class="line"><span style="color:#a6accd">next_server: 192.168.56.11  #设置PXE server的IP地址</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ul><li>问题3解决</li></ul><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# vim /etc/xinetd.d/tftp</span></span>
<span class="line"><span style="color:#a6accd">service tftp</span></span>
<span class="line"><span style="color:#a6accd">{</span></span>
<span class="line"><span style="color:#a6accd">        socket_type             = dgram</span></span>
<span class="line"><span style="color:#a6accd">        protocol                = udp</span></span>
<span class="line"><span style="color:#a6accd">        wait                    = yes</span></span>
<span class="line"><span style="color:#a6accd">        user                    = root</span></span>
<span class="line"><span style="color:#a6accd">        server                  = /usr/sbin/in.tftpd</span></span>
<span class="line"><span style="color:#a6accd">        server_args             = -s /var/lib/tftpboot</span></span>
<span class="line"><span style="color:#a6accd">        disable                 = no</span></span>
<span class="line"><span style="color:#a6accd">        per_source              = 11</span></span>
<span class="line"><span style="color:#a6accd">        cps                     = 100 2</span></span>
<span class="line"><span style="color:#a6accd">        flags                   = IPv4</span></span>
<span class="line"><span style="color:#a6accd">}</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ul><li>问题4解决</li></ul><p>启动rsyncd服务，并设置开机自动启动</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# systemctl enable rsyncd</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# systemctl start rsyncd</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ul><li>问题5、6解决</li></ul><p>安装debmirror是debian系列系统使用的</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# yum install -y debmirror</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# vim /etc/debmirror.conf</span></span>
<span class="line"><span style="color:#a6accd">#请注释掉下面两行配置</span></span>
<span class="line"><span style="color:#a6accd">#@dists="sid";</span></span>
<span class="line"><span style="color:#a6accd">#@arches="i386"</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ul><li>问题7解决</li></ul><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# openssl passwd -1 -salt 'example' 'devopsedu.com'</span></span>
<span class="line"><span style="color:#a6accd">$1$example$I.i3m26O7QYNja8p5Cj9.0 </span></span>
<span class="line"><span style="color:#a6accd">[root@ops-node1 ~]# vim /etc/cobbler/settings</span></span>
<span class="line"><span style="color:#a6accd">将下面字段替换为上面生成的字段：</span></span>
<span class="line"><span style="color:#a6accd">default_password_crypted: "$1$example$I.i3m26O7QYNja8p5Cj9.0"</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# systemctl restart cobblerd</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ul><li>再次检查Cobbler</li></ul><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd"> [root@ops-node1 ~]# cobbler check</span></span>
<span class="line"><span style="color:#a6accd">No configuration problems found.  All systems go.</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h3 id="_2-1-3-cobbler管理dhcp" tabindex="-1">2.1.3 Cobbler管理DHCP <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_2-1-3-cobbler管理dhcp" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><ol><li>开启管理DHCP服务，这样Cobbler就可以接管DHCP的管理工作</li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@ops-node1 ~]# vim /etc/cobbler/settings</span></span>
<span class="line"><span style="color:#a6accd">manage_dhcp: 1</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li>首先修改dhcp的配置文件</li></ol><p>修改dhcp的模板配置文件，设置相对应的DHCP的IP地址和分配的网段。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@ops-node1 ~]# vim /etc/cobbler/dhcp.template </span></span>
<span class="line"><span style="color:#a6accd">subnet 192.168.56.0 netmask 255.255.255.0 {</span></span>
<span class="line"><span style="color:#a6accd">     option routers             192.168.56.2;</span></span>
<span class="line"><span style="color:#a6accd">     option domain-name-servers 192.168.56.2;</span></span>
<span class="line"><span style="color:#a6accd">     option subnet-mask         255.255.255.0;</span></span>
<span class="line"><span style="color:#a6accd">     range dynamic-bootp        192.168.56.100 192.168.56.254;</span></span>
<span class="line"><span style="color:#a6accd">     default-lease-time         21600;</span></span>
<span class="line"><span style="color:#a6accd">     max-lease-time             43200;</span></span>
<span class="line"><span style="color:#a6accd">     next-server                $next_server;</span></span>
<span class="line"><span style="color:#a6accd">  …</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>重启Cobbler并进行同步操作</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# systemctl restart cobblerd </span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# cobbler sync</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><blockquote><p>执行完毕cobbler sync后，会自动生成/etc/dhcpd.conf。并重启dhcp服务。所以说使用Cobbler管理DHCP后，请勿修改/etc/dhcpd.conf。以后所有dhcp相关的配置都是修改Cobbler的DHCP模板文件/etc/cobbler/dhcp.template。</p></blockquote><h3 id="_2-1-4-cobbler导入镜像" tabindex="-1">2.1.4 Cobbler导入镜像 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_2-1-4-cobbler导入镜像" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><ol><li>下载并导入镜像</li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@ops-node1 ~]# cd /usr/local/src</span></span>
<span class="line"><span style="color:#a6accd">[root@ops-node1 ~]# wget https://mirror.tuna.tsinghua.edu.cn/centos/7.7.1908/isos/x86_64/CentOS-7-x86_64-Minimal-1908.iso</span></span>
<span class="line"><span style="color:#a6accd">[root@ops-node1 ~]# mmount -o loop /usr/local/src/CentOS-7-x86_64-Minimal-1908.iso /mnt/</span></span>
<span class="line"><span style="color:#a6accd">[root@ops-node1 ~]# cobbler import --path=/mnt/ --name=CentOS-7.7-x86_64 --arch=x86_64</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>参数说明:</strong></p><ul><li>--name 为安装源定义一个名字</li><li>--arch 指定安装源是32位还是64位、ia64, 目前支持的选项有: x86│x86_64│ia64</li></ul><blockquote><p>小提示：Cobbler 会把安装的镜像ISO拷贝到源安装镜像目录下: /var/www/cobbler/ks_mirror/</p></blockquote><ol><li>查看导入后结果</li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# cobbler list</span></span>
<span class="line"><span style="color:#a6accd">distros:</span></span>
<span class="line"><span style="color:#a6accd">   CentOS-7-x86_64</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">profiles:</span></span>
<span class="line"><span style="color:#a6accd">   CentOS-7-x86_64</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h3 id="_2-1-5-自定义kickstart文件" tabindex="-1">2.1.5 自定义kickstart文件 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_2-1-5-自定义kickstart文件" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>Cobbler的Kickstart文件和默认的不同，我们需要修改，主要是增加上Cobbler的变量。导入镜像后，我们一般会自定义kickstart文件给这个镜像。首先可以将自定义后的Cobbler的kickstart文件放置在/var/lib/cobbler/kickstarts目录下，这也是Cobbler的默认存放kickstart文件的地方。</p><ol><li>自定义Kickstart文件</li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# vim /var/lib/cobbler/kickstarts/CentOS-7.7-x86_64-Cobbler.cfg</span></span>
<span class="line"><span style="color:#a6accd">#Kickstart Configurator by Jason Zhao</span></span>
<span class="line"><span style="color:#a6accd">#platform=x86, AMD64, or Intel EM64T</span></span>
<span class="line"><span style="color:#a6accd">#System  language</span></span>
<span class="line"><span style="color:#a6accd">lang en_US</span></span>
<span class="line"><span style="color:#a6accd">#System keyboard</span></span>
<span class="line"><span style="color:#a6accd">keyboard us</span></span>
<span class="line"><span style="color:#a6accd">#Sytem timezone</span></span>
<span class="line"><span style="color:#a6accd">timezone Asia/Shanghai</span></span>
<span class="line"><span style="color:#a6accd">#Root password</span></span>
<span class="line"><span style="color:#a6accd">rootpw --iscrypted $default_password_crypted</span></span>
<span class="line"><span style="color:#a6accd">#Use text mode install</span></span>
<span class="line"><span style="color:#a6accd">text</span></span>
<span class="line"><span style="color:#a6accd">#Install OS instead of upgrade</span></span>
<span class="line"><span style="color:#a6accd">install</span></span>
<span class="line"><span style="color:#a6accd">#Use NFS installation Media</span></span>
<span class="line"><span style="color:#a6accd">url --url=$tree</span></span>
<span class="line"><span style="color:#a6accd">#System bootloader configuration</span></span>
<span class="line"><span style="color:#a6accd">bootloader --location=mbr --driveorder=sda --append="net.ifnames=0 biosdevname=0"</span></span>
<span class="line"><span style="color:#a6accd">#Clear the Master Boot Record</span></span>
<span class="line"><span style="color:#a6accd">zerombr</span></span>
<span class="line"><span style="color:#a6accd">#Partition clearing information</span></span>
<span class="line"><span style="color:#a6accd">clearpart --all --initlabel </span></span>
<span class="line"><span style="color:#a6accd">#Disk partitioning information</span></span>
<span class="line"><span style="color:#a6accd">part /boot --fstype=xfs --size=1024</span></span>
<span class="line"><span style="color:#a6accd">part swap --asprimary --fstype="swap" --size=1024</span></span>
<span class="line"><span style="color:#a6accd">part / --fstype=xfs --size=1 --grow</span></span>
<span class="line"><span style="color:#a6accd">#System authorization infomation</span></span>
<span class="line"><span style="color:#a6accd">auth  --useshadow  --enablemd5 </span></span>
<span class="line"><span style="color:#a6accd">#Network information</span></span>
<span class="line"><span style="color:#a6accd">$SNIPPET('network_config')</span></span>
<span class="line"><span style="color:#a6accd"># Reboot after installation</span></span>
<span class="line"><span style="color:#a6accd">reboot</span></span>
<span class="line"><span style="color:#a6accd">#Firewall configuration</span></span>
<span class="line"><span style="color:#a6accd">firewall --disabled </span></span>
<span class="line"><span style="color:#a6accd">#SELinux configuration</span></span>
<span class="line"><span style="color:#a6accd">selinux --disabled</span></span>
<span class="line"><span style="color:#a6accd">#Service configuration</span></span>
<span class="line"><span style="color:#a6accd">services --disabled=postfix</span></span>
<span class="line"><span style="color:#a6accd">#Do not configure XWindows</span></span>
<span class="line"><span style="color:#a6accd">skipx</span></span>
<span class="line"><span style="color:#a6accd">#Package install information</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">%pre</span></span>
<span class="line"><span style="color:#a6accd">$SNIPPET('log_ks_pre')</span></span>
<span class="line"><span style="color:#a6accd">$SNIPPET('kickstart_start')</span></span>
<span class="line"><span style="color:#a6accd">$SNIPPET('pre_install_network_config')</span></span>
<span class="line"><span style="color:#a6accd"># Enable installation monitoring</span></span>
<span class="line"><span style="color:#a6accd">$SNIPPET('pre_anamon')</span></span>
<span class="line"><span style="color:#a6accd">%end</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">%packages</span></span>
<span class="line"><span style="color:#a6accd">@ base</span></span>
<span class="line"><span style="color:#a6accd">@ core</span></span>
<span class="line"><span style="color:#a6accd">bash-completion</span></span>
<span class="line"><span style="color:#a6accd">sysstat</span></span>
<span class="line"><span style="color:#a6accd">ntp</span></span>
<span class="line"><span style="color:#a6accd">lrzsz</span></span>
<span class="line"><span style="color:#a6accd">openssl-devel</span></span>
<span class="line"><span style="color:#a6accd">zlib-devel</span></span>
<span class="line"><span style="color:#a6accd">OpenIPMI-tools</span></span>
<span class="line"><span style="color:#a6accd">screen</span></span>
<span class="line"><span style="color:#a6accd">%end</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li>然后编辑profile来制定kickstart文件。</li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@ops-node1 ~]# cobbler profile edit --name=CentOS-7.7-x86_64 --kickstart=/var/lib/cobbler/kickstarts/CentOS-7.7-x86_64-Cobbler.cfg</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li>增加安装时的内核参数</li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@ops-node1 ~]# cobbler profile edit --name=CentOS-7.7-x86_64 --kopts='net.ifnames=0 biosdevname=0'</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li>最后，一定要执行同步，才能将Cobbler设置完成</li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# cobbler sync</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h2 id="_2-2-使用cobbler自动化安装centos" tabindex="-1">2.2 使用Cobbler自动化安装CentOS <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_2-2-使用cobbler自动化安装centos" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h2><p>现在我们就可以使用Cobbler进行安装了。实验环境下，你可以新创建一个虚拟机（注意网卡的选择）来进行测试了。Cobbler非常人性化的创建了一个Local，即使从硬盘启动，这样也避免了有服务器从网卡启动开始自动安装的问题。</p><h3 id="_2-2-1-使用cobbler安装centos" tabindex="-1">2.2.1 使用Cobbler安装CentOS <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_2-2-1-使用cobbler安装centos" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><ol><li>启动物理服务器</li><li>选择CentOS-7.7-x86_64之后即可重新开始安装。</li></ol><blockquote><p>在生产环境如果想要自动开始安装，我们的通常做法是有一个专门进行服务器安装的VLAN，安装完毕后，进入待调配状态，然后根据流程进入到生产环境。但是提醒大家，自动化开始进行操作系统安装风险依然很大。</p></blockquote><h3 id="_2-2-2-koan重新安装系统" tabindex="-1">2.2.2 Koan重新安装系统 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_2-2-2-koan重新安装系统" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>在实践的运维工作中，你肯定需要重新安装操作系统，有了Cobbler你就不用去机房了，直接使用Koan就可以进行自动化重新安装操作系统。需要再待重装的服务器上安装koan。</p><p>安装Koan软件包</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@localhost ~]# rpm -ivh http://mirrors.aliyun.com/epel/epel-release-latest-7.noarch.rpm</span></span>
<span class="line"><span style="color:#a6accd">[root@localhost ~]# yum install -y koan</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>列出可以安装的系统</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@localhost ~]# koan --server=192.168.56.11 --list=profiles</span></span>
<span class="line"><span style="color:#a6accd">- looking for Cobbler at http://192.168.56.11:80/cobbler_api</span></span>
<span class="line"><span style="color:#a6accd">CentOS-7.7-x86_64</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>指定需要重新安装的操作系统</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@localhost ~]# koan --replace-self --server=192.168.56.11 --profile=CentOS-7.7-x86_64</span></span>
<span class="line"><span style="color:#a6accd">- looking for Cobbler at http://192.168.56.11:80/cobbler_api</span></span>
<span class="line"><span style="color:#a6accd">- reading URL: http://192.168.56.11/cblr/svc/op/ks/profile/CentOS-7.7-x86_64</span></span>
<span class="line"><span style="color:#a6accd">install_tree: http://192.168.56.11/cblr/links/CentOS-7.7-x86_64</span></span>
<span class="line"><span style="color:#a6accd">downloading initrd initrd.img to /boot/initrd.img_koan</span></span>
<span class="line"><span style="color:#a6accd">url=http://192.168.56.11/cobbler/images/CentOS-7.7-x86_64/initrd.img</span></span>
<span class="line"><span style="color:#a6accd">- reading URL: http://192.168.56.11/cobbler/images/CentOS-7.7-x86_64/initrd.img</span></span>
<span class="line"><span style="color:#a6accd">downloading kernel vmlinuz to /boot/vmlinuz_koan</span></span>
<span class="line"><span style="color:#a6accd">url=http://192.168.56.11/cobbler/images/CentOS-7.7-x86_64/vmlinuz</span></span>
<span class="line"><span style="color:#a6accd">- reading URL: http://192.168.56.11/cobbler/images/CentOS-7.4-x86_64/vmlinuz</span></span>
<span class="line"><span style="color:#a6accd">- ['/sbin/grubby', '--add-kernel', '/boot/vmlinuz_koan', '--initrd', '/boot/initrd.img_koan', '--args', '"ksdevice=link lang= text net.ifnames=0 ks=http://192.168.56.11/cblr/svc/op/ks/profile/CentOS-7.4-x86_64 biosdevname=0 kssendmac "', '--copy-default', '--make-default', '--title=kick1526449287']</span></span>
<span class="line"><span style="color:#a6accd">- ['/sbin/grubby', '--update-kernel', '/boot/vmlinuz_koan', '--remove-args=root']</span></span>
<span class="line"><span style="color:#a6accd">- reboot to apply changes</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>如下图所示，Koan会创建一个新的启动选项，重启后，直接开始自动安装。</p><p>我们先不要执行reboot先研究下是如何实现的。你打开Grub的配置文件</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@localhost ~]# vim /boot/grub2/grub.cfg</span></span>
<span class="line"><span style="color:#a6accd">你会发现这样的配置</span></span>
<span class="line"><span style="color:#a6accd">menuentry 'kick1526449287' --class centos --class gnu-linux --class gnu --class os --unrestricted $menuentry_id_option 'gnulinux-3.10.0-693.el7.x86_64-advanced-49e658db-2c61-475a-9325-903fdbeb7cd4' {</span></span>
<span class="line"><span style="color:#a6accd">        load_video</span></span>
<span class="line"><span style="color:#a6accd">        set gfxpayload=keep</span></span>
<span class="line"><span style="color:#a6accd">        insmod gzio</span></span>
<span class="line"><span style="color:#a6accd">        insmod part_msdos</span></span>
<span class="line"><span style="color:#a6accd">        insmod xfs</span></span>
<span class="line"><span style="color:#a6accd">        set root='hd0,msdos1'</span></span>
<span class="line"><span style="color:#a6accd">        if [ x$feature_platform_search_hint = xy ]; then</span></span>
<span class="line"><span style="color:#a6accd">          search --no-floppy --fs-uuid --set=root --hint-bios=hd0,msdos1 --hint-efi=hd0,msdos1 --hint-baremetal=ahci0,msdos1 --hint='hd0,msdos1'  0a801c26-5320-448d-b261-883499529bc6</span></span>
<span class="line"><span style="color:#a6accd">        else</span></span>
<span class="line"><span style="color:#a6accd">          search --no-floppy --fs-uuid --set=root 0a801c26-5320-448d-b261-883499529bc6</span></span>
<span class="line"><span style="color:#a6accd">        fi</span></span>
<span class="line"><span style="color:#a6accd">        linux16 /vmlinuz_koan ro crashkernel=auto biosdevname=0 net.ifnames=0 rhgb quiet LANG=en_US.UTF-8 ksdevice=link lang= text net.ifnames=0 ks=http://192.168.56.11/cblr/svc/op/ks/profile/CentOS-7.4-x86_64 biosdevname=0 kssendmac</span></span>
<span class="line"><span style="color:#a6accd">        initrd16 /initrd.img_koan</span></span>
<span class="line"><span style="color:#a6accd">也就是说，koan帮我们下载了安装需要的文件，并且修改了Grub的开机启动选项，增加了安装的配置，并且设置为默认启动选项，这样我们执行重启默认就开始安装了。现在执行重启来验证一下。</span></span>
<span class="line"><span style="color:#a6accd">[root@hadoop-node1 ~]# reboot</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>你可以看到如下图所示的界面，然后开始进行自动化重新安装。</p><h2 id="_2-3-cobbler-web介绍" tabindex="-1">2.3 Cobbler Web介绍 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_2-3-cobbler-web介绍" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h2><p>Cobbler还提供了可视化的Web节界面叫做Cobbler Web。我们使用yum安装完毕后，会再/etc/http/conf.d/目录下生成cobbler_web.conf配置文件，可以通过<a target="_blank" rel="noreferrer" href="https://ip/cobbler_web%E6%9D%A5%E8%BF%9B%E8%A1%8C%E8%AE%BF%E9%97%AE%E3%80%82"><!--[-->https://IP/cobbler_web来进行访问。<!--]--><!----></a></p><ol><li><p>安装Cobbler Web</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# yum install -y cobbler-web</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div></li><li><p>配置Cobbler Web</p></li></ol><p>Cobbler web的权限管理有两个配置文件 /etc/cobbler/users.conf和/etc/cobbler/users.digest 后者为Cobbler权限配置文件，我们需要使用htdigest来为用户设置密码</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# htdigest  /etc/cobbler/users.digest "Cobbler" cobbler</span></span>
<span class="line"><span style="color:#a6accd">Changing password for user cobbler in realm Cobbler</span></span>
<span class="line"><span style="color:#a6accd">New password: </span></span>
<span class="line"><span style="color:#a6accd">Re-type new password:</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>然后就可以使用cobbler用户和你设置的密码登陆了。 <a target="_blank" rel="noreferrer" href="https://192.168.56.11/cobbler_web"><!--[-->https://192.168.56.11/cobbler_web<!--]--><!----></a></p><h2 id="_2-4-深入理解cobbler" tabindex="-1">2.4 深入理解Cobbler <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_2-4-深入理解cobbler" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h2><p>经过前面的内容，我们已经可以顺利的使用Cobbler进行操作系统的安装，使用Koan进行操作系统自动重新安装的操作了，那么我们还需要更多的掌握一些Cobbler的知识，才能在生产的应用中，更如鱼得水。</p><h3 id="_2-4-1-cobbler-配置文件" tabindex="-1">2.4.1 Cobbler 配置文件 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_2-4-1-cobbler-配置文件" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><ol><li>Cobbler的目录 Cobbler安装完毕后会在系统生成如下目录：</li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# find / -name cobbler</span></span>
<span class="line"><span style="color:#a6accd">/etc/selinux/targeted/active/modules/100/cobbler</span></span>
<span class="line"><span style="color:#a6accd">/etc/cobbler</span></span>
<span class="line"><span style="color:#a6accd">/var/lib/cobbler</span></span>
<span class="line"><span style="color:#a6accd">/var/log/cobbler</span></span>
<span class="line"><span style="color:#a6accd">/var/www/cobbler</span></span>
<span class="line"><span style="color:#a6accd">/usr/bin/cobbler</span></span>
<span class="line"><span style="color:#a6accd">/usr/lib/python2.7/site-packages/cobbler</span></span>
<span class="line"><span style="color:#a6accd">/usr/share/cobbler</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li>Cobbler配置文件目录</li></ol><p>Cobbler的配置文件存放在/etc/cobbler下。</p><ul><li>/etc/cobbler/settings为主配置文件；</li><li>在/etc/cobbler下你还能看到dhcp、dns、pxe、dnsmasq的模板配置文件；</li><li>/etc/cobbler/users.digest为用于web访问的用户名密码配置文件；</li><li>/etc/cobbler/modules.conf 为模块配置文件；</li><li>/etc/cobbler/users.conf为Cobbler WebUI/Web service授权配置文件。</li><li>修改Cobbler提示</li></ul><p>如果你想修改Cobbler的提示，可以直接编辑下面文件。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@test-node1 ~]# vim /etc/cobbler/pxe/pxedefault.template</span></span>
<span class="line"><span style="color:#a6accd">DEFAULT menu</span></span>
<span class="line"><span style="color:#a6accd">PROMPT 0</span></span>
<span class="line"><span style="color:#a6accd">MENU TITLE DevOpsEDU | http://www.devopsedu.com</span></span>
<span class="line"><span style="color:#a6accd">TIMEOUT 200</span></span>
<span class="line"><span style="color:#a6accd">TOTALTIMEOUT 6000</span></span>
<span class="line"><span style="color:#a6accd">ONTIMEOUT $pxe_timeout_profile</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">LABEL local</span></span>
<span class="line"><span style="color:#a6accd">        MENU LABEL (local)</span></span>
<span class="line"><span style="color:#a6accd">        MENU DEFAULT</span></span>
<span class="line"><span style="color:#a6accd">        LOCALBOOT -1</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">$pxe_menu_items</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">MENU end</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li>Cobbler数据目录/var/lib/cobbler，</li></ol><p>此目录存储和Cobbler profiles、systems、distros相关的配置。</p><ul><li>configs/ - 此目录用于存储distros、repos、systems和profiles相关信息。</li><li>backup/ - 备份目录</li><li>snippets/ - 用于放置一些可以在kickstarts导入的脚本小片段</li><li>triggers/ - 此目录用来放置一些可执行脚本</li><li>kickstarts/ - 此目录用来放置kickstart模板文件</li><li>Repo数据目录/var/www/cobbler</li></ul><p>导入的发行版，repos镜像和kickstart文件都放置在/var/www/cobbler目录下。确保/var目录有足够的空间来存储这些文件。</p><ul><li>images/ - 存储所有导入发行版的Kernel和initrd镜像用于远程网络启动</li><li>ks_mirror/ - 存储导入的发行版</li><li>repo_mirror/ - yum repos存储目录</li><li>Cobbler日志目录</li></ul><p>/var/log/cobbler用于存放日志文件/var/log/cobbler/cobbler.log</p><h3 $cs="" id="_2-4-2-cobbler设计方式" tabindex="-1">2.4.2 Cobbler设计方式 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_2-4-2-cobbler设计方式" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>Cobbler 有多个对象组成的，对象和对象之间可以相互引用：</p><ul><li>Repo（存储库）：保存一个 yum 或 rsync 存储库的镜像信息。例如我们可以将Zabbix的软件仓库同步到我们本地，就是一个Repo。</li><li>Distro（发行版）：表示一个操作系统。它承载了内核和 initrd 的信息，以及内核参数等其他数据。</li><li>profile（配置文件）：包含一个distro（发行版）、一个 kickstart 文件以及可能的Repo（存储库），还包含更多特定的内核参数等其他数据。</li><li>system（系统）：表示要安装的机器。它包含一个配置文件或一个镜像，还包含 IP 和 MAC 地址、电源管理（地址、凭据、类型）以及更为专业的数据等信息。</li><li>Image（镜像）：可替换一个包含不属于此类别的文件的发行版对象（例如，无法分为内核和 initrd 的对象）。 基于注册的对象以及各个对象之间的关联，Cobbler 知道如何更改文件系统以反映具体配置。因为系统配置的内部是抽象的，所以您可以仅关注想要执行的操作。</li></ul><h3 id="_2-4-3-cobbler-distro" tabindex="-1">2.4.3 Cobbler distro <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_2-4-3-cobbler-distro" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>使用Cobbler的第一步就是定义Distro，回想下我们最早执行的cobbler import，就会帮我们创建一个distro，我们可以使用下面的命令进行查看：</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# cobbler distro list</span></span>
<span class="line"><span style="color:#a6accd">   CentOS-7-x86_64</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>Cobbler所有的命令都可以使用help这样的方式获取帮助。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# cobbler distro help</span></span>
<span class="line"><span style="color:#a6accd">usage</span></span>
<span class="line"><span style="color:#a6accd">=====</span></span>
<span class="line"><span style="color:#a6accd">cobbler distro add</span></span>
<span class="line"><span style="color:#a6accd">cobbler distro copy</span></span>
<span class="line"><span style="color:#a6accd">cobbler distro edit</span></span>
<span class="line"><span style="color:#a6accd">cobbler distro find</span></span>
<span class="line"><span style="color:#a6accd">cobbler distro list</span></span>
<span class="line"><span style="color:#a6accd">cobbler distro remove</span></span>
<span class="line"><span style="color:#a6accd">cobbler distro rename</span></span>
<span class="line"><span style="color:#a6accd">cobbler distro report</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>我们可以使用report命令来看distro都包含哪些内容。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# cobbler distro report</span></span>
<span class="line"><span style="color:#a6accd">Name                           : CentOS-7.7-x86_64</span></span>
<span class="line"><span style="color:#a6accd">Architecture                   : x86_64</span></span>
<span class="line"><span style="color:#a6accd">TFTP Boot Files                : {}</span></span>
<span class="line"><span style="color:#a6accd">Breed                          : redhat</span></span>
<span class="line"><span style="color:#a6accd">Comment                        : </span></span>
<span class="line"><span style="color:#a6accd">Fetchable Files                : {}</span></span>
<span class="line"><span style="color:#a6accd">Initrd                         : /var/www/cobbler/ks_mirror/CentOS-7.7-x86_64/images/pxeboot/initrd.img</span></span>
<span class="line"><span style="color:#a6accd">Kernel                         : /var/www/cobbler/ks_mirror/CentOS-7.7-x86_64/images/pxeboot/vmlinuz</span></span>
<span class="line"><span style="color:#a6accd">Kernel Options                 : {}</span></span>
<span class="line"><span style="color:#a6accd">Kernel Options (Post Install)  : {}</span></span>
<span class="line"><span style="color:#a6accd">Kickstart Metadata             : {'tree': 'http://@@http_server@@/cblr/links/CentOS-7.7-x86_64'}</span></span>
<span class="line"><span style="color:#a6accd">Management Classes             : []</span></span>
<span class="line"><span style="color:#a6accd">OS Version                     : rhel7</span></span>
<span class="line"><span style="color:#a6accd">Owners                         : ['admin']</span></span>
<span class="line"><span style="color:#a6accd">Red Hat Management Key         : &lt;&lt;inherit&gt;&gt;</span></span>
<span class="line"><span style="color:#a6accd">Red Hat Management Server      : &lt;&lt;inherit&gt;&gt;</span></span>
<span class="line"><span style="color:#a6accd">Template Files                 : {}</span></span>
<span class="line"><span style="color:#a6accd">  可以看到distro定义的仅仅是我们要安装操作系统发行版的kernel和initrd。</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h3 id="_2-1-4-cobbler-profile" tabindex="-1">2.1.4 Cobbler profile <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_2-1-4-cobbler-profile" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>那么在cobbler import的同时也默认创建了一个和distro同名的profile，那么cobbler profile里面包括了distribution、kickstart file和repo。我们也可以把profile理解为一个配置集合，比如在distro的基础上增加可一个kiskstart文件来生成一个特定的系统安装配置。 比如前面我们多次使用cobbler profile edit为指定的Profile设置kickstart文件和内核参数，profile的名称即使我们在使用cobbler进行自动化安装选择的菜单名称。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# cobbler profile report</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h3 id="_2-1-5cobbler-repo" tabindex="-1">2.1.5Cobbler repo <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_2-1-5cobbler-repo" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>Cobbler repos可以帮我们管理yum仓库，把创建企业内部的yum源变成了一件极其简单的工作，比如通常生成环境我们想把EPEL仓库同步到本地，这样就避免每次安装软件包占用公网贷款。 添加EPEL源：</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd"># cobbler repo add --name=CentOS-7-x86_64-epel \</span></span>
<span class="line"><span style="color:#a6accd">--mirror=http://mirrors.aliyun.com/epel/7/x86_64/ --arch=x86_64 --breed=wget</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>参数说明: --name 为安装源定义一个名字 --arch 指定安装源是32位还是64位、ia64, 目前支持的选项有: x86│x86_64│ia64 reposync 操作很重要，因为它会从远程存储库中复制文件。如果创建了存储库对象但未运行 reposync，那么您的存储库将是空的，而且您的安装可能会失败。 根据需要同步的仓库大小，注意硬盘空间，第一次同步时间比较长，建议放到screen里面允许。</p><p>添加repo到profile 把我们自定义的repo添加到对应的profile后，那么使用对应profile安装的机器，默认就会添加该repo，前提是需要进行设置</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd"># cobbler profile edit --name=CentOS-7-x86_64 --repos="openstack-liberty"</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# vim /etc/cobbler/settings</span></span>
<span class="line"><span style="color:#a6accd">yum_post_install_mirror: 1  #默认是开启的。</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>添加更新仓库源计划任务 如果使用的外包源链接，可以定期的进行同步，放在crontab里面每天执行。 echo "0 2 0 cobbler reposync --tries=3 --no-fail" &gt;&gt; /var/spool/cron/root</p><p>设置装机自动设置Yum 修改ks脚本，增加</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">%post</span></span>
<span class="line"><span style="color:#a6accd"># Start yum configuration</span></span>
<span class="line"><span style="color:#a6accd">$yum_config_stanza</span></span>
<span class="line"><span style="color:#a6accd"># End yum configuration</span></span>
<span class="line"><span style="color:#a6accd">%end</span></span>
<span class="line"><span style="color:#a6accd">这样开机的时候就可以调用yum_config_stanza这个snippets进行设置了。等等，这个snippets是什么东东？</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h3 id="_2-1-6-cobbler-snippets" tabindex="-1">2.1.6 Cobbler snippets <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_2-1-6-cobbler-snippets" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>snippets可以说是cobbler管理中的精华部分，很少有文档提及，因为大多数人使用简单的cobbler功能已经足够了，如果你想对安装过程和安装后进行定制，可以自己编写Snippet来实现。现在我们可以参考Cobbler自带的kickstart模板，给我们的模板增加上snippets的功能</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# vim /var/lib/cobbler/kickstarts/sample_end.ks</span></span>
<span class="line"><span style="color:#a6accd">%post</span></span>
<span class="line"><span style="color:#a6accd">$SNIPPET('log_ks_post')</span></span>
<span class="line"><span style="color:#a6accd"># Start yum configuration</span></span>
<span class="line"><span style="color:#a6accd">$yum_config_stanza</span></span>
<span class="line"><span style="color:#a6accd"># End yum configuration</span></span>
<span class="line"><span style="color:#a6accd">$SNIPPET('post_install_kernel_options')</span></span>
<span class="line"><span style="color:#a6accd">$SNIPPET('post_install_network_config')</span></span>
<span class="line"><span style="color:#a6accd">$SNIPPET('func_register_if_enabled')</span></span>
<span class="line"><span style="color:#a6accd">$SNIPPET('download_config_files')</span></span>
<span class="line"><span style="color:#a6accd">$SNIPPET('koan_environment')</span></span>
<span class="line"><span style="color:#a6accd">$SNIPPET('redhat_register')</span></span>
<span class="line"><span style="color:#a6accd">$SNIPPET('cobbler_register')</span></span>
<span class="line"><span style="color:#a6accd"># Enable post-install boot notification</span></span>
<span class="line"><span style="color:#a6accd">$SNIPPET('post_anamon')</span></span>
<span class="line"><span style="color:#a6accd"># Start final steps</span></span>
<span class="line"><span style="color:#a6accd">$SNIPPET('kickstart_done')</span></span>
<span class="line"><span style="color:#a6accd"># End final steps</span></span>
<span class="line"><span style="color:#a6accd">%end</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>将%post开始到%end的内容复制下来，编辑到 /var/lib/cobbler/kickstarts/CentOS-7.4-x86_64-Cobbler.cfg文件中。在后面的自动化实践和高级话题中，我们将用到这个功能。</p><h1 id="_3-cobbler自动化实践" tabindex="-1">3 Cobbler自动化实践 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_3-cobbler自动化实践" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h1><p>在了解了Cobbler的基本使用和Cobbler的各种组件之后，在生产环境中，我们就可以灵活的运用Cobbler来完成很多自动化的工作。</p><h2 id="_3-1-cobbler构建私有yum仓库" tabindex="-1">3.1 Cobbler构建私有YUM仓库 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_3-1-cobbler构建私有yum仓库" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h2><p>Cobbler除了用来进行自动化安装之外，最方便的一个功能就是做YUM仓库了。从此再也不用担心，在服务器无法上外网的情况下，如何使用开源工具了。而且如果你需要，Cobbler可以帮你进行同步，及时的和官方的源保持一致。</p><h3 id="_3-1-1-构建openstack私有仓库" tabindex="-1">3.1.1 构建OpenStack私有仓库 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_3-1-1-构建openstack私有仓库" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>添加OpenStack源：</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# cobbler repo add --name=openstack-queens-x86_64 \</span></span>
<span class="line"><span style="color:#a6accd"> --mirror=http://mirrors.aliyun.com/centos/7.4.1708/cloud/x86_64/openstack-queens/ \</span></span>
<span class="line"><span style="color:#a6accd"> --arch=x86_64 --breed=yum</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>对于 yum 存储库 URL，Cobbler 接受 <a target="_blank" rel="noreferrer" href="http://xn--ftp-3y3b//%E3%80%81rsync://%E3%80%81%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E8%B7%AF%E5%BE%84%E5%92%8C"><!--[-->http://、ftp://、rsync://、文件系统路径和<!--]--><!----></a> ssh 位置（通过使用基于私钥的身份验证）。</p><ul><li>http协议 方式：只能拉取到某一软件的最新版本，无法拉取所有版本。</li><li>rsync协议方式：镜像方式，可以拉取提供 rsync 服务目录下的所有目录和文件。</li></ul><p>同步仓库</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# cobbler reposync</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><blockquote><p>提示：OpenStack Queens的源同步完成大约在900M左右。</p></blockquote><h3 id="_3-1-2-构建zabbix私有仓库" tabindex="-1">3.1.2 构建Zabbix私有仓库 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_3-1-2-构建zabbix私有仓库" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>由于Zabbix官方不支持rsync的方式同步，所以只能使用http的方式同步最新版本的源。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# cobbler repo add --name=zabbix-3.4-rhel7-x86_64  \</span></span>
<span class="line"><span style="color:#a6accd">--mirror=http://repo.zabbix.com/zabbix/3.4/rhel/7/x86_64/</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# cobbler repo list</span></span>
<span class="line"><span style="color:#a6accd">   zabbix-3.4-rhel7-x86_64</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>同步仓库</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# cobbler reposync</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><blockquote><p>提示：Zabbix 3.4版本的源同步完成大约在20M左右。</p></blockquote><h3 id="_3-1-3-构建saltstack私有仓库" tabindex="-1">3.1.3 构建SaltStack私有仓库 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_3-1-3-构建saltstack私有仓库" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>由于SaltStack的国外源相对比较慢，所以生产会使用Cobbler构建内部源。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# cobbler repo add --name=saltstack-2018.3-rhel7-x86_64 \</span></span>
<span class="line"><span style="color:#a6accd"> --mirror=https://repo.saltstack.com/yum/redhat/7/x86_64/2018.3</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>同步仓库</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# cobbler reposync</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><blockquote><p>提示：SaltStack 2018.3版本的源同步完成大约在50M左右。</p></blockquote><h3 id="_3-1-4-将私有仓库添加到profile" tabindex="-1">3.1.4 将私有仓库添加到Profile <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_3-1-4-将私有仓库添加到profile" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# cobbler repo list</span></span>
<span class="line"><span style="color:#a6accd">   openstack-queens-rhel7-x86_64</span></span>
<span class="line"><span style="color:#a6accd">   saltstack-2018.3-rhel7-x86_64</span></span>
<span class="line"><span style="color:#a6accd">   zabbix-3.4-rhel7-x86_64</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>将自定义的Repo添加到对应的Profile之后，我们使用该Profile安装完毕的虚拟机默认就会增加这些仓库的配置。 提示：必须执行完毕reposync之后，才能添加到Profile。多个repo之间使用空格分隔。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# cobbler profile edit --name CentOS-7.4-x86_64 \</span></span>
<span class="line"><span style="color:#a6accd">--repos="openstack-queens-rhel7-x86_64 saltstack-2018.3-rhel7-x86_64 zabbix-3.4-rhel7-x86_64"</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>使用Profile CentOS-7.4-x86_64创建出来的虚拟机都会有一个cobbler-config.repo文件</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@localhost ~]# cd /etc/yum.repos.d/</span></span>
<span class="line"><span style="color:#a6accd">[root@localhost yum.repos.d]# cat cobbler-config.repo </span></span>
<span class="line"><span style="color:#a6accd">[openstack-queens-rhel7-x86_64]</span></span>
<span class="line"><span style="color:#a6accd">name=openstack-queens-rhel7-x86_64</span></span>
<span class="line"><span style="color:#a6accd">baseurl=http://192.168.56.11/cobbler/repo_mirror/openstack-queens-rhel7-x86_64</span></span>
<span class="line"><span style="color:#a6accd">enabled=1</span></span>
<span class="line"><span style="color:#a6accd">priority=99</span></span>
<span class="line"><span style="color:#a6accd">gpgcheck=0</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">[saltstack-2018.3-rhel7-x86_64]</span></span>
<span class="line"><span style="color:#a6accd">name=saltstack-2018.3-rhel7-x86_64</span></span>
<span class="line"><span style="color:#a6accd">baseurl=http://192.168.56.11/cobbler/repo_mirror/saltstack-2018.3-rhel7-x86_64</span></span>
<span class="line"><span style="color:#a6accd">enabled=1</span></span>
<span class="line"><span style="color:#a6accd">priority=99</span></span>
<span class="line"><span style="color:#a6accd">gpgcheck=0</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">[zabbix-3.4-rhel7-x86_64]</span></span>
<span class="line"><span style="color:#a6accd">name=zabbix-3.4-rhel7-x86_64</span></span>
<span class="line"><span style="color:#a6accd">baseurl=http://192.168.56.11/cobbler/repo_mirror/zabbix-3.4-rhel7-x86_64</span></span>
<span class="line"><span style="color:#a6accd">enabled=1</span></span>
<span class="line"><span style="color:#a6accd">priority=99</span></span>
<span class="line"><span style="color:#a6accd">gpgcheck=0</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h2 id="_3-2-使用cobbler自动化安装esxi" tabindex="-1">3.2 使用Cobbler自动化安装ESXi <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_3-2-使用cobbler自动化安装esxi" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h2><ol><li>挂载ESXI镜像并导入</li></ol><p>首先使用Vmware Workstation挂载对应的ESXi镜像，这里使用的6.5版本。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# mount /dev/cdrom /mnt/</span></span>
<span class="line"><span style="color:#a6accd">mount: /dev/sr0 is write-protected, mounting read-only</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# cobbler import --path=/mnt --name=ESXi6.5 --arch=x86_64</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li>会自动生成distros和profiles</li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# cobbler list</span></span>
<span class="line"><span style="color:#a6accd">distros:</span></span>
<span class="line"><span style="color:#a6accd">   CentOS-7.4-x86_64</span></span>
<span class="line"><span style="color:#a6accd">   ESXi6.5-x86_64</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">profiles:</span></span>
<span class="line"><span style="color:#a6accd">   CentOS-7.4-x86_64</span></span>
<span class="line"><span style="color:#a6accd">   ESXi6.5-x86_64</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li>查看kickstart文件</li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# cobbler profile report --name ESXi6.5-x86_64</span></span>
<span class="line"><span style="color:#a6accd">Name                           : ESXi6.5-x86_64</span></span>
<span class="line"><span style="color:#a6accd">TFTP Boot Files                : {}</span></span>
<span class="line"><span style="color:#a6accd">Comment                        : </span></span>
<span class="line"><span style="color:#a6accd">DHCP Tag                       : default</span></span>
<span class="line"><span style="color:#a6accd">Distribution                   : ESXi6.5-x86_64</span></span>
<span class="line"><span style="color:#a6accd">Enable gPXE?                   : 0</span></span>
<span class="line"><span style="color:#a6accd">Enable PXE Menu?               : 1</span></span>
<span class="line"><span style="color:#a6accd">Fetchable Files                : {}</span></span>
<span class="line"><span style="color:#a6accd">Kernel Options                 : {}</span></span>
<span class="line"><span style="color:#a6accd">Kernel Options (Post Install)  : {}</span></span>
<span class="line"><span style="color:#a6accd">Kickstart                      : /var/lib/cobbler/kickstarts/sample_esxi6.ks</span></span>
<span class="line"><span style="color:#a6accd">Kickstart Metadata             : {}</span></span>
<span class="line"><span style="color:#a6accd">Management Classes             : []</span></span>
<span class="line"><span style="color:#a6accd">Management Parameters          : &lt;&lt;inherit&gt;&gt;</span></span>
<span class="line"><span style="color:#a6accd">Name Servers                   : []</span></span>
<span class="line"><span style="color:#a6accd">Name Servers Search Path       : []</span></span>
<span class="line"><span style="color:#a6accd">Owners                         : ['admin']</span></span>
<span class="line"><span style="color:#a6accd">Parent Profile                 : </span></span>
<span class="line"><span style="color:#a6accd">Internal proxy                 : </span></span>
<span class="line"><span style="color:#a6accd">Red Hat Management Key         : &lt;&lt;inherit&gt;&gt;</span></span>
<span class="line"><span style="color:#a6accd">Red Hat Management Server      : &lt;&lt;inherit&gt;&gt;</span></span>
<span class="line"><span style="color:#a6accd">Repos                          : []</span></span>
<span class="line"><span style="color:#a6accd">Server Override                : &lt;&lt;inherit&gt;&gt;</span></span>
<span class="line"><span style="color:#a6accd">Template Files                 : {}</span></span>
<span class="line"><span style="color:#a6accd">Virt Auto Boot                 : 1</span></span>
<span class="line"><span style="color:#a6accd">Virt Bridge                    : xenbr0</span></span>
<span class="line"><span style="color:#a6accd">Virt CPUs                      : 1</span></span>
<span class="line"><span style="color:#a6accd">Virt Disk Driver Type          : raw</span></span>
<span class="line"><span style="color:#a6accd">Virt File Size(GB)             : 5</span></span>
<span class="line"><span style="color:#a6accd">Virt Path                      : </span></span>
<span class="line"><span style="color:#a6accd">Virt RAM (MB)                  : 512</span></span>
<span class="line"><span style="color:#a6accd">Virt Type                      : kvm</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li>使用Cobbler安装ESXi</li></ol><p>首先需要同步一下</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# cobbler sync</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li>创建虚拟机进行实验。</li></ol><h2 id="_3-3-定制化服务器安装" tabindex="-1">3.3 定制化服务器安装 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_3-3-定制化服务器安装" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h2><h3 id="_3-3-1-自动化安装流程梳理" tabindex="-1">3.3.1 自动化安装流程梳理 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_3-3-1-自动化安装流程梳理" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><ul><li>采购设备送到机房。</li><li>机房同事，开机，插电验收，验收完毕。</li><li>进行CMDB资产录入。提供机器位置和MAC地址列表（供应商也可以提供）</li><li>调用IPMI开机，Cobbler API安装操作系统。</li><li>安装完毕，启动后，自动启动SaltStack。然后剩下的以SaltStack为核心了。</li></ul><p>说明： 服务器网卡必须支持PXE功能 定制化 至少有这么几个：网络配置，Hostname 这些 注意： 如果不定制化，网卡通过PXE启动后，会出现一个菜单供人选择用哪个配置（其实就是Profile的名字）而且每个安装源 都有自己的默认KS文件，超过时间后，会指定使用默认的 KS文件 进行安装本文主要讨论 自动化一键部署，所以必须要做定制化 这部分的操作 主要通过 cobbler system 来实现</p><h3 id="_3-3-2-cobbler获取安装进度" tabindex="-1">3.3.2 Cobbler获取安装进度 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_3-3-2-cobbler获取安装进度" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>后面有章节介绍如何通过Cobbler API来调用Cobbler，在开始进行系统定制安装之前，我们可能有一个需求就是需要想知道安装进度。针对于基于Redhat的Linux发行版的系统安装程序叫做Anaconda，Cobbler提供了对Anaconda的监控，从而获取安装进度。</p><ol><li><p>开启Anaconda监控</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# vim /etc/cobbler/settings</span></span>
<span class="line"><span style="color:#a6accd">anamon_enabled: 1</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div></li></ol><p>重启Cobbler</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# systemctl restart cobblerd</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li>查看Kickstart文件确保对应的snippet存在 ``` %pre $SNIPPET('pre_anamon')</li></ol><p>%post $SNIPPET('post_anamon')</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">3. 进行系统安装后，会在/var/log/cobbler/anamon/下创建对应的System目录</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>[root@linux-node1 ~]# tree /var/log/cobbler/anamon/ /var/log/cobbler/anamon/ └── cobbler-api-test ├── anaconda.log ├── boot.log ├── dmesg ├── ks-post.log ├── ks-pre.log ├── messages ├── program.log ├── storage.log └── sys.log</p><p>1 directory, 9 files</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">### 3.3.3 Cobbler system定制安装</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">Cobbler system是Cobbler提供的可以用来进行系统定制的模块，我们可以通过Cobbler system来定制待安装服务器的IP地址、主机名等操作。</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">1. 添加一个主机</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>[root@linux-node1 ~]# cobbler system add --name=linux-node3 --mac=00:50:56:3F:84:4F \ --profile=CentOS-7.4-x86_64 --ip-address=192.168.56.110 --subnet=255.255.255.0 \ --gateway=192.168.56.2 --interface=eth0 --static=1 --hostname=<a target="_blank" rel="noreferrer" href="http://new.devopsedu.com"><!--[-->new.devopsedu.com<!--]--><!----></a> \ --name-servers="192.168.56.2" \ --kickstart=/var/lib/cobbler/kickstarts/CentOS-7.4-x86_64-Cobbler.cfg</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">- --name=linux-node3   #设置一个主机名称</span></span>
<span class="line"><span style="color:#a6accd">- --mac=08:00:27:67:A0:BB  #设置对应的MAC地址</span></span>
<span class="line"><span style="color:#a6accd">- --profile=CentOS-7.4-x86_64  #设置需要安装的操作系统</span></span>
<span class="line"><span style="color:#a6accd">- --ip-address=192.168.56.13 #设置一个静态IP地址</span></span>
<span class="line"><span style="color:#a6accd">- --subnet=255.255.255.0    #设置子网掩码</span></span>
<span class="line"><span style="color:#a6accd">- --gateway=192.168.56.2    #设置网关</span></span>
<span class="line"><span style="color:#a6accd">- --interface=eth0 \          设置网卡</span></span>
<span class="line"><span style="color:#a6accd">- --static=1                 设置静态IP</span></span>
<span class="line"><span style="color:#a6accd">- --dns-name=linux-node3.example.com  #设置dns name</span></span>
<span class="line"><span style="color:#a6accd">- --hostname=linux-node3.example.com  #设置主机名</span></span>
<span class="line"><span style="color:#a6accd">- --netboot-enabled=true #设置网络启动</span></span>
<span class="line"><span style="color:#a6accd">- --name-servers="192.168.56.2" #设置DNS域名服务器</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">2. 查看主机</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>[root@linux-node1 ~]# cobbler system list linux-node3 ``` 下面，我们来启动虚拟机，进行自动化安装，不再出现选择的菜单，直接安装你制定的操作系统。</p><h1 id="_4-cobbler高级话题" tabindex="-1">4 Cobbler高级话题 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_4-cobbler高级话题" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h1><p>在实际的工作环境中，Cobbler还有很多高级的应用可以帮助我们更快的进行操作系统的部署工作。例如网卡绑定、电源管理和Cobbler API等。</p><h2 id="_4-1-自定义物理网卡bonding" tabindex="-1">4.1 自定义物理网卡bonding <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_4-1-自定义物理网卡bonding" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h2><ol><li>添加一台主机linux-node4</li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# cobbler system add --name=linux-node4 --profile=CentOS-7.7-x86_64 \</span></span>
<span class="line"><span style="color:#a6accd"> --hostname=new.devopsedu.com \</span></span>
<span class="line"><span style="color:#a6accd"> --name-servers="192.168.56.2" \</span></span>
<span class="line"><span style="color:#a6accd"> --kickstart=/var/lib/cobbler/kickstarts/CentOS-7.7-x86_64-Cobbler.cfg</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li>添加bonding网卡</li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# cobbler system edit --name=linux-node4 --interface=eth0 \</span></span>
<span class="line"><span style="color:#a6accd"> --mac=00:50:56:27:F0:07 --interface-type=bond_slave --interface-master=bond0</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li>添加bonding网卡</li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# cobbler system edit --name=linux-node4 --interface=eth1 \</span></span>
<span class="line"><span style="color:#a6accd"> --mac=00:50:56:36:09:66 --interface-type=bond_slave --interface-master=bond0</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li>设置绑定网卡</li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# cobbler system edit --name=linux-node4 --interface=bond0 \</span></span>
<span class="line"><span style="color:#a6accd"> --interface-type=bond --bonding-opts="miimon=100 mode=1" \</span></span>
<span class="line"><span style="color:#a6accd"> --ip-address=192.168.56.111 --subnet=255.255.255.0 --gateway=192.168.56.2 --static=1</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>好的，万事具备，就差开机安装操作系统了。等等，怎么开机呢？既然是自动化安装，打电话让机房值班人员帮我们开机当然是不合适的，还记得IPMI吗，我们可以使用IPMI来进行电源管理。</p><ol><li>网卡绑定后的效果</li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@new ~]# cat /proc/net/bonding/bond0 </span></span>
<span class="line"><span style="color:#a6accd">Ethernet Channel Bonding Driver: v3.7.1 (April 27, 2011)</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">Bonding Mode: fault-tolerance (active-backup)</span></span>
<span class="line"><span style="color:#a6accd">Primary Slave: None</span></span>
<span class="line"><span style="color:#a6accd">Currently Active Slave: eth0</span></span>
<span class="line"><span style="color:#a6accd">MII Status: up</span></span>
<span class="line"><span style="color:#a6accd">MII Polling Interval (ms): 100</span></span>
<span class="line"><span style="color:#a6accd">Up Delay (ms): 0</span></span>
<span class="line"><span style="color:#a6accd">Down Delay (ms): 0</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">Slave Interface: eth0</span></span>
<span class="line"><span style="color:#a6accd">MII Status: up</span></span>
<span class="line"><span style="color:#a6accd">Speed: 1000 Mbps</span></span>
<span class="line"><span style="color:#a6accd">Duplex: full</span></span>
<span class="line"><span style="color:#a6accd">Link Failure Count: 0</span></span>
<span class="line"><span style="color:#a6accd">Permanent HW addr: 00:50:56:27:f0:07</span></span>
<span class="line"><span style="color:#a6accd">Slave queue ID: 0</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">Slave Interface: eth1</span></span>
<span class="line"><span style="color:#a6accd">MII Status: up</span></span>
<span class="line"><span style="color:#a6accd">Speed: 1000 Mbps</span></span>
<span class="line"><span style="color:#a6accd">Duplex: full</span></span>
<span class="line"><span style="color:#a6accd">Link Failure Count: 0</span></span>
<span class="line"><span style="color:#a6accd">Permanent HW addr: 00:50:56:36:09:66</span></span>
<span class="line"><span style="color:#a6accd">Slave queue ID: 0</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h2 id="_4-2-cobbler电源管理" tabindex="-1">4.2 Cobbler电源管理 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_4-2-cobbler电源管理" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h2><p>要管理服务器的电源，你可以直接使用IPMI命令进行，也可以使用cobbler间接的调用IPMI进行操作。默认情况下Cobbler使用IPMI进行电源管理，我们可以在cobbler的settings配置看到以下配置。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# vim /etc/cobbler/settings</span></span>
<span class="line"><span style="color:#a6accd">power_management_default_type: 'ipmitool'</span></span>
<span class="line"><span style="color:#a6accd">power_template_dir: "/etc/cobbler/power"</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# cobbler system edit --name=linux-node3 --power-address=192.168.0.21 --power-type='ipmilan' --power-user=ipmi --power-pass=ipmi</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>服务器电源管理：</p><ul><li>cobbler system poweroff</li><li>cobbler system poweron</li><li>cobbler system powerstatus</li><li>cobbler system reboot</li></ul><p>重启从PXE引导启动，并指定安装的操作系统配置，如下命令，</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@log-node1 ~]# cobbler system edit  --name=host-188116  --netboot-enabled=1 --profile=centos5.8-x86_64</span></span>
<span class="line"><span style="color:#a6accd">[root@log-node1 ~]# cobbler reposync</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h2 id="_4-3-cobbler-api使用" tabindex="-1">4.3 Cobbler API使用 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_4-3-cobbler-api使用" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h2><ol><li><p>连接Cobbler</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# cat cobbler.py </span></span>
<span class="line"><span style="color:#a6accd">#!/usr/bin/env python</span></span>
<span class="line"><span style="color:#a6accd">import xmlrpclib</span></span>
<span class="line"><span style="color:#a6accd">server = xmlrpclib.Server("http://192.168.56.11/cobbler_api")</span></span>
<span class="line"><span style="color:#a6accd">print server.get_distros()</span></span>
<span class="line"><span style="color:#a6accd">print server.get_profiles()</span></span>
<span class="line"><span style="color:#a6accd">print server.get_systems()</span></span>
<span class="line"><span style="color:#a6accd">print server.get_images()</span></span>
<span class="line"><span style="color:#a6accd">print server.get_repos()</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div></li><li><p>登录Cobbler</p></li></ol><p>默认情况下，不用登录Cobbler就可以获取到Cobbler的信息，但是如果需要进行操作，就需要登录。使用用户名和密码登录后，Cobbler会返回一个token令牌，后面的操作，我们调用任何一个Cobbler的方法，都需要将token作为最后一个参数。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# cat cobbler.py </span></span>
<span class="line"><span style="color:#a6accd">#!/usr/bin/env python</span></span>
<span class="line"><span style="color:#a6accd">import xmlrpclib</span></span>
<span class="line"><span style="color:#a6accd">server = xmlrpclib.Server("http://192.168.56.11/cobbler_api")</span></span>
<span class="line"><span style="color:#a6accd">print server.get_distros()</span></span>
<span class="line"><span style="color:#a6accd">print server.get_profiles()</span></span>
<span class="line"><span style="color:#a6accd">print server.get_systems()</span></span>
<span class="line"><span style="color:#a6accd">print server.get_images()</span></span>
<span class="line"><span style="color:#a6accd">print server.get_repos()</span></span>
<span class="line"><span style="color:#a6accd">token = server.login("cobbler","devopsedu.com")</span></span>
<span class="line"><span style="color:#a6accd">print(token)</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">一个添加主机的例子</span></span>
<span class="line"><span style="color:#a6accd">#!/usr/bin/env python </span></span>
<span class="line"><span style="color:#a6accd"># -*- coding: utf-8 -*-</span></span>
<span class="line"><span style="color:#a6accd">import xmlrpclib </span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">class CobblerAPI(object):</span></span>
<span class="line"><span style="color:#a6accd">    def __init__(self,url,user,password):</span></span>
<span class="line"><span style="color:#a6accd">        self.cobbler_user= user</span></span>
<span class="line"><span style="color:#a6accd">        self.cobbler_pass = password</span></span>
<span class="line"><span style="color:#a6accd">        self.cobbler_url = url</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">    def add_system(self,hostname,ip_add,mac_add,profile):</span></span>
<span class="line"><span style="color:#a6accd">        '''</span></span>
<span class="line"><span style="color:#a6accd">        Add Cobbler System Infomation</span></span>
<span class="line"><span style="color:#a6accd">        '''</span></span>
<span class="line"><span style="color:#a6accd">        ret = {</span></span>
<span class="line"><span style="color:#a6accd">            "result": True,</span></span>
<span class="line"><span style="color:#a6accd">            "comment": [],</span></span>
<span class="line"><span style="color:#a6accd">        }</span></span>
<span class="line"><span style="color:#a6accd">        #get token</span></span>
<span class="line"><span style="color:#a6accd">        remote = xmlrpclib.Server(self.cobbler_url) </span></span>
<span class="line"><span style="color:#a6accd">        token = remote.login(self.cobbler_user,self.cobbler_pass) </span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">        #add system</span></span>
<span class="line"><span style="color:#a6accd">        system_id = remote.new_system(token) </span></span>
<span class="line"><span style="color:#a6accd">        remote.modify_system(system_id,"name",hostname,token) </span></span>
<span class="line"><span style="color:#a6accd">        remote.modify_system(system_id,"hostname",hostname,token) </span></span>
<span class="line"><span style="color:#a6accd">        remote.modify_system(system_id,'modify_interface', { </span></span>
<span class="line"><span style="color:#a6accd">            "macaddress-eth0" : mac_add, </span></span>
<span class="line"><span style="color:#a6accd">            "ipaddress-eth0" : ip_add, </span></span>
<span class="line"><span style="color:#a6accd">            "dnsname-eth0" : hostname, </span></span>
<span class="line"><span style="color:#a6accd">        }, token) </span></span>
<span class="line"><span style="color:#a6accd">        remote.modify_system(system_id,"profile",profile,token) </span></span>
<span class="line"><span style="color:#a6accd">        remote.save_system(system_id, token) </span></span>
<span class="line"><span style="color:#a6accd">        try:</span></span>
<span class="line"><span style="color:#a6accd">            remote.sync(token)</span></span>
<span class="line"><span style="color:#a6accd">        except Exception as e:</span></span>
<span class="line"><span style="color:#a6accd">            ret['result'] = False</span></span>
<span class="line"><span style="color:#a6accd">            ret['comment'].append(str(e))</span></span>
<span class="line"><span style="color:#a6accd">        return ret</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">def main():</span></span>
<span class="line"><span style="color:#a6accd">    cobbler = CobblerAPI("http://192.168.56.11/cobbler_api","cobbler","devopsedu.com")</span></span>
<span class="line"><span style="color:#a6accd">    ret = cobbler.add_system(hostname='cobbler-api-test',ip_add='192.168.56.111',mac_add='00:50:56:25:C2:AA',profile='CentOS-7.4-x86_64')</span></span>
<span class="line"><span style="color:#a6accd">    print ret</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">if __name__ == '__main__':</span></span>
<span class="line"><span style="color:#a6accd">    main()</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h1 id="第二部分-kvm和openstack基础" tabindex="-1">第二部分 KVM和OpenStack基础 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#第二部分-kvm和openstack基础" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h1><h1 id="云计算概述" tabindex="-1">云计算概述 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#云计算概述" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h1><p>在第一章，会介绍什么是云计算？什么是虚拟化？以及云计算与虚拟化的关系，然后讲解目前比较流行的虚拟化项目：KVM，了解KVM虚拟化的基本概念，使用KVM创建并管理虚拟机，为学习OpenStack打下坚实的基础。</p><h2 id="云计算概述-1" tabindex="-1">云计算概述 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#云计算概述-1" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h2><p>任何事物的出现都是有背景的，云计算也不例外。云计算是由需求驱动的，首先让我们看看没有云计算之前我们都面临什么问题。</p><p><strong>传统数据中心面临的问题？</strong></p><ul><li>资源利用率低：资源利用率低是数据中心目前普遍存在的问题，多项调查数据表明企业数据中心的服务器的平均利用率普遍低于15%。</li><li>资源分配不合理：系统统建设相对独立、各自为政，其基础设施相当于一个个独立的“孤岛”，因此很难从整体的角度考虑IT基础架构的资源分配及使用的合理性。</li><li>自动化能力差：资源配置和部署过程多采用人工方式，没有相应的平台支持，使大量人力资源耗费在繁重的重复性工作上，没有自服务和自动部署的能力。</li></ul><p><strong>什么是云计算？</strong></p><p>云计算（Cloud Computing）是基于互联网的相关服务的增加、使用和交付模式。 好吧，我承认！如果你把上面那句话告诉业内人员，估计只有很少一部分人可以理解，更不用说非业内人士了。对云计算的定义有多种说法。对于到底什么是云计算,目前广为接受的是美国国家标准与技术研究院（NIST）定义：云计算是一种按使用量付费的模式，这种模式提供可用的、便捷的、按需的网络访问，进入可配置的计算资源共享池（资源包括网络，服务器，存储，应用软件，服务），这些资源能够被快速提供，只需投入很少的管理工作，或与服务供应商进行很少的交互。</p><p><strong>用比较好理解的方式解释就是：</strong></p><ol><li>是什么：云计算指的是一种模式。</li><li>使用方法：云计算的必须通过网络来使用。</li><li>特点和优势：弹性计算、按需付费、快速扩展，也就是你用多少，付多少钱。同时也不用关心基础设施的管理，由云计算供应商提供。</li></ol><h3 id="云计算的分类" tabindex="-1">云计算的分类 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#云计算的分类" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>根据云计算服务性质的不同,可以将云计算区分为公有云、私有云和混合云，如图1-1</p><p>（图1-1）</p><p><strong>公有云</strong></p><p>是将搭建好的云资源池放到Internet上，所有有使用权限的用户都可以按需使用。相对于私有云，公有云的所有者是提供商，企业用户只是具备使用权。 优势：云计算的最大优势就是其规模经济效益，大多数企业选择云计算方案是出于成本考虑，那么公有云不需要投入基础建设，可以实现按需付费，随时使用。</p><p><strong>私有云</strong> 是为一个客户单独使用而构建的一套IAAS（基础设施既服务），企业可以对数据、安全性和服务质量进行最有效控制。该公司拥有基础设施，并可以控制在此基础设施上部署应用程序的方式。私有云可部署在企业数据中心的防火墙内，也可以将它们部署在一个安全的主机托管场所。 优势：数据安全对于企业来说是至关重要的，公有云服务存在较大的安全隐患，公有云平台只适合那些非关键性业务。尤其是大型企业会更多地倾向于选择私有云计算平台。</p><p><strong>混合云</strong> 指公有云和私有云的混合，大多数是指在私有云搭建好自后，由于业务发展等原因，资源需求量超过了资源池，所以需要通过申请使用公有云作为私有云的补充。 优势：混合云既可以尽可能多地发挥云计算系统的规模经济效益，同时又可以保证数据安全性。对于不是很敏感的非关键业务可以由混合云中的公有模块实现，而对那些安全性要求较高的应用则可以迁移到私有模块实现。 公有云、私有云、混合云三种云计算模式并不会谁取代谁，谁优过谁。不同企业、不同需求，需要不同的解决方案。公有云、私有云、混合云会长期共存，优势互补，共同服务于企业用户。</p><h3 id="云计算的分层" tabindex="-1">云计算的分层 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#云计算的分层" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>如果你理解了什么是云计算，那么就需要掌握，云基础是分层的。通常情况下，我们将云计算分为三层，分别是Infrastructure（基础设施）-as-a- Service，Platform（平台）-as-a-Service，Software（软件）-as-a-Service。基础设施在最下端，平台在中间，软件在顶端。</p><p>（图1-6） IaaS(Infrastructure as a Service，基础架构即服务)通过互联网提供了数据中心、基础架构硬件和软件资源。IaaS可以提供服务器、操作系统、磁盘存储、数据库和/或信息资源。最高端IaaS的代表产品是亚马逊的AWS(Elastic Compute Cloud)，不过IBM、Vmware和惠普以及其他一些传统IT厂商也提供这类的服务。国内的话阿里云、腾讯云、青云、盛大云等代表。 IaaS通常会按照"弹性云"的模式引入其他的使用和计价模式，也就是在任何一个特定的时间，都只使用你需要的服务，并且只为之付费。我们要讲的OpenStack就是IAAS的开源项目，可以用来构建公有云或者私有云。 PaaS(Platform as a Service，平台即服务)提供了基础架构，软件开发者可以在这个基础架构之上建设新的应用，或者扩展已有的应用，同时却不必购买开发、质量控制或生产服务器。Google的App Engine和新浪的SAE都采用了PASS的模式。这些平台允许公司创建个性化的应用，也允许独立软件厂商或者其他的第三方机构针对垂直细分行业创造新的解决方案。我们要讲的Docker可以用来构建PAAS平台，百度的PAAS平台核心就是基于Docker。 SaaS(Software as a Service，软件即服务)是最为成熟、最出名，也是得到最广泛应用的一种云计算。大家可以将它理解为一种软件分布模式，在这种模式下，应用软件安装在厂商或者服务供应商那里，用户可以通过某个网络来使用这些软件，通常使用的网络是互联网。这种模式通常也被称为"随需应变(on demand)"软件，这是最成熟的云计算模式，因为这种模式具有高度的灵活性、已经证明可靠的支持服务、强大的可扩展性，因此能够降低客户的维护成本和投入，而且由于这种模式的多宗旨式的基础架构，运营成本也得以降低。Google的Gmail和Jira都是SAAS的模式。</p><h2 id="云计算与虚拟化" tabindex="-1">云计算与虚拟化 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#云计算与虚拟化" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h2><p>首先请大家不要把虚拟化和我们之前讲解的云计算进行联想，先入为主很容易让我们陷入概念的误区，让我来代理大家搞明白云计算与虚拟化的恩恩怨怨。因为提到云计算就不得不提到虚拟化，甚至说很多人将云计算和虚拟化混为一谈，这是不正确的。好的，答案有了，云计算不等于虚拟化，那么为什么呢？ 虚拟化，是指通过虚拟化技术将一台计算机虚拟为多台逻辑计算机。在一台计算机上同时运行多个逻辑计算机，每个逻辑计算机可运行不同的操作系统，并且应用程序都可以在相互独立的空间内运行而互不影响，从而显著提高计算机的工作效率。如图1.1.1所示：</p><p>（图1-1）</p><h3 id="全虚拟化和半虚拟化" tabindex="-1">全虚拟化和半虚拟化 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#全虚拟化和半虚拟化" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>虚拟化技术根据特点，可以分为全虚拟化和半虚拟化。 全虚拟化（Full Virtualization)， 是原始虚拟化技术，该模型使用虚拟机协调guest操作系统和原始硬件，VMM（Virtual Machine Monitor）在GuestOS和裸硬件之间用于工作协调，一些受保护指令必须由Hypervisor（虚拟机管理程序）来捕获处理，如图1-2。</p><p>（图1-2） KVM是全虚拟化的典型代表，KVM是集成到Linux内核的Hypervisor，是X86架构且硬件支持虚拟化技术（Intel VT或AMD-V）的Linux的全虚拟化解决方案。它是Linux的一个很小的模块，利用Linux做大量的事，如任务调度、内存管理与硬件设备交互等，如图1-3。</p><p>半虚拟化（Para Virtualization），是另一种类似于全虚拟化的技术，它使用Hypervisor分享存取底层的硬件，但是它的guest操作系统集成了虚拟化方面的代码。该方法无需重新编译或引起陷阱，因为操作系统自身能够与虚拟进程进行很好的协作。半虚拟化需要guest操作系统做一些修改，使guest操作系统意识到自己是处于虚拟化环境的，但是半虚拟化提供了与原操作系统相近的性能，如图1-4。</p><p>（图1-4） 半虚拟化的一个典型代表是Xen。Xen是第一类运行在裸机上的虚拟化管理程序(Hypervisor)。它支持全虚拟化和半虚拟化,Xen支持hypervisor和虚拟机互相通讯。Xen最重要的优势在于半虚拟化，此外未经修改的操作系统也可以直接在xen上运行(如Windows)，能让虚拟机有效运行而不需要仿真，因此虚拟机能感知到hypervisor，而不需要模拟虚拟硬件，从而能实现高性能，如图1-5。</p><p>（图1-5） 在Xen环境中，主要有两个组成部分。 一个是虚拟机监控器（VMM），也叫hypervisor。Hypervisor层硬件与虚拟机之间，最先被载入到硬件的第一层。 Hypervisor载入就可部署虚拟机。在Xen中，虚拟机叫做Domain。在这些虚拟机中，Domain0具有很高的特权,负责一些专门的工作。由于hypervisor中不包含任何与硬件对话的驱动，也没有与管理员对话的接口，这些驱动就由 domain0来提供了。通过domain0，管理员可以利用一些Xen工具来创建其它虚拟机（DomainU）。这些domainU属于无特权domain。</p><h3 id="虚拟化分类" tabindex="-1">虚拟化分类 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#虚拟化分类" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>刚才从类型中把虚拟化分为全虚拟化和半虚拟化，从使用场景中进行区分，虚拟化可以分为服务器虚拟化、桌面虚拟化和应用虚拟化。更详细的根据资源的不同还提出了网络虚拟化、存储虚拟化等。 服务器虚拟化是我们最常见的方式，我们通过KVM、Xen、VMWARE ESXi等软件将我们的服务器虚拟成多个来使用，提高硬件的使用效率。不熟悉的朋友可以使用搜索引擎来获取相关知识，目前我的生产环境还稳定的运行着数年前的ESXi。</p><p><strong>桌面虚拟</strong></p><p>化是指将计算机的终端系统（也称作桌面）进行虚拟化，以达到桌面使用的安全性和灵活性。可以通过任何设备，在任何地点，任何时间通过网络访问属于我们个人的桌面系统。目前在中小企业并没有完全普及，主要用于IT外包、呼叫中心、移动桌面等，因为在显示方面对于低成本的解决方案效果并不好，效果好的初始采购成本较高。 桌面虚拟化依赖于服务器虚拟化，首先在数据中心的服务器上进行服务器虚拟化，生成大量的独立的桌面操作系统（比如Win7），同时根据专有的虚拟桌面协议发送给终端设备（例如RDP、VNC、Spice协议）。用户终端通过以太网登陆到虚拟主机上，只需要记住用户名和密码及网关信息，即可随时随地的通过网络访问自己的桌面系统，从而实现单机多用户。 通过与IAAS的结合，桌面虚拟化也演变成桌面云（DAAS ：Desktop As a Service）.IAAS提供基础资源平台，桌面虚拟化和云平台的完美融合达到类似于SAAS一样的效果，这便是DAAS。</p><p><strong>应用虚拟化</strong></p><p>比较简单的解释就是通过虚拟化软件实现传统C/S结构的软件通过Web浏览器进行交付。在用户访问一个服务器虚拟化后的应用时，用户计算机只需要把人机交互逻辑传送到服务器端，服务器端为用户开设独立的会话空间，应用程序的计算逻辑在这个会话空间中运行，把变化后的人机交互逻辑传送给客户端，并且在客户端相应设备展示出来，从而使用户获得如同运行本地应用程序一样的访问感受。比如我曾经使用过Cirtrix公司的Xenapp这款应用虚拟化产品，实现ERP客户端的浏览器交付。 经过上面的理解，我们应该可以看出，从表面来看虚拟化是一种技术，那么云计算是一个概念。云计算里面包含了很多的技术，也包括虚拟化技术。云计算是和服务相关的，虚拟化是相对于物理设备的。所以说虚拟化绝不是云计算。而云计算则远远超出了虚拟化的范畴。</p><h3 id="云计算不等于虚拟化" tabindex="-1">云计算不等于虚拟化 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#云计算不等于虚拟化" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>经过之前对云计算和虚拟化的学习，我们可以直接得出答案：云计算不等于虚拟化，云计算实质上是根据需要通过 Internet 交付共享计算资源。云计算可以通过虚拟化来实现，千万不要以为云计算就是虚拟化。我们再总结一下，云计算是一种资源使用和交付方式，虚拟化是一种具体的技术实现。但是呢，云计算的具体实现需要使用到虚拟化这项技术。</p><h1 id="_2-kvm虚拟化实战" tabindex="-1">2 KVM虚拟化实战 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_2-kvm虚拟化实战" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h1><p>可翻阅www.xoxoyun.cn博客其他KVM文章进行学习</p><h1 id="_3-openstack入门实战" tabindex="-1">3 OpenStack入门实战 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_3-openstack入门实战" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h1><p>在OpenStack入门实战章节，我们首先会介绍OpenStack的历史、架构。然后通过部署一个两个节点的OpenStack集群来学习OpenStack的基础组建。包括</p><ul><li>OpenStack共享服务MySQL、RabbitMQ、Memcached</li><li>OpenStack验证服务Keystone</li><li>OpenStack镜像服务Glance</li><li>OpenStack计算服务Nova</li><li>OpenSTack网络服务Neutron</li><li>OpenStack管理节目Horizon</li></ul><h1 id="_1-1-openstack介绍" tabindex="-1">1.1 OpenStack介绍 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_1-1-openstack介绍" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h1><p>OpenStack 是由 Rackspace 和 NASA（美国宇航局） 共同开发的云计算平台，是云计算中IAAS的开源实现。通过Apache许可证授权开放源码，它可以帮助服务商和企业实现类似于Amazon EC2和S3的云基础架构服务。</p><p>OpenStack是一个云平台管理的项目，它不是一个软件。OpenStack是一个可以管理整个数据中心里大量资源池的云操作系统，包括计算、存储及网络资源。管理员可以通过管理台管理整个系统，并可以通过web接口为用户划定资源。由以上可以知道OpenStack的主要目标是管理数据中心的资源，简化资源分派。它管理三部分资源，分别是：</p><ul><li><p>计算资源：OpenStack可以规划并管理大量云主机，从而允许企业或服务提供商按需提供计算资源；开发者可以通过API访问计算资源从而创建云应用，管理员与用户则可以通过web访问这些资源；</p></li><li><p>存储资源：OpenStack可以为云服务或云应用提供所需的对象及块存储资源；因对性能及价格有需求，很多组织已经不能满足于传统的企业级存储技术，因此OpenStack可以根据用户需要提供可配置的对象存储或块存储功能；</p></li><li><p>网络资源：如今的数据中心存在大量的设置，如服务器、网络设备、存储设备、安全设备而它们还将被划分成更多的虚拟设备或虚拟网络；这会导致IP地址的数量、路由配置、安全规则将爆炸式增长；传统的网络管理技术无法真正的可高扩展、高自动化地管理下一代网络；因而OpenStack提供了插件式、可扩展、API驱动型的网络及IP管理； OpenStack通过整合相关的一组服务，提供了基础设施即服务（IaaS）的解决方案。每个服务提供了一组应用程序接口（API）来促进他们之间的整合。你可以根据您的需要，选择安装这些服务中的一些或全部。</p></li></ul><h1 id="_3-2-openstack环境准备" tabindex="-1">3.2 OpenStack环境准备 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_3-2-openstack环境准备" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h1><h3 id="_3-2-1-openstack实战案例" tabindex="-1">3.2.1 OpenStack实战案例 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_3-2-1-openstack实战案例" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>从本章开始，开始介绍OpenStack的各个组件，为了让读者更直观的了解组件的作用和组件之间的依赖关系，将通过理论和实践部署相结合的方式进行讲解。 请参考《实验环境》来完成本实例的环境准备工作。</p><p>下图是我们快速学习OpenStack的实战架构图，目标是先以最小组件依赖运行一个OpenStack平台，然后再后面的章节添加其它组件。</p><p>注意：在后面的实验过程中，我们会使用到控制节点部署和计算节点部署这样的描述方法，请到对应的主机上进行操作。</p><h3 id="_3-2-2-ntp时间同步" tabindex="-1">3.2.2 NTP时间同步 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_3-2-2-ntp时间同步" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>各个服务器的时间同步是OpenStack部署和运维中往往被忽视的问题。如果各个节点时间不同步，会影响OpenStack平台的正常。所以需要保证所有节点时间同步。 在CentOS 7推荐使用Chrony来进行时间同步。 Chrony是一个开源的自由软件，它能帮助你保持系统时钟与时钟服务器（NTP）同步，因此让你的时间保持精确。它由两个程序组成，分别是chronyd和chronyc。chronyd是一个后台运行的守护进程，用于调整内核中运行的系统时钟和时钟服务器同步。它确定计算机增减时间的比率，并对此进行补偿。chronyc提供了一个用户界面，用于监控性能并进行多样化的配置。它可以在chronyd实例控制的计算机上工作，也可以在一台不同的远程计算机上工作。</p><p><strong>控制节点部署</strong> 本实例中，我们把控制节点的chrony作为内部的时钟服务器，其它节点都同步控制节点的时间，如果你的环境中已经存在时间服务器可以设置为已经存在的时间服务器。</p><ol><li><p>安装chrony</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# yum install -y chrony</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div></li><li><p>配置chrony</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# vim /etc/chrony.conf</span></span>
<span class="line"><span style="color:#a6accd">#设置时钟服务器，可以设置多个，这里推荐使用阿里云的公共NTP时间服务器</span></span>
<span class="line"><span style="color:#a6accd">server time1.aliyun.com iburst</span></span>
<span class="line"><span style="color:#a6accd">server time2.aliyun.com iburst</span></span>
<span class="line"><span style="color:#a6accd">server time3.aliyun.com iburst</span></span>
<span class="line"><span style="color:#a6accd">server time4.aliyun.com iburst</span></span>
<span class="line"><span style="color:#a6accd">#设置允许192.168.0.0/16网段连接</span></span>
<span class="line"><span style="color:#a6accd">allow 192.168.56.0/24</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div></li><li><p>启动 NTP 服务并将其配置为开机启动：</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# systemctl enable chronyd.service</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# systemctl start chronyd.service</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div></li></ol><p><strong>其它节点部署</strong></p><ol><li><p>安装chrony</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node2 ~]# yum install -y chrony</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div></li><li><p>配置chrony</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node2 ~]# vim /etc/chrony.conf</span></span>
<span class="line"><span style="color:#a6accd">#设置为控制节点的IP地址，并将其它配置全部删除。</span></span>
<span class="line"><span style="color:#a6accd">server 192.168.56.11 iburst</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div></li><li><p>启动 NTP 服务并将其配置为开机启动：</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node2 ~]# systemctl enable chronyd.service</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node2 ~]# systemctl start chronyd.service</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div></li></ol><p><strong>验证同步时间</strong></p><p>在同时时间之前要确保你的时区是正确的。CentOS7提供了timedatectl用来修改。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# timedatectl set-timezone Asia/Shanghai</span></span>
<span class="line"><span style="color:#a6accd">在所有节点上执行chronyc source保证配置都正确</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# chronyc sources</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h3 id="_3-2-3-基础软件包安装" tabindex="-1">3.2.3 基础软件包安装 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_3-2-3-基础软件包安装" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>基础软件包需要在所有的OpenStack节点上进行安装，包括控制节点和计算节点。</p><ol><li>安装EPEL仓库</li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd"># rpm -ivh http://mirrors.aliyun.com/epel/epel-release-latest-7.noarch.rpm</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li>安装OpenStack仓库</li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd"># yum install -y centos-release-openstack-train</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li>安装OpenStack客户端</li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd"># yum install -y python-openstackclient</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><blockquote><p>注意：本文的实验环境是关闭SELinux，如果你启用了，需要安装openstack-selinux 包实现对OpenStac服务的安全策略进行自动管理。</p></blockquote><ol><li>安装openstack SELinux管理包</li></ol><p>虽然我们在实验环境准备中，已经要求环境中关闭SELinux，但是为了以防万一，你还是需要安装上openstack-selinux。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd"># yum install -y openstack-selinux</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>OpenStack源码包</strong></p><p>本案例使用的是yum安装，如果你想使用源码进行OpenStack部署和开发，目前OpenStack所有的源码包，都可以在这里找到<a target="_blank" rel="noreferrer" href="http://tarballs.openstack.org/%EF%BC%8C%E5%90%8C%E6%97%B6%E4%B9%9F%E5%8F%AF%E4%BB%A5%E8%AE%BF%E9%97%AEhttps://launchpad.net/openstack/%E8%8E%B7%E5%8F%96%E5%88%B0%E6%AF%8F%E4%B8%AA%E9%A1%B9%E7%9B%AE%E7%9A%84%E8%AF%A6%E6%83%85%E5%92%8C%E4%B8%8B%E8%BD%BD%E5%9C%B0%E5%9D%80%E3%80%82"><!--[-->http://tarballs.openstack.org/，同时也可以访问https://launchpad.net/openstack/获取到每个项目的详情和下载地址。<!--]--><!----></a></p><h3 id="_3-2-4-mysql数据库部署" tabindex="-1">3.2.4 MySQL数据库部署 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_3-2-4-mysql数据库部署" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>数据库是OpenStack的基础组件之一，OpenStack的大多数组件如KeyStone、Glance、Nova、Neutron和Cinder都需要使用数据库存储数据，包括Horizon也可以使用数据库来进行Session的存储。因此数据库的可用率会直接影响到整个OpenStack平台的可用率。如果数据库故障，OpenStack平台将无法创建新的虚拟机和管理虚拟机，但是不不会影响到正在运行的虚拟机。 OpenStack支持MySQL、PostgreSQL等多种数据库，需要注意的是在CentOS7上默认不是MySQL，而是mariadb。 你可以将MySQL Server安装在任意的服务器上，当然最好是专用的数据库服务器，使用更多的CPU和IO性能更好的硬盘。 注：生产环境可以咨询DBA进行MySQL的性能优化和高可用的配置。</p><ol><li>MySQL安装</li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# yum install -y mariadb mariadb-server python2-PyMySQL</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li><p>修改MySQL配置文件</p><p>默认情况下MySQL已经包含了/etc/my.cnf.d的目录，我们需要把配置放在该目录下即可。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# vim /etc/my.cnf.d/openstack.cnf</span></span>
<span class="line"><span style="color:#a6accd">[mysqld]</span></span>
<span class="line"><span style="color:#a6accd">bind-address = 192.168.56.11 #设置监听的IP地址</span></span>
<span class="line"><span style="color:#a6accd">default-storage-engine = innodb  #设置默认的存储引擎</span></span>
<span class="line"><span style="color:#a6accd">innodb_file_per_table = on#使用独享表空间</span></span>
<span class="line"><span style="color:#a6accd">collation-server = utf8_general_ci #服务器的默认校对规则</span></span>
<span class="line"><span style="color:#a6accd">character-set-server = utf8 #服务器安装时指定的默认字符集设定</span></span>
<span class="line"><span style="color:#a6accd">max_connections = 4096 #设置MySQL的最大连接数，生产请根据实际情况设置。</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div></li><li><p>启动MySQL Server并设置开机启动</p></li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# systemctl enable mariadb.service </span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# systemctl start mariadb.service</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li>进行数据库安全设置</li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# mysql_secure_installation</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li>数据库创建</li></ol><p>在部署完MySQL数据后，可以将各个服务的数据库创建上，给后期安装使用。注意在实际生产环境中，一定要使用复杂度更高的密码和做好访问控制，这里实验使用用户名和密码相同。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# mysql -u root -p</span></span>
<span class="line"><span style="color:#a6accd">Enter password: </span></span>
<span class="line"><span style="color:#a6accd">…</span></span>
<span class="line"><span style="color:#a6accd">MariaDB [(none)]&gt;</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">#Keystone数据库</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">CREATE DATABASE keystone;</span></span>
<span class="line"><span style="color:#a6accd">GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'localhost' IDENTIFIED BY 'keystone';</span></span>
<span class="line"><span style="color:#a6accd">GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'%' IDENTIFIED BY 'keystone';</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">#Glance数据库</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">CREATE DATABASE glance;</span></span>
<span class="line"><span style="color:#a6accd">GRANT ALL PRIVILEGES ON glance.* TO 'glance'@'localhost' IDENTIFIED BY 'glance';</span></span>
<span class="line"><span style="color:#a6accd">GRANT ALL PRIVILEGES ON glance.* TO 'glance'@'%' IDENTIFIED BY 'glance';</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">#Placement数据库</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">CREATE DATABASE placement;</span></span>
<span class="line"><span style="color:#a6accd">GRANT ALL PRIVILEGES ON placement.* TO placement@'localhost' IDENTIFIED BY 'placement';</span></span>
<span class="line"><span style="color:#a6accd">GRANT ALL PRIVILEGES ON placement.* TO placement@'%' IDENTIFIED BY 'placement';</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">#Nova数据库</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">CREATE DATABASE nova;</span></span>
<span class="line"><span style="color:#a6accd">GRANT ALL PRIVILEGES ON nova.* TO 'nova'@'localhost' IDENTIFIED BY 'nova';</span></span>
<span class="line"><span style="color:#a6accd">GRANT ALL PRIVILEGES ON nova.* TO 'nova'@'%' IDENTIFIED BY 'nova';</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">CREATE DATABASE nova_api;</span></span>
<span class="line"><span style="color:#a6accd">GRANT ALL PRIVILEGES ON nova_api.* TO 'nova'@'localhost' IDENTIFIED BY 'nova';</span></span>
<span class="line"><span style="color:#a6accd">GRANT ALL PRIVILEGES ON nova_api.* TO 'nova'@'%' IDENTIFIED BY 'nova';</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">CREATE DATABASE nova_cell0;</span></span>
<span class="line"><span style="color:#a6accd">GRANT ALL PRIVILEGES ON nova_cell0.* TO 'nova'@'localhost' IDENTIFIED BY 'nova';</span></span>
<span class="line"><span style="color:#a6accd">GRANT ALL PRIVILEGES ON nova_cell0.* TO 'nova'@'%' IDENTIFIED BY 'nova';</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">#Neutron 数据库</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">CREATE DATABASE neutron;</span></span>
<span class="line"><span style="color:#a6accd">GRANT ALL PRIVILEGES ON neutron.* TO 'neutron'@'localhost' IDENTIFIED BY 'neutron';</span></span>
<span class="line"><span style="color:#a6accd">GRANT ALL PRIVILEGES ON neutron.* TO 'neutron'@'%' IDENTIFIED BY 'neutron';</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">#Cinder数据库</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">CREATE DATABASE cinder;</span></span>
<span class="line"><span style="color:#a6accd">GRANT ALL PRIVILEGES ON cinder.* TO 'cinder'@'localhost' IDENTIFIED BY 'cinder';</span></span>
<span class="line"><span style="color:#a6accd">GRANT ALL PRIVILEGES ON cinder.* TO 'cinder'@'%' IDENTIFIED BY 'cinder';</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>数据库部署完毕后，后期的很多操作都可以通过命令，甚至直接修改数据库的数据来进行。注意：本实例为了简化步骤，使用了较为简单的密码，生产环境请设置符合运维规范的复杂密码来进行替换。 注意：一定要给数据库做好相关的高可用和备份措施，像对待生成业务数据库一样对待它哦。</p><h3 id="消息代理rabbitmq" tabindex="-1">消息代理RabbitMQ <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#消息代理rabbitmq" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>OpenStack 使用（Message broker）消息代理在各个服务之间进行协调和状态管理。OpenStack支持三种开源的消息队列服务。分别为RabbitMQ、Qpid、ZeroMQ。Openstack默认使用RabbitMQ。 消息队列在OpenStack整个架构中扮演着交通枢纽的作用，正是因为OpenStack部署的灵活性、各个组件的松耦合、架构的扁平化，反而使得OpenStack更加依赖于消息队列。OpenStack的控制、计算、网络与存储服务均需要通过消息队列进行通信。 你可以将RabbitMQ部署在独立的服务器中，或者使用RabbitMQ集群这取决于你的实际情况，在实验环境，部署在控制节点即可。</p><ol><li>安装RabbitMQ</li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# yum install -y rabbitmq-server</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li>设置开启启动，并启动RabbitMQ</li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# systemctl enable rabbitmq-server.service</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# systemctl start rabbitmq-server.service</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li>添加openstack用户。</li></ol><p>创建一个openstack用户，密码为openstack。注意实际使用中进行密码修改，这里设置的用户名和密码在后面配置OpenStack组件的时候需要在配置文件里面设置。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# rabbitmqctl add_user openstack openstack</span></span>
<span class="line"><span style="color:#a6accd">Creating user "openstack" ...</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li>给刚才创建的openstack用户，创建权限。</li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# rabbitmqctl set_permissions openstack ".*" ".*" ".*"</span></span>
<span class="line"><span style="color:#a6accd">Setting permissions for user "openstack" in vhost "/" ...</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li>启用Web监控插件</li></ol><p>RabbitMQ自带了一个Web监控插件，可以通过Web界面监控RabbitMQ的运行状态。同时也提供了HTTP API。可以方便的集成到Nagios、Zabbix等监控平台上。 Web监控插件启用后就可以通过<a target="_blank" rel="noreferrer" href="http://ip:15672/%E6%9D%A5%E8%AE%BF%E9%97%AEweb%E7%AE%A1%E7%90%86%E7%95%8C%E9%9D%A2%E3%80%82"><!--[-->http://IP:15672/来访问web管理界面。<!--]--><!----></a></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# rabbitmq-plugins list</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# rabbitmq-plugins enable rabbitmq_management</span></span>
<span class="line"><span style="color:#a6accd">（注：如果主机名不能解析，rabbitMQ将无法启动。在生产应用时建议设置为集群模式，建议三个节点。1个硬盘节点、两个内存节点。）</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# lsof -i:15672</span></span>
<span class="line"><span style="color:#a6accd">COMMAND  PID     USER   FD   TYPE DEVICE SIZE/OFF NODE NAME</span></span>
<span class="line"><span style="color:#a6accd">beam    2620 rabbitmq   15u  IPv4  16805      0t0  TCP *:15672 (LISTEN)</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>默认情况下RabbitMQ服务使用5672端口，而Web管理插件监听15672端口，直接在浏览器输入<a target="_blank" rel="noreferrer" href="http://192.168.56.11:15672/"><!--[-->http://192.168.56.11:15672<!--]--><!----></a></p><blockquote><p>RabbitMQ默认的用户名和密码均为guest。之前创建的openstack的用户是无法通过Web界面登录的。</p></blockquote><p>（图2.2.2）</p><blockquote><p>实践经验：消息队列收发消息的性能和消息队列的HA能力直接影响OpenStack的性能和高可用。如果消息队列Down机，OpenStack将无法创建新的云主机，以及进行其它的管理工作。所以在企业生产环境中，需要使用RabbitMQ的集群来保证可用性。同时做好监控工作，RabbitMQ自带HTTP的API，便于集成到企业的监控平台上。</p></blockquote><h1 id="_1-3-openstack验证服务keystone" tabindex="-1">1.3 OpenStack验证服务KeyStone <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_1-3-openstack验证服务keystone" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h1><p>Keystone（OpenStack Identity Service）是OpenStack框架中，负责身份验证、服务规则和服务令牌的功能，它实现了OpenStack的Identity API。</p><p>Keystone类似一个服务总线， 或者说是整个OpenStack框架的注册中心，其他服务通过keystone来注册其服务的Endpoint（服务访问的URL），任何服务之间相互的调用，需要经过Keystone的身份验证，来获得目标服务的Endpoint来找到目标服务。</p><h3 id="keystone概述" tabindex="-1">Keystone概述 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#keystone概述" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>Keystone提供了以下两个主要的功能：</p><ul><li>用户与认证:用户权限与用户行为跟踪；</li><li>服务目录:提供一个服务目录，包括所有服务项与相关Api的端点。</li></ul><p>下面我们从这两个主要功能来了解Keystone的概念，和Keystone在Openstack中的作用。</p><ol><li><p>Keystone用户与认证</p><p>Keystone为各个Openstack组件提供用户和认证服务，各个组件都必须与Keystone进行交互。比如登陆认证是用户在访问Openstack各个组件中的API时，必须通过Keystone的用户名和密码验证，最终通过Kyestone验证获取token，完成对用户的登陆认证。如果认证失败，该用户将不能访问该API。</p></li></ol><p><strong>User</strong></p><p>User即用户，它是用一个数字代表使用OpenStack云服务的一个人、系统、或服务。身份验证服务将会验证传入的由用户声明将调用的请求。用户如果已经登录，可就能分配令牌（tokens）访问资源。 用户可能被直接分配给特定的租户，用户行为被包含在租户（tenant），Users通过认证信息（credentials，如密码、API Keys等）进行验证。</p><p><strong>Credentials</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">Credentials即证书，用户可以通过身份验证，保证数据通常只被一个用户属于或拥有(因为没有其他人应该知道数据)。</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>以下为证书的例子: 一个匹配的用户名和密码 一个匹配的用户名和API键 一个身份认证令牌</p><p><strong>Authentication</strong></p><p>Authentication即认证，认证是确认身份或识别真实性的行为。身份服务通过与用户的一组声明验证以确认传入的请求是由该用户声明过的。这些声明被初始化为一组证书 (username&amp;password, or username and API key)。初始确认后，身份服务将会给用户发布一个确认身份的令牌(Token)，在用户随后的请求中可以使用这个令牌授权访问。</p><p><strong>Token：</strong></p><p>Token即令牌，是一个用户访问资源的任意文本，每一个令牌都有一个范围，描述了可以访问哪些资源。令牌可能随时撤消，是有有效期的。身份认证是支持令牌认证的，它的目的为在未来支持额外的协议。其目的是为了让它首先成为一种集成服务，而不是一种渴望成为丰富的身份存储和管理的解决方案。 T <strong>Tenant：</strong></p><p>Tenant即租户，它是各个服务中的一些可以访问的资源集合。它是一个容器，用于组织和隔离资源，或标识对象。一个租户可以一个客户、账户、组织、项目的映射。例如，在Nova中一个tenant可以是一些机器，在Swift和Glance中一个tenant可以是一些镜像存储，在Neutron中一个tenant可以是一些网络资源。Users默认的总是绑定到某些tenant上。</p><p><strong>Role：</strong></p><p>Role即角色，Roles代表一组用户可以访问的资源权限，例如Nova中的虚拟机、Glance中的镜像。Users可以被添加到任意一个全局的或租户内的角色中。在全局的role中，用户的role权限作用于所有的租户，即可以对所有的租户执行role规定的权限；在租户内的role中，用户仅能在当前租户内执行role规定的权限。</p><p><strong>Domain：</strong></p><p>在KeyStone V3版本将 Tenant 改为 Project 并在其上添加 Domain 的概念，这更加符合现实世界和云服务的映射。利用 Domain 实现真正的多租户（multi-tenancy）架构，Domain 担任 Project 的高层容器。云服务的客户是 Domain 的所有者，他们可以在自己的 Domain 中创建多个 Projects、Users、Groups 和 Roles。通过引入 Domain，云服务客户可以对其拥有的多个 Project 进行统一管理，而不必再向过去那样对每一个 Project 进行单独管理。</p><p><strong>Group：</strong></p><p>Group 是一组 Users 的容器，可以向 Group 中添加用户，并直接给 Group 分配角色，那么在这个 Group 中的所有用户就都拥有了 Group 所拥有的角色权限。通过引入 Group 的概念，Keystone V3 实现了对用户组的管理，达到了同时管理一组用户权限的目的。这与 V2 中直接向 User/Project 指定 Role 不同，使得对云服务进行管理更加便捷。 Domain、Group、Project、User 和 Role 的关系图</p><p>如图 2 所示，在一个 Domain 中包含 3 个 Projects,可以通过 Group1 将 Role Sysadmin直接赋予 Domain,那么 Group1 中的所有用户将会对 Domain 中的所有 Projects 都拥有管理员权限。也可以通过 Group2 将 Role Engineer 只赋予 Project3,这样 Group2 中的 User 就只拥有对 Project3 相应的权限，而不会影响其它 Projects。</p><ol><li><p>Keystone服务目录</p><p>上文说过，Keystone除了用户管理相关的支撑，还有一个重要的作用是一个服务目录。KeyStone为Openstack各个服务提供了一个REST API端点列表。</p></li><li><p>Service Service即服务，如Nova、Glance、Swift。根据前三个概念（User，Tenant和Role）一个服务可以确认当前用户是否具有访问其资源的权限。但是当一个user尝试着访问其租户内的service时，他必须知道这个service是否存在以及如何访问这个service，这里通常使用一些不同的名称表示不同的服务。在上文中谈到的Role，实际上也是可以绑定到某个service的。</p></li><li><p>Endpoint Endpoint，翻译为“端点”，我们可以理解它是一个服务暴露出来的访问点，如果需要访问一个服务，则必须知道他的endpoint。因此，在keystone中包含一个endpoint模板（endpoint template，在安装keystone的时候我们可以在conf文件夹下看到这个文件），这个模板提供了所有存在的服务endpoints信息。一个endpoint template包含一个URLs列表，列表中的每个URL都对应一个服务实例的访问地址，并且具有public、private和admin这三种权限。public url可以被全局访问（如<a target="_blank" rel="noreferrer" href="http://compute.example.com"><!--[-->http://compute.example.com），private<!--]--><!----></a>%2Cprivate/) url只能被局域网访问（如<a target="_blank" rel="noreferrer" href="http://compute.example.local"><!--[-->http://compute.example.local），admin<!--]--><!----></a>%2Cadmin/) url被从常规的访问中分离。</p></li></ol><h3 id="keystone部署" tabindex="-1">KeyStone部署 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#keystone部署" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>OpenStack中所有服务的安装可以说是最简单的。复杂的就在于本身的配置文件的修改和与Keystone相关的配置。还记得上面说的，OpenStack的每个组件如果能够使用都必须在Keystone创建Service和Endpoint吗？好的，我们开始。</p><ol><li>安装keystone</li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# yum install -y openstack-keystone httpd mod_wsgi memcached python-memcached</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li>设置Memcache开启启动并启动Memcached</li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">#keystone使用Memcached来缓存验证后生成的token。</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# systemctl enable memcached.service</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# vim /etc/sysconfig/memcached</span></span>
<span class="line"><span style="color:#a6accd">PORT="11211"</span></span>
<span class="line"><span style="color:#a6accd">USER="memcached"</span></span>
<span class="line"><span style="color:#a6accd">MAXCONN="1024"</span></span>
<span class="line"><span style="color:#a6accd">CACHESIZE="64"</span></span>
<span class="line"><span style="color:#a6accd">OPTIONS="-l 192.168.56.11,::1"</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# systemctl start memcached.service</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li>Keystone配置</li></ol><p>KeyStone的配置只需要修改/etc/keystone/keystone.conf即可，涉及Admin Token和Mysql连接，其它均使用默认配置即可。所有配置文件都存放到/etc/keystone（项目名）目录下。日志都存放在/var/log/keystone（项目名）目录下，其它项目也是如此。 好的，我们暂停一下，在后面的部署中，我们会使用这样的方式列出配置，未列出的配置，保持默认不进行修改，除非特别提示到。另外要注意配置的位置，OpenStack所有服务都会有[DEFAULT] [dtabase] 这样的配置部分，请在文中列出的配置部分里面进行配置，切记！。因为确实有相同的配置存在在不同的配置端的情况，这样例外情况，如果没有按要求配置，可能就无法正常运行。</p><ul><li><p>配置KeyStone数据库</p><p>OpenStack中所有服务的数据库同步都需要依赖与配置文件中的数据库连接的配置，所以必须先修改完毕配置文件后，再进行数据库的初始化操作。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# vim /etc/keystone/keystone.conf</span></span>
<span class="line"><span style="color:#a6accd">[database]</span></span>
<span class="line"><span style="color:#a6accd">connection = mysql+pymysql://keystone:keystone@192.168.56.11/keystone</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div></li><li><p>设置Token和Memcached</p></li></ul><p>在之前的KeyStone版本由于所有的API通信都要请求Keystone来生产Token，之前是存放在MySQL表里，导致表特别大，经常需要手动清理，而且性能下降，现在新的版本我们可以存放在memcache里面了。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[token]</span></span>
<span class="line"><span style="color:#a6accd">provider = fernet</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ul><li>同步数据库：</li></ul><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# su -s /bin/sh -c "keystone-manage db_sync" keystone</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><blockquote><p>小提示：为什么要使用keystone用户来同步数据呢，因为同步操作会生产日志/var/log/keystone/keystone.log，如果使用root用户，那么生成的文件权限为root用户，keystone启动的时候没有权限读取该文件，会无法启动。如果你的同步执行有问题，那么可以查看日志获取详细的错误信息。</p></blockquote><ul><li>验证数据库创建是否正常：</li></ul><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# mysql -h 192.168.56.11 -ukeystone -pkeystone -e "use keystone;show tables;"</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li><p>初始化fernet keys</p><p>生成完毕之后，会创建/etc/keystone/fernet-keys目录</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# keystone-manage fernet_setup --keystone-user keystone --keystone-group keystone</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# keystone-manage credential_setup --keystone-user keystone --keystone-group keystone</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div></li><li><p>初始化keystone</p></li></ol><p>还记得上面说的Openstack的每个组件都必须在Keystone上进行注册。当然也包括Keystone本身。身份认证服务管理了一个与您环境相关的 API 端点的目录。服务使用这个目录来决定如何与您环境中的其他服务进行通信。 OpenStack使用三个API端点变种代表每种服务：admin，internal和public。默认情况下，管理API端点允许修改用户和租户而公共和内部APIs不允许这些操作。在生产环境中，处于安全原因，变种为了服务不同类型的用户可能驻留在单独的网络上。 对实例而言，公共API网络为了让顾客管理他们自己的云在互联网上是可见的。管理API网络在管理云基础设施的组织中操作也是有所限制的。内部API网络可能会被限制在包含OpenStack服务的主机上。此外，OpenStack支持可伸缩性的多区域。为了简单起见，我们这里均使用一个IP地址。192.168.56.11。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# keystone-manage bootstrap --bootstrap-password admin \</span></span>
<span class="line"><span style="color:#a6accd"> --bootstrap-admin-url http://192.168.56.11:5000/v3/ \</span></span>
<span class="line"><span style="color:#a6accd"> --bootstrap-internal-url http://192.168.56.11:5000/v3/ \</span></span>
<span class="line"><span style="color:#a6accd"> --bootstrap-public-url http://192.168.56.11:5000/v3/ \</span></span>
<span class="line"><span style="color:#a6accd"> --bootstrap-region-id RegionOne</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li>验证Keystone配置</li></ol><p>下面我将上面进行的配置列出来，供读者进行比对，配置应该和本文保持一致。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# grep "^[a-z]" /etc/keystone/keystone.conf </span></span>
<span class="line"><span style="color:#a6accd">connection = mysql+pymysql://keystone:keystone@192.168.56.11/keystone</span></span>
<span class="line"><span style="color:#a6accd">provider = fernet</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li>KeyStone启动</li></ol><p>OpenStack Keystone使用Apache来进行启动，提高了性能。 配置ServerName，注意一定要配置，不然会有奇怪的问题出现。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# vim /etc/httpd/conf/httpd.conf</span></span>
<span class="line"><span style="color:#a6accd">ServerName 192.168.56.11:80</span></span>
<span class="line"><span style="color:#a6accd">创建配置文件</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# ln -s /usr/share/keystone/wsgi-keystone.conf /etc/httpd/conf.d/</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>启动keystone，并查看端口。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# systemctl enable httpd.service</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# systemctl start httpd.service</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# netstat -ntlp | grep httpd</span></span>
<span class="line"><span style="color:#a6accd">tcp6       0      0 :::5000                 :::*              LISTEN      3408/httpd</span></span>
<span class="line"><span style="color:#a6accd">tcp6       0      0 :::80                   :::*              LISTEN      3408/httpd</span></span>
<span class="line"><span style="color:#a6accd">现在，如果你的Keystone没有正常的监听端口，那么请查看日志，后期我们会不停的强调读者通过查看日志来排查问题，OpenStack的日志是值得信赖的。如果你想让朋友帮助解决问题，请不要使用日志截图的方式，请发送完整的日志。</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h3 id="keystone权限管理" tabindex="-1">Keystone权限管理 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#keystone权限管理" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>我们已经成功的运行了Keystone服务，要提供身份认证服务，需要使用域、项目、用户和角色的组合。 首先我们要创建一个超级管理员用户、角色和项目。默认情况下。在创建用户之前，我们需要连接到Keystone。问题来了，现在我们没有任何用户，我们如何登录KeyStone呢？这个时候ADMIN_TOKEN就可以帮忙了，我们可以使用这个超级密码直接登录到KeyStone上，所以说，一定要保护好你的ADMIN_TOKEN。 好的，现在我们要连接到Keystone上，有两种方法：</p><ul><li>使用--os-token等参数的方式（就是通过参数的方式）</li><li>使用环境变量（将参数设置为环境变量，keystone直接读取环境变量）</li><li>设置环境变量</li></ul><p>在这里，我们使用三个环境变量用来连接keystone。admin用户和密码就是我们前面使用keystone-manage bootstrap初始化的。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# </span></span>
<span class="line"><span style="color:#a6accd">export OS_USERNAME=admin</span></span>
<span class="line"><span style="color:#a6accd">export OS_PASSWORD=admin</span></span>
<span class="line"><span style="color:#a6accd">export OS_PROJECT_NAME=admin</span></span>
<span class="line"><span style="color:#a6accd">export OS_USER_DOMAIN_NAME=Default</span></span>
<span class="line"><span style="color:#a6accd">export OS_PROJECT_DOMAIN_NAME=Default</span></span>
<span class="line"><span style="color:#a6accd">export OS_AUTH_URL=http://192.168.56.11:5000/v3</span></span>
<span class="line"><span style="color:#a6accd">export OS_IDENTITY_API_VERSION=3</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>请注意环境变量的问题，不要打开新的会话进行操作。</p><ol><li>创建实验用的demo项目、用户和角色</li></ol><p>下面我们创建一个普通用户和租户，我们下面的实验均使用这个普通用户进行Openstack的管理。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd"># openstack domain create --description "An Example Domain" example</span></span>
<span class="line"><span style="color:#a6accd"># openstack project create --domain default --description "Demo Project" myproject</span></span>
<span class="line"><span style="color:#a6accd"># openstack user create --domain default --password-prompt myuser</span></span>
<span class="line"><span style="color:#a6accd"># openstack role create myrole</span></span>
<span class="line"><span style="color:#a6accd"># openstack role add --project myproject --user myuser myrole</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><blockquote><p>在创建用户的时候可以使用--password-prompt，然后根据提示输入密码</p></blockquote><ol><li><p>创建一个服务的项目</p><p>OpenStack服务也需要使用用户名、租户和角色，用来访问OpenStack的各个服务。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd"># openstack project create --domain default --description "Service Project" service</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div></li><li><p>创建各个服务连接Keystone的账户 后面我们部署的其它服务都需要到Keystone上进行认证，所以现在就可以提前将各个服务的账号创建上。并全部加入service项目。</p><ul><li><p>创建glance用户</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd"># openstack user create --domain default --password glance glance</span></span>
<span class="line"><span style="color:#a6accd"># openstack role add --project service --user glance admin</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div></li><li><p>创建nova用户</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd"># openstack user create --domain default --password nova nova</span></span>
<span class="line"><span style="color:#a6accd"># openstack role add --project service --user nova admin</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div></li><li><p>创建placement用户</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd"># openstack user create --domain default --password placement placement</span></span>
<span class="line"><span style="color:#a6accd"># openstack role add --project service --user placement admin</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div></li><li><p>创建Neutron用户</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd"># openstack user create --domain default --password neutron neutron</span></span>
<span class="line"><span style="color:#a6accd"># openstack role add --project service --user neutron admin</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div></li><li><p>创建cinder用户</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd"># openstack user create --domain default --password cinder cinder</span></span>
<span class="line"><span style="color:#a6accd"># openstack role add --project service --user cinder admin</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div></li></ul></li></ol><h3 id="验证keystone安装" tabindex="-1">验证Keystone安装 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#验证keystone安装" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>首先，我们需要取消OS_TOKEN和OS_URL这两个变量。这两个变量是用于引导我们创建Admin用户和Keystone服务自身Service和Endpoint注册时候的用的，现在已经不需要了。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# unset OS_AUTH_URL OS_PASSWORD</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>测试admin用户</strong></p><p>密码为admin，需要输入两次，如果能正常获取token说明配置成功。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# openstack --os-auth-url http://192.168.56.11:5000/v3 \</span></span>
<span class="line"><span style="color:#a6accd">--os-project-domain-name Default --os-user-domain-name Default \</span></span>
<span class="line"><span style="color:#a6accd">--os-project-name admin --os-username admin token issue</span></span>
<span class="line"><span style="color:#a6accd">Password:</span></span>
<span class="line"><span style="color:#a6accd">…</span></span>
<span class="line"><span style="color:#a6accd">#密码为admin，需要输入两次，如果能正常获取token说明配置成功。</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>测试myuser用户</strong></p><p>密码为你设置的，需要输入两次，如果能正常获取token说明配置成功。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# openstack --os-auth-url http://192.168.56.11:5000/v3 \</span></span>
<span class="line"><span style="color:#a6accd">--os-project-domain-name Default --os-user-domain-name Default \</span></span>
<span class="line"><span style="color:#a6accd">--os-project-name myproject --os-username myuser token issue</span></span>
<span class="line"><span style="color:#a6accd">Password:</span></span>
<span class="line"><span style="color:#a6accd">…</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h3 id="环境变量脚本配置" tabindex="-1">环境变量脚本配置 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#环境变量脚本配置" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>使用Keystone命令有两种方式，第一种就像上面。我们使用指定--os-username、--os-password和--os-password这样的选项来进行操作，但是这样在我们日常操作中会比较麻烦。第二种方式就是使用环境变量，这样可以避免每次使用都要制定变量。我们将这些常用的变量设置为环境变量。 下面建立环境变量为其它服务部署和配置使用</p><ol><li>设置admin环境变量脚本</li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# vim /root/admin-openstack.sh</span></span>
<span class="line"><span style="color:#a6accd">export OS_PROJECT_DOMAIN_NAME=Default</span></span>
<span class="line"><span style="color:#a6accd">export OS_USER_DOMAIN_NAME=Default</span></span>
<span class="line"><span style="color:#a6accd">export OS_PROJECT_NAME=admin</span></span>
<span class="line"><span style="color:#a6accd">export OS_USERNAME=admin</span></span>
<span class="line"><span style="color:#a6accd">export OS_PASSWORD=admin</span></span>
<span class="line"><span style="color:#a6accd">export OS_AUTH_URL=http://192.168.56.11:5000/v3</span></span>
<span class="line"><span style="color:#a6accd">export OS_IDENTITY_API_VERSION=3</span></span>
<span class="line"><span style="color:#a6accd">export OS_IMAGE_API_VERSION=2</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li>创建普通用户的环境变量脚本</li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# vim /root/demo-openstack.sh</span></span>
<span class="line"><span style="color:#a6accd">export OS_PROJECT_DOMAIN_NAME=Default</span></span>
<span class="line"><span style="color:#a6accd">export OS_USER_DOMAIN_NAME=Default</span></span>
<span class="line"><span style="color:#a6accd">export OS_PROJECT_NAME=myproject</span></span>
<span class="line"><span style="color:#a6accd">export OS_USERNAME=myuser</span></span>
<span class="line"><span style="color:#a6accd">export OS_PASSWORD=123.com</span></span>
<span class="line"><span style="color:#a6accd">export OS_AUTH_URL=http://192.168.56.11:5000/v3</span></span>
<span class="line"><span style="color:#a6accd">export OS_IDENTITY_API_VERSION=3</span></span>
<span class="line"><span style="color:#a6accd">export OS_IMAGE_API_VERSION=2</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>创建完毕变量后，在后期使用某个账户的权限，只需要source一下就可以使用，例如：</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# source /root/admin-openstack.sh</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# openstack token issue</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>测试demo用户，是否能够获取token</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# source /root/demo-openstack.sh </span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# openstack token issue</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h1 id="_1-3-openstack验证服务keystone-1" tabindex="-1">1.3 OpenStack验证服务KeyStone <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_1-3-openstack验证服务keystone-1" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h1><p>Keystone（OpenStack Identity Service）是OpenStack框架中，负责身份验证、服务规则和服务令牌的功能，它实现了OpenStack的Identity API。</p><p>Keystone类似一个服务总线， 或者说是整个OpenStack框架的注册中心，其他服务通过keystone来注册其服务的Endpoint（服务访问的URL），任何服务之间相互的调用，需要经过Keystone的身份验证，来获得目标服务的Endpoint来找到目标服务。</p><h3 id="keystone概述-1" tabindex="-1">Keystone概述 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#keystone概述-1" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>Keystone提供了以下两个主要的功能：</p><ul><li>用户与认证:用户权限与用户行为跟踪；</li><li>服务目录:提供一个服务目录，包括所有服务项与相关Api的端点。</li></ul><p>下面我们从这两个主要功能来了解Keystone的概念，和Keystone在Openstack中的作用。</p><ol><li><p>Keystone用户与认证</p><p>Keystone为各个Openstack组件提供用户和认证服务，各个组件都必须与Keystone进行交互。比如登陆认证是用户在访问Openstack各个组件中的API时，必须通过Keystone的用户名和密码验证，最终通过Kyestone验证获取token，完成对用户的登陆认证。如果认证失败，该用户将不能访问该API。</p></li></ol><p><strong>User</strong></p><p>User即用户，它是用一个数字代表使用OpenStack云服务的一个人、系统、或服务。身份验证服务将会验证传入的由用户声明将调用的请求。用户如果已经登录，可就能分配令牌（tokens）访问资源。 用户可能被直接分配给特定的租户，用户行为被包含在租户（tenant），Users通过认证信息（credentials，如密码、API Keys等）进行验证。</p><p><strong>Credentials</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">Credentials即证书，用户可以通过身份验证，保证数据通常只被一个用户属于或拥有(因为没有其他人应该知道数据)。</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>以下为证书的例子: 一个匹配的用户名和密码 一个匹配的用户名和API键 一个身份认证令牌</p><p><strong>Authentication</strong></p><p>Authentication即认证，认证是确认身份或识别真实性的行为。身份服务通过与用户的一组声明验证以确认传入的请求是由该用户声明过的。这些声明被初始化为一组证书 (username&amp;password, or username and API key)。初始确认后，身份服务将会给用户发布一个确认身份的令牌(Token)，在用户随后的请求中可以使用这个令牌授权访问。</p><p><strong>Token：</strong></p><p>Token即令牌，是一个用户访问资源的任意文本，每一个令牌都有一个范围，描述了可以访问哪些资源。令牌可能随时撤消，是有有效期的。身份认证是支持令牌认证的，它的目的为在未来支持额外的协议。其目的是为了让它首先成为一种集成服务，而不是一种渴望成为丰富的身份存储和管理的解决方案。 T <strong>Tenant：</strong></p><p>Tenant即租户，它是各个服务中的一些可以访问的资源集合。它是一个容器，用于组织和隔离资源，或标识对象。一个租户可以一个客户、账户、组织、项目的映射。例如，在Nova中一个tenant可以是一些机器，在Swift和Glance中一个tenant可以是一些镜像存储，在Neutron中一个tenant可以是一些网络资源。Users默认的总是绑定到某些tenant上。</p><p><strong>Role：</strong></p><p>Role即角色，Roles代表一组用户可以访问的资源权限，例如Nova中的虚拟机、Glance中的镜像。Users可以被添加到任意一个全局的或租户内的角色中。在全局的role中，用户的role权限作用于所有的租户，即可以对所有的租户执行role规定的权限；在租户内的role中，用户仅能在当前租户内执行role规定的权限。</p><p><strong>Domain：</strong></p><p>在KeyStone V3版本将 Tenant 改为 Project 并在其上添加 Domain 的概念，这更加符合现实世界和云服务的映射。利用 Domain 实现真正的多租户（multi-tenancy）架构，Domain 担任 Project 的高层容器。云服务的客户是 Domain 的所有者，他们可以在自己的 Domain 中创建多个 Projects、Users、Groups 和 Roles。通过引入 Domain，云服务客户可以对其拥有的多个 Project 进行统一管理，而不必再向过去那样对每一个 Project 进行单独管理。</p><p><strong>Group：</strong></p><p>Group 是一组 Users 的容器，可以向 Group 中添加用户，并直接给 Group 分配角色，那么在这个 Group 中的所有用户就都拥有了 Group 所拥有的角色权限。通过引入 Group 的概念，Keystone V3 实现了对用户组的管理，达到了同时管理一组用户权限的目的。这与 V2 中直接向 User/Project 指定 Role 不同，使得对云服务进行管理更加便捷。 Domain、Group、Project、User 和 Role 的关系图</p><p>如图 2 所示，在一个 Domain 中包含 3 个 Projects,可以通过 Group1 将 Role Sysadmin直接赋予 Domain,那么 Group1 中的所有用户将会对 Domain 中的所有 Projects 都拥有管理员权限。也可以通过 Group2 将 Role Engineer 只赋予 Project3,这样 Group2 中的 User 就只拥有对 Project3 相应的权限，而不会影响其它 Projects。</p><ol><li><p>Keystone服务目录</p><p>上文说过，Keystone除了用户管理相关的支撑，还有一个重要的作用是一个服务目录。KeyStone为Openstack各个服务提供了一个REST API端点列表。</p></li><li><p>Service Service即服务，如Nova、Glance、Swift。根据前三个概念（User，Tenant和Role）一个服务可以确认当前用户是否具有访问其资源的权限。但是当一个user尝试着访问其租户内的service时，他必须知道这个service是否存在以及如何访问这个service，这里通常使用一些不同的名称表示不同的服务。在上文中谈到的Role，实际上也是可以绑定到某个service的。</p></li><li><p>Endpoint Endpoint，翻译为“端点”，我们可以理解它是一个服务暴露出来的访问点，如果需要访问一个服务，则必须知道他的endpoint。因此，在keystone中包含一个endpoint模板（endpoint template，在安装keystone的时候我们可以在conf文件夹下看到这个文件），这个模板提供了所有存在的服务endpoints信息。一个endpoint template包含一个URLs列表，列表中的每个URL都对应一个服务实例的访问地址，并且具有public、private和admin这三种权限。public url可以被全局访问（如<a target="_blank" rel="noreferrer" href="http://compute.example.com"><!--[-->http://compute.example.com），private<!--]--><!----></a>%2Cprivate/) url只能被局域网访问（如<a target="_blank" rel="noreferrer" href="http://compute.example.local"><!--[-->http://compute.example.local），admin<!--]--><!----></a>%2Cadmin/) url被从常规的访问中分离。</p></li></ol><h3 id="keystone部署-1" tabindex="-1">KeyStone部署 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#keystone部署-1" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>OpenStack中所有服务的安装可以说是最简单的。复杂的就在于本身的配置文件的修改和与Keystone相关的配置。还记得上面说的，OpenStack的每个组件如果能够使用都必须在Keystone创建Service和Endpoint吗？好的，我们开始。</p><ol><li>安装keystone</li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# yum install -y openstack-keystone httpd mod_wsgi memcached python-memcached</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li>设置Memcache开启启动并启动Memcached</li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">#keystone使用Memcached来缓存验证后生成的token。</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# systemctl enable memcached.service</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# vim /etc/sysconfig/memcached</span></span>
<span class="line"><span style="color:#a6accd">PORT="11211"</span></span>
<span class="line"><span style="color:#a6accd">USER="memcached"</span></span>
<span class="line"><span style="color:#a6accd">MAXCONN="1024"</span></span>
<span class="line"><span style="color:#a6accd">CACHESIZE="64"</span></span>
<span class="line"><span style="color:#a6accd">OPTIONS="-l 192.168.56.11,::1"</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# systemctl start memcached.service</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li>Keystone配置</li></ol><p>KeyStone的配置只需要修改/etc/keystone/keystone.conf即可，涉及Admin Token和Mysql连接，其它均使用默认配置即可。所有配置文件都存放到/etc/keystone（项目名）目录下。日志都存放在/var/log/keystone（项目名）目录下，其它项目也是如此。 好的，我们暂停一下，在后面的部署中，我们会使用这样的方式列出配置，未列出的配置，保持默认不进行修改，除非特别提示到。另外要注意配置的位置，OpenStack所有服务都会有[DEFAULT] [dtabase] 这样的配置部分，请在文中列出的配置部分里面进行配置，切记！。因为确实有相同的配置存在在不同的配置端的情况，这样例外情况，如果没有按要求配置，可能就无法正常运行。</p><ul><li><p>配置KeyStone数据库</p><p>OpenStack中所有服务的数据库同步都需要依赖与配置文件中的数据库连接的配置，所以必须先修改完毕配置文件后，再进行数据库的初始化操作。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# vim /etc/keystone/keystone.conf</span></span>
<span class="line"><span style="color:#a6accd">[database]</span></span>
<span class="line"><span style="color:#a6accd">connection = mysql+pymysql://keystone:keystone@192.168.56.11/keystone</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div></li><li><p>设置Token和Memcached</p></li></ul><p>在之前的KeyStone版本由于所有的API通信都要请求Keystone来生产Token，之前是存放在MySQL表里，导致表特别大，经常需要手动清理，而且性能下降，现在新的版本我们可以存放在memcache里面了。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[token]</span></span>
<span class="line"><span style="color:#a6accd">provider = fernet</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ul><li>同步数据库：</li></ul><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# su -s /bin/sh -c "keystone-manage db_sync" keystone</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><blockquote><p>小提示：为什么要使用keystone用户来同步数据呢，因为同步操作会生产日志/var/log/keystone/keystone.log，如果使用root用户，那么生成的文件权限为root用户，keystone启动的时候没有权限读取该文件，会无法启动。如果你的同步执行有问题，那么可以查看日志获取详细的错误信息。</p></blockquote><ul><li>验证数据库创建是否正常：</li></ul><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# mysql -h 192.168.56.11 -ukeystone -pkeystone -e "use keystone;show tables;"</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li><p>初始化fernet keys</p><p>生成完毕之后，会创建/etc/keystone/fernet-keys目录</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# keystone-manage fernet_setup --keystone-user keystone --keystone-group keystone</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# keystone-manage credential_setup --keystone-user keystone --keystone-group keystone</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div></li><li><p>初始化keystone</p></li></ol><p>还记得上面说的Openstack的每个组件都必须在Keystone上进行注册。当然也包括Keystone本身。身份认证服务管理了一个与您环境相关的 API 端点的目录。服务使用这个目录来决定如何与您环境中的其他服务进行通信。 OpenStack使用三个API端点变种代表每种服务：admin，internal和public。默认情况下，管理API端点允许修改用户和租户而公共和内部APIs不允许这些操作。在生产环境中，处于安全原因，变种为了服务不同类型的用户可能驻留在单独的网络上。 对实例而言，公共API网络为了让顾客管理他们自己的云在互联网上是可见的。管理API网络在管理云基础设施的组织中操作也是有所限制的。内部API网络可能会被限制在包含OpenStack服务的主机上。此外，OpenStack支持可伸缩性的多区域。为了简单起见，我们这里均使用一个IP地址。192.168.56.11。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# keystone-manage bootstrap --bootstrap-password admin \</span></span>
<span class="line"><span style="color:#a6accd"> --bootstrap-admin-url http://192.168.56.11:5000/v3/ \</span></span>
<span class="line"><span style="color:#a6accd"> --bootstrap-internal-url http://192.168.56.11:5000/v3/ \</span></span>
<span class="line"><span style="color:#a6accd"> --bootstrap-public-url http://192.168.56.11:5000/v3/ \</span></span>
<span class="line"><span style="color:#a6accd"> --bootstrap-region-id RegionOne</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li>验证Keystone配置</li></ol><p>下面我将上面进行的配置列出来，供读者进行比对，配置应该和本文保持一致。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# grep "^[a-z]" /etc/keystone/keystone.conf </span></span>
<span class="line"><span style="color:#a6accd">connection = mysql+pymysql://keystone:keystone@192.168.56.11/keystone</span></span>
<span class="line"><span style="color:#a6accd">provider = fernet</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li>KeyStone启动</li></ol><p>OpenStack Keystone使用Apache来进行启动，提高了性能。 配置ServerName，注意一定要配置，不然会有奇怪的问题出现。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# vim /etc/httpd/conf/httpd.conf</span></span>
<span class="line"><span style="color:#a6accd">ServerName 192.168.56.11:80</span></span>
<span class="line"><span style="color:#a6accd">创建配置文件</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# ln -s /usr/share/keystone/wsgi-keystone.conf /etc/httpd/conf.d/</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>启动keystone，并查看端口。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# systemctl enable httpd.service</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# systemctl start httpd.service</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# netstat -ntlp | grep httpd</span></span>
<span class="line"><span style="color:#a6accd">tcp6       0      0 :::5000                 :::*              LISTEN      3408/httpd</span></span>
<span class="line"><span style="color:#a6accd">tcp6       0      0 :::80                   :::*              LISTEN      3408/httpd</span></span>
<span class="line"><span style="color:#a6accd">现在，如果你的Keystone没有正常的监听端口，那么请查看日志，后期我们会不停的强调读者通过查看日志来排查问题，OpenStack的日志是值得信赖的。如果你想让朋友帮助解决问题，请不要使用日志截图的方式，请发送完整的日志。</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h3 id="keystone权限管理-1" tabindex="-1">Keystone权限管理 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#keystone权限管理-1" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>我们已经成功的运行了Keystone服务，要提供身份认证服务，需要使用域、项目、用户和角色的组合。 首先我们要创建一个超级管理员用户、角色和项目。默认情况下。在创建用户之前，我们需要连接到Keystone。问题来了，现在我们没有任何用户，我们如何登录KeyStone呢？这个时候ADMIN_TOKEN就可以帮忙了，我们可以使用这个超级密码直接登录到KeyStone上，所以说，一定要保护好你的ADMIN_TOKEN。 好的，现在我们要连接到Keystone上，有两种方法：</p><ul><li>使用--os-token等参数的方式（就是通过参数的方式）</li><li>使用环境变量（将参数设置为环境变量，keystone直接读取环境变量）</li><li>设置环境变量</li></ul><p>在这里，我们使用三个环境变量用来连接keystone。admin用户和密码就是我们前面使用keystone-manage bootstrap初始化的。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# </span></span>
<span class="line"><span style="color:#a6accd">export OS_USERNAME=admin</span></span>
<span class="line"><span style="color:#a6accd">export OS_PASSWORD=admin</span></span>
<span class="line"><span style="color:#a6accd">export OS_PROJECT_NAME=admin</span></span>
<span class="line"><span style="color:#a6accd">export OS_USER_DOMAIN_NAME=Default</span></span>
<span class="line"><span style="color:#a6accd">export OS_PROJECT_DOMAIN_NAME=Default</span></span>
<span class="line"><span style="color:#a6accd">export OS_AUTH_URL=http://192.168.56.11:5000/v3</span></span>
<span class="line"><span style="color:#a6accd">export OS_IDENTITY_API_VERSION=3</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>请注意环境变量的问题，不要打开新的会话进行操作。</p><ol><li>创建实验用的demo项目、用户和角色</li></ol><p>下面我们创建一个普通用户和租户，我们下面的实验均使用这个普通用户进行Openstack的管理。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd"># openstack domain create --description "An Example Domain" example</span></span>
<span class="line"><span style="color:#a6accd"># openstack project create --domain default --description "Demo Project" myproject</span></span>
<span class="line"><span style="color:#a6accd"># openstack user create --domain default --password-prompt myuser</span></span>
<span class="line"><span style="color:#a6accd"># openstack role create myrole</span></span>
<span class="line"><span style="color:#a6accd"># openstack role add --project myproject --user myuser myrole</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><blockquote><p>在创建用户的时候可以使用--password-prompt，然后根据提示输入密码</p></blockquote><ol><li><p>创建一个服务的项目</p><p>OpenStack服务也需要使用用户名、租户和角色，用来访问OpenStack的各个服务。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd"># openstack project create --domain default --description "Service Project" service</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div></li><li><p>创建各个服务连接Keystone的账户 后面我们部署的其它服务都需要到Keystone上进行认证，所以现在就可以提前将各个服务的账号创建上。并全部加入service项目。</p><ul><li><p>创建glance用户</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd"># openstack user create --domain default --password glance glance</span></span>
<span class="line"><span style="color:#a6accd"># openstack role add --project service --user glance admin</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div></li><li><p>创建nova用户</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd"># openstack user create --domain default --password nova nova</span></span>
<span class="line"><span style="color:#a6accd"># openstack role add --project service --user nova admin</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div></li><li><p>创建placement用户</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd"># openstack user create --domain default --password placement placement</span></span>
<span class="line"><span style="color:#a6accd"># openstack role add --project service --user placement admin</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div></li><li><p>创建Neutron用户</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd"># openstack user create --domain default --password neutron neutron</span></span>
<span class="line"><span style="color:#a6accd"># openstack role add --project service --user neutron admin</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div></li><li><p>创建cinder用户</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd"># openstack user create --domain default --password cinder cinder</span></span>
<span class="line"><span style="color:#a6accd"># openstack role add --project service --user cinder admin</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div></li></ul></li></ol><h3 id="验证keystone安装-1" tabindex="-1">验证Keystone安装 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#验证keystone安装-1" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>首先，我们需要取消OS_TOKEN和OS_URL这两个变量。这两个变量是用于引导我们创建Admin用户和Keystone服务自身Service和Endpoint注册时候的用的，现在已经不需要了。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# unset OS_AUTH_URL OS_PASSWORD</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>测试admin用户</strong></p><p>密码为admin，需要输入两次，如果能正常获取token说明配置成功。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# openstack --os-auth-url http://192.168.56.11:5000/v3 \</span></span>
<span class="line"><span style="color:#a6accd">--os-project-domain-name Default --os-user-domain-name Default \</span></span>
<span class="line"><span style="color:#a6accd">--os-project-name admin --os-username admin token issue</span></span>
<span class="line"><span style="color:#a6accd">Password:</span></span>
<span class="line"><span style="color:#a6accd">…</span></span>
<span class="line"><span style="color:#a6accd">#密码为admin，需要输入两次，如果能正常获取token说明配置成功。</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>测试myuser用户</strong></p><p>密码为你设置的，需要输入两次，如果能正常获取token说明配置成功。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# openstack --os-auth-url http://192.168.56.11:5000/v3 \</span></span>
<span class="line"><span style="color:#a6accd">--os-project-domain-name Default --os-user-domain-name Default \</span></span>
<span class="line"><span style="color:#a6accd">--os-project-name myproject --os-username myuser token issue</span></span>
<span class="line"><span style="color:#a6accd">Password:</span></span>
<span class="line"><span style="color:#a6accd">…</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h3 id="环境变量脚本配置-1" tabindex="-1">环境变量脚本配置 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#环境变量脚本配置-1" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>使用Keystone命令有两种方式，第一种就像上面。我们使用指定--os-username、--os-password和--os-password这样的选项来进行操作，但是这样在我们日常操作中会比较麻烦。第二种方式就是使用环境变量，这样可以避免每次使用都要制定变量。我们将这些常用的变量设置为环境变量。 下面建立环境变量为其它服务部署和配置使用</p><ol><li>设置admin环境变量脚本</li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# vim /root/admin-openstack.sh</span></span>
<span class="line"><span style="color:#a6accd">export OS_PROJECT_DOMAIN_NAME=Default</span></span>
<span class="line"><span style="color:#a6accd">export OS_USER_DOMAIN_NAME=Default</span></span>
<span class="line"><span style="color:#a6accd">export OS_PROJECT_NAME=admin</span></span>
<span class="line"><span style="color:#a6accd">export OS_USERNAME=admin</span></span>
<span class="line"><span style="color:#a6accd">export OS_PASSWORD=admin</span></span>
<span class="line"><span style="color:#a6accd">export OS_AUTH_URL=http://192.168.56.11:5000/v3</span></span>
<span class="line"><span style="color:#a6accd">export OS_IDENTITY_API_VERSION=3</span></span>
<span class="line"><span style="color:#a6accd">export OS_IMAGE_API_VERSION=2</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li>创建普通用户的环境变量脚本</li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# vim /root/demo-openstack.sh</span></span>
<span class="line"><span style="color:#a6accd">export OS_PROJECT_DOMAIN_NAME=Default</span></span>
<span class="line"><span style="color:#a6accd">export OS_USER_DOMAIN_NAME=Default</span></span>
<span class="line"><span style="color:#a6accd">export OS_PROJECT_NAME=myproject</span></span>
<span class="line"><span style="color:#a6accd">export OS_USERNAME=myuser</span></span>
<span class="line"><span style="color:#a6accd">export OS_PASSWORD=123.com</span></span>
<span class="line"><span style="color:#a6accd">export OS_AUTH_URL=http://192.168.56.11:5000/v3</span></span>
<span class="line"><span style="color:#a6accd">export OS_IDENTITY_API_VERSION=3</span></span>
<span class="line"><span style="color:#a6accd">export OS_IMAGE_API_VERSION=2</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>创建完毕变量后，在后期使用某个账户的权限，只需要source一下就可以使用，例如：</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# source /root/admin-openstack.sh</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# openstack token issue</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>测试demo用户，是否能够获取token</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# source /root/demo-openstack.sh </span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# openstack token issue</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h1 id="_1-4-openstack镜像服务glance" tabindex="-1">1.4 OpenStack镜像服务Glance <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_1-4-openstack镜像服务glance" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h1><h3 id="glance概述" tabindex="-1">Glance概述 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#glance概述" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>OpenStack中的Glance是镜像服务，能够提供发现、注册并查询虚拟机镜像，也是Openstack的一个组件或者说项目之一。镜像服务提供了一个Rest API的方式。 OpenStack的Glance镜像可以存放在本地文件系统，也可以存放在OpenStack的对象存储上。默认情况下是本地文件，存放在/var/lib/glance/images/目录下。 Glance还管理着快照，快照也是一种镜像，可以基于快照创建新的虚拟机。</p><p>Glance主要有两个组件</p><p>（图2.5.2）</p><ul><li>Glance-api接收REST API请求，然后通过其他模块（glance-registry及image store）来完成诸如镜像的查找、获取、上传、删除等操作，api默认监听端口9292。</li><li>glance-registry用于与MySQL数据库交互，用于存储或获取镜像的元数据（metadata）；提供镜像元数据相关的REST接口，通过glance-registry，可以向数据库中写入或获取镜像的各种数据，glance-registry监听端口9191。Glance的数据库中有两张表，一张是image表，另一张是image property表。Image表保存了镜像格式、大小等信息；image property表则主要保存镜像的定制化信息。</li><li>database 用户存放镜像的元数据。</li><li>image store是一个存储的接口层，通过这个接口，glance可以获取镜像，image store支持的存储有Amazon的S3、OpenStack本身的Swift，还有诸如ceph，sheepdog，GlusterFS等分布式存储。 Image store是镜像保存与获取的接口，它仅仅是一个接口层，具体的实现需要外部的存储支持，</li></ul><h3 id="glance部署" tabindex="-1">Glance部署 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#glance部署" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><ol><li><p>安装Glance</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# yum install -y openstack-glance</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div></li><li><p>配置Glance</p></li></ol><p>Glance的配置是通过修改两个组件的配置文件来完成。Glance使用了MySQL和Keystone。</p><ul><li>Glance数据库配置</li></ul><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# vim /etc/glance/glance-api.conf </span></span>
<span class="line"><span style="color:#a6accd">[database]</span></span>
<span class="line"><span style="color:#a6accd">connection= mysql+pymysql://glance:glance@192.168.56.11/glance</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ul><li>设置Keystone</li></ul><p>就像在Keystone章节提到了。OpenStack其它组件都需要和Keystone进行交互，来进行用户认证和服务目录的管理。对于Glance来说，Keystone的配置在glance-api.conf和glance-registry.conf都需要设置，而且配置相同。 glance-api.conf配置</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# vim /etc/glance/glance-api.conf</span></span>
<span class="line"><span style="color:#a6accd">[keystone_authtoken]</span></span>
<span class="line"><span style="color:#a6accd">www_authenticate_uri  = http://192.168.56.11:5000</span></span>
<span class="line"><span style="color:#a6accd">auth_url = http://192.168.56.11:5000</span></span>
<span class="line"><span style="color:#a6accd">memcached_servers = 192.168.56.11:11211</span></span>
<span class="line"><span style="color:#a6accd">auth_type = password</span></span>
<span class="line"><span style="color:#a6accd">project_domain_name = Default</span></span>
<span class="line"><span style="color:#a6accd">user_domain_name = Default</span></span>
<span class="line"><span style="color:#a6accd">project_name = service</span></span>
<span class="line"><span style="color:#a6accd">username = glance</span></span>
<span class="line"><span style="color:#a6accd">password = glance</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">[paste_deploy]</span></span>
<span class="line"><span style="color:#a6accd">flavor=keystone</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ul><li><p>设置Glance镜像存储</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# vim /etc/glance/glance-api.conf</span></span>
<span class="line"><span style="color:#a6accd">#默认的存储方式是文件，可以指定镜像存储的路径</span></span>
<span class="line"><span style="color:#a6accd">[glance_store]</span></span>
<span class="line"><span style="color:#a6accd">stores = file,http</span></span>
<span class="line"><span style="color:#a6accd">default_store=file</span></span>
<span class="line"><span style="color:#a6accd">filesystem_store_datadir=/var/lib/glance/images/</span></span>
<span class="line"><span style="color:#a6accd">#也就是说，我们后面上传的镜像都保存在/var/lib/glance/images目录下。</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div></li></ul><ol><li><p>同步数据库</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# su -s /bin/sh -c "glance-manage db_sync" glance</span></span>
<span class="line"><span style="color:#a6accd">测试数据库同步情况</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# mysql -h 192.168.56.11 -uglance -pglance -e "use glance;show tables;"</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div></li><li><p>启动Glance服务</p></li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# systemctl enable openstack-glance-api.service </span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# systemctl start openstack-glance-api.service</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li><p>Glance服务注册</p><p>想要让别的服务可以使用Glance，就需要在Keystone上完成服务的注册。注意需要先source一下admin的环境变量。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# source /root/admin-openstack.sh</span></span>
<span class="line"><span style="color:#a6accd"># openstack service create --name glance --description "OpenStack Image" image</span></span>
<span class="line"><span style="color:#a6accd"># openstack endpoint create --region RegionOne image public http://192.168.56.11:9292</span></span>
<span class="line"><span style="color:#a6accd"># openstack endpoint create --region RegionOne image internal http://192.168.56.11:9292</span></span>
<span class="line"><span style="color:#a6accd"># openstack endpoint create --region RegionOne image admin http://192.168.56.11:9292</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div></li></ol><h3 id="验证glance安装" tabindex="-1">验证Glance安装 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#验证glance安装" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><ol><li><p>测试Glance状态</p><p>我们可以直接使用glance命令列出现在的所有镜像，如果出现一个空列表，说明Glance启动并配置完成。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# source /root/admin-openstack.sh </span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# openstack image list</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>如果未出现报错，基本上可以确实部署没有问题，可以上传镜像进行测试。</p></li><li><p>Glance镜像上传测试</p></li></ol><p>在刚开始实施OpenStack平台阶段，如果没有制作镜像。可以使用一个实验的镜像进行测试，这是一个小的Linux系统。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# cd /usr/local/src</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 src]# wget http://download.cirros-cloud.net/0.4.0/cirros-0.4.0-x86_64-disk.img</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">#上传并注册一个镜像。注意以后只要看到使用openstack命令，都需要保证，已经使用source命令导入了环境变量。因为这些操作都需要环境变量里面的账号进行认证授权。</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 src]# openstack image create "cirros" --disk-format qcow2 \</span></span>
<span class="line"><span style="color:#a6accd">--container-format bare --file cirros-0.4.0-x86_64-disk.img --public</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h1 id="上传完毕后可以通过openstack-image-list再次查看上传的镜像。默认情况下，glance使用本地文件系统来保存上传的镜像。默认存放在-var-lib-glance-images-目录下，会将镜像以image-id来进行命名。" tabindex="-1">上传完毕后可以通过openstack image list再次查看上传的镜像。默认情况下，Glance使用本地文件系统来保存上传的镜像。默认存放在/var/lib/glance/images/目录下，会将镜像以image id来进行命名。 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#上传完毕后可以通过openstack-image-list再次查看上传的镜像。默认情况下，glance使用本地文件系统来保存上传的镜像。默认存放在-var-lib-glance-images-目录下，会将镜像以image-id来进行命名。" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h1><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 src]# openstack image list</span></span>
<span class="line"><span style="color:#a6accd">+--------------------------------------+--------+--------+</span></span>
<span class="line"><span style="color:#a6accd">| ID                                   | Name   | Status |</span></span>
<span class="line"><span style="color:#a6accd">+--------------------------------------+--------+--------+</span></span>
<span class="line"><span style="color:#a6accd">| cf154a84-a73a-451b-bcb3-83c98e7c0d3e | cirros | active |</span></span>
<span class="line"><span style="color:#a6accd">+--------------------------------------+--------+--------+</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h1 id="同时，这个文件会被保存到配置文件设置的目录下，并以id命名。" tabindex="-1">同时，这个文件会被保存到配置文件设置的目录下，并以ID命名。 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#同时，这个文件会被保存到配置文件设置的目录下，并以id命名。" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h1><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# ls -l /var/lib/glance/images/</span></span>
<span class="line"><span style="color:#a6accd">total 12980</span></span>
<span class="line"><span style="color:#a6accd">-rw-r----- 1 glance glance 13287936 Dec  1 14:11 65f9826a-5ccb-47d9-8a1a-75e31fb88a4a</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>Glance在OpenStack组件中，属于比较简单的一个。我建议读者如果一直跟着做实验的话，现在是时候停下来总结一下了。因为后面的服务的部署流程和glance都大同小异。</p><h1 id="_3-5-placement服务安装" tabindex="-1">3.5 Placement服务安装 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_3-5-placement服务安装" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h1><h3 id="_3-5-1-placement概述" tabindex="-1">3.5.1 Placement概述 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_3-5-1-placement概述" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><h3 id="_3-5-2-placement部署" tabindex="-1">3.5.2 Placement部署 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_3-5-2-placement部署" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><ol><li>Placement服务注册</li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd"># openstack service create --name placement   --description "Placement API" placement</span></span>
<span class="line"><span style="color:#a6accd"># openstack endpoint create --region RegionOne   placement public http://192.168.56.11:8778</span></span>
<span class="line"><span style="color:#a6accd"># openstack endpoint create --region RegionOne   placement internal http://192.168.56.11:8778</span></span>
<span class="line"><span style="color:#a6accd"># openstack endpoint create --region RegionOne   placement admin http://192.168.56.11:8778</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li><p>安装Placement</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# yum install -y openstack-placement-api</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div></li><li><p>配置Palacement</p></li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# vim /etc/placement/placement.conf</span></span>
<span class="line"><span style="color:#a6accd">[placement_database]</span></span>
<span class="line"><span style="color:#a6accd">connection = mysql+pymysql://placement:placement@192.168.56.11/placement</span></span>
<span class="line"><span style="color:#a6accd">[api]</span></span>
<span class="line"><span style="color:#a6accd">auth_strategy = keystone</span></span>
<span class="line"><span style="color:#a6accd">[keystone_authtoken]</span></span>
<span class="line"><span style="color:#a6accd">auth_url = http://192.168.56.11:5000/v3</span></span>
<span class="line"><span style="color:#a6accd">memcached_servers = 192.168.56.11:11211</span></span>
<span class="line"><span style="color:#a6accd">auth_type = password</span></span>
<span class="line"><span style="color:#a6accd">project_domain_name = default</span></span>
<span class="line"><span style="color:#a6accd">user_domain_name = default</span></span>
<span class="line"><span style="color:#a6accd">project_name = service</span></span>
<span class="line"><span style="color:#a6accd">username = placement</span></span>
<span class="line"><span style="color:#a6accd">password = placement</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# vim /etc/httpd/conf.d/00-placement-api.conf </span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">#增加下面配置</span></span>
<span class="line"><span style="color:#a6accd">&lt;Directory /usr/bin&gt;</span></span>
<span class="line"><span style="color:#a6accd">Require all granted</span></span>
<span class="line"><span style="color:#a6accd">&lt;/Directory&gt;</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">增加后如下：</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">Listen 8778</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">&lt;VirtualHost *:8778&gt;</span></span>
<span class="line"><span style="color:#a6accd">  &lt;Directory /usr/bin&gt;</span></span>
<span class="line"><span style="color:#a6accd">Require all granted</span></span>
<span class="line"><span style="color:#a6accd">&lt;/Directory&gt;</span></span>
<span class="line"><span style="color:#a6accd">  WSGIProcessGroup placement-api</span></span>
<span class="line"><span style="color:#a6accd">  WSGIApplicationGroup %{GLOBAL}</span></span>
<span class="line"><span style="color:#a6accd">  WSGIPassAuthorization On</span></span>
<span class="line"><span style="color:#a6accd">  WSGIDaemonProcess placement-api processes=3 threads=1 user=placement group=placement</span></span>
<span class="line"><span style="color:#a6accd">  WSGIScriptAlias / /usr/bin/placement-api</span></span>
<span class="line"><span style="color:#a6accd">  &lt;IfVersion &gt;= 2.4&gt;</span></span>
<span class="line"><span style="color:#a6accd">    ErrorLogFormat "%M"</span></span>
<span class="line"><span style="color:#a6accd">  &lt;/IfVersion&gt;</span></span>
<span class="line"><span style="color:#a6accd">  ErrorLog /var/log/placement/placement-api.log</span></span>
<span class="line"><span style="color:#a6accd">  #SSLEngine On</span></span>
<span class="line"><span style="color:#a6accd">  #SSLCertificateFile ...</span></span>
<span class="line"><span style="color:#a6accd">  #SSLCertificateKeyFile ...</span></span>
<span class="line"><span style="color:#a6accd">&lt;/VirtualHost&gt;</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">Alias /placement-api /usr/bin/placement-api</span></span>
<span class="line"><span style="color:#a6accd">&lt;Location /placement-api&gt;</span></span>
<span class="line"><span style="color:#a6accd">  SetHandler wsgi-script</span></span>
<span class="line"><span style="color:#a6accd">  Options +ExecCGI</span></span>
<span class="line"><span style="color:#a6accd">  WSGIProcessGroup placement-api</span></span>
<span class="line"><span style="color:#a6accd">  WSGIApplicationGroup %{GLOBAL}</span></span>
<span class="line"><span style="color:#a6accd">  WSGIPassAuthorization On</span></span>
<span class="line"><span style="color:#a6accd">&lt;/Location&gt;</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li>同步数据库</li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# su -s /bin/sh -c "placement-manage db sync" placement</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li>重启httpd</li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# systemctl restart httpd</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h1 id="_1-6-openstack计算服务nova" tabindex="-1">1.6 OpenStack计算服务Nova <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_1-6-openstack计算服务nova" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h1><p>Nova是OpenStack最早的两个模块之一（另一个是对象存储）。在OpenStack体系中是计算资源虚拟化的项目。同时Nova也是OpenStack项目中组件最多的一个项目。在中小型部署中，我们常常把除nova-compute之外的其它组件部署到一台服务器上，称之为控制节点。将nova-compute部署到单独的一台服务器上，称之为计算节点。</p><h3 id="nova服务组件" tabindex="-1">Nova服务组件 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#nova服务组件" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><ul><li><p>Nova-Api服务</p><p>主要用于接收和响应外部请求，它支持OpenStack API、Amazon EC2 等API</p><ul><li>nova-api组件实现了RESTful API功能，是外部访问Nova的唯一途径。</li><li>接收外部的请求并通过Message Queue将请求发送给其他的服务组件，同时也兼容EC2 API，所以也可以用EC2的管理工具对nova进行日常管理。</li></ul></li><li><p>Nova-Cert服务</p><p>是Nova的证书管理服务，用来为EC2服务提供身份验证，仅仅是在EC2 API的请求中使用。</p></li><li><p>Nova-Scheduler服务</p><p>用于Nova的调度工作，在创建虚拟机时，由它选择将虚拟机创建在哪台计算节点上。</p></li><li><p>Nova-Conductor服务</p><p>这个是服务是计算节点访问数据库时的一个中间层，它出现的作用是防止计算节点的Nova-Compute服务直接访问数据库。同时这个中间层可以进行水平扩展。</p></li><li><p>Nova-Console服务</p><p>Nova增强了它的控制台服务。控制台服务允许用户可以通过代理服务器访问虚拟化实例。这就涉及了一对新的守护进程（nova-console和nova-consoleauth).</p></li><li><p>Nova-Consoleauth服务</p><p>主要用于控制台的用户访问授权</p></li><li><p>Nova-Novncproxy服务</p><p>用于为用户访问虚拟机提供了一个VNC的代理。通过VNC协议，可以支持基于浏览器的novnc客户端，后面你在网页打开的console界面就是依靠这个服务提供的。</p></li><li><p>Nova-Compute</p><p>Nova-compute是Nova最重要的组件之一。</p><ul><li>nova-compute 一般运行在计算节点上，通过Message Queue接收并管理VM的生命周期。</li><li>Nova-compute 通过Libvirt管理KVM，通过XenAPI管理Xen等。</li></ul></li></ul><h3 id="nova控制节点部署" tabindex="-1">Nova控制节点部署 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#nova控制节点部署" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>首先我们需要先在控制节点部署除nova-compute之外的其它必备的服务。</p><ol><li>控制节点安装</li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# yum install -y openstack-nova-api \</span></span>
<span class="line"><span style="color:#a6accd">  openstack-nova-conductor \</span></span>
<span class="line"><span style="color:#a6accd">  openstack-nova-novncproxy openstack-nova-scheduler</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>nova.conf是配置Nova的核心配置文件，Nova的配置主要围绕该配置文件。</p><ul><li>数据库配置</li></ul><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# vim /etc/nova/nova.conf</span></span>
<span class="line"><span style="color:#a6accd">[api_database]</span></span>
<span class="line"><span style="color:#a6accd">connection= mysql+pymysql://nova:nova@192.168.56.11/nova_api</span></span>
<span class="line"><span style="color:#a6accd">[database]</span></span>
<span class="line"><span style="color:#a6accd">connection= mysql+pymysql://nova:nova@192.168.56.11/nova</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ul><li><p>RabbitMQ配置</p><p>Nova和其它组件之间，以及Nova各个服务之间的通信都要通过消息队列来进行，所以需要配置RabbmitMQ的连接。注意是DEFAULT配置栏目下面。</p></li></ul><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# vim /etc/nova/nova.conf</span></span>
<span class="line"><span style="color:#a6accd">[DEFAULT]</span></span>
<span class="line"><span style="color:#a6accd">transport_url = rabbit://openstack:openstack@192.168.56.11</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ul><li><p>Keystone相关配置</p><p>Nova需要连接到Keystone进行认证和服务管理。</p></li></ul><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# vim /etc/nova/nova.conf</span></span>
<span class="line"><span style="color:#a6accd">[api]</span></span>
<span class="line"><span style="color:#a6accd">auth_strategy=keystone</span></span>
<span class="line"><span style="color:#a6accd">[keystone_authtoken]</span></span>
<span class="line"><span style="color:#a6accd">www_authenticate_uri = http://192.168.56.11:5000/</span></span>
<span class="line"><span style="color:#a6accd">auth_url = http://192.168.56.11:5000/</span></span>
<span class="line"><span style="color:#a6accd">memcached_servers = 192.168.56.11:11211</span></span>
<span class="line"><span style="color:#a6accd">auth_type = password</span></span>
<span class="line"><span style="color:#a6accd">project_domain_name = Default</span></span>
<span class="line"><span style="color:#a6accd">user_domain_name = Default</span></span>
<span class="line"><span style="color:#a6accd">project_name = service</span></span>
<span class="line"><span style="color:#a6accd">username = nova</span></span>
<span class="line"><span style="color:#a6accd">password = nova</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ul><li><p>关闭Nova的防火墙功能</p><p>因为我们要使用Neutron的防火墙功能，所以关闭Nova自己的防火墙驱动。</p></li></ul><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[DEFAULT]</span></span>
<span class="line"><span style="color:#a6accd">use_neutron=true</span></span>
<span class="line"><span style="color:#a6accd">firewall_driver = nova.virt.firewall.NoopFirewallDriver</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ul><li><p>VNC配置</p><p>vnc的配置用于虚拟机VNC端口的监听和novncproxy的配置，这样后面我们通过DashBoard的novnc界面就可以查看虚拟机的界面。</p></li></ul><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# vim /etc/nova/nova.conf</span></span>
<span class="line"><span style="color:#a6accd">[vnc]</span></span>
<span class="line"><span style="color:#a6accd">enabled=true</span></span>
<span class="line"><span style="color:#a6accd">server_listen = 0.0.0.0</span></span>
<span class="line"><span style="color:#a6accd">server_proxyclient_address = 192.168.56.11</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ul><li>设置glance</li></ul><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[glance]</span></span>
<span class="line"><span style="color:#a6accd">api_servers = http://192.168.56.11:9292</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ul><li>在 [oslo_concurrency] 部分，配置锁路径：</li></ul><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[oslo_concurrency]</span></span>
<span class="line"><span style="color:#a6accd">lock_path=/var/lib/nova/tmp</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ul><li>设置启用的api</li></ul><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[DEFAULT]</span></span>
<span class="line"><span style="color:#a6accd">enabled_apis=osapi_compute,metadata</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ul><li>设置placement</li></ul><h1 id="新增到配置文件末尾" tabindex="-1">新增到配置文件末尾 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#新增到配置文件末尾" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h1><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[placement]</span></span>
<span class="line"><span style="color:#a6accd">region_name = RegionOne</span></span>
<span class="line"><span style="color:#a6accd">project_domain_name = Default</span></span>
<span class="line"><span style="color:#a6accd">project_name = service</span></span>
<span class="line"><span style="color:#a6accd">auth_type = password</span></span>
<span class="line"><span style="color:#a6accd">user_domain_name = Default</span></span>
<span class="line"><span style="color:#a6accd">auth_url = http://192.168.56.11:5000/v3</span></span>
<span class="line"><span style="color:#a6accd">username = placement</span></span>
<span class="line"><span style="color:#a6accd">password = placement</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li>同步数据库</li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# su -s /bin/sh -c "nova-manage api_db sync" nova</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ul><li>注册cell0数据库</li></ul><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# su -s /bin/sh -c "nova-manage cell_v2 map_cell0" nova</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ul><li>创建cell1的cell</li></ul><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# su -s /bin/sh -c "nova-manage cell_v2 create_cell --name=cell1 --verbose" nova</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ul><li>同步nova数据库</li></ul><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# su -s /bin/sh -c "nova-manage db sync" nova</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ul><li>验证cell0和cell1的注册是否正确</li></ul><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# su -s /bin/sh -c "nova-manage cell_v2 list_cells" nova</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ul><li>测试数据库同步情况</li></ul><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# mysql -h 192.168.56.11 -unova -pnova -e "use nova;show tables;"</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# mysql -h 192.168.56.11 -unova -pnova -e "use nova_api;show tables;"</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li>启动Nova Service<ul><li>设置开机自动启动</li></ul></li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# systemctl enable openstack-nova-api.service \</span></span>
<span class="line"><span style="color:#a6accd">openstack-nova-scheduler.service \</span></span>
<span class="line"><span style="color:#a6accd">openstack-nova-conductor.service \</span></span>
<span class="line"><span style="color:#a6accd">openstack-nova-novncproxy.service</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ul><li>启动控制节点服务</li></ul><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# systemctl start openstack-nova-api.service \</span></span>
<span class="line"><span style="color:#a6accd">  openstack-nova-scheduler.service openstack-nova-conductor.service \</span></span>
<span class="line"><span style="color:#a6accd">  openstack-nova-novncproxy.service</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li><p>Nova服务注册</p><p>和Keystone、Glance一样，Nova服务也需要在Keystone上做注册，并告诉Keystone它所暴漏的API。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd"># source admin-openstack.sh</span></span>
<span class="line"><span style="color:#a6accd"># openstack service create --name nova --description "OpenStack Compute" compute</span></span>
<span class="line"><span style="color:#a6accd"># openstack endpoint create --region RegionOne compute public http://192.168.56.11:8774/v2.1</span></span>
<span class="line"><span style="color:#a6accd"># openstack endpoint create --region RegionOne compute internal http://192.168.56.11:8774/v2.1</span></span>
<span class="line"><span style="color:#a6accd"># openstack endpoint create --region RegionOne compute admin http://192.168.56.11:8774/v2.1</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ul><li>验证控制节点服务</li></ul></li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@openstack-node1 ~]# openstack compute service list</span></span>
<span class="line"><span style="color:#a6accd">+----+----------------+-----------------------------+----------+---------+-------+----------------------------+</span></span>
<span class="line"><span style="color:#a6accd">| ID | Binary         | Host                        | Zone     | Status  | State | Updated At                 |</span></span>
<span class="line"><span style="color:#a6accd">+----+----------------+-----------------------------+----------+---------+-------+----------------------------+</span></span>
<span class="line"><span style="color:#a6accd">|  4 | nova-scheduler | linux-node1.example.com | internal | enabled | up    | 2020-02-04T04:48:47.000000 |</span></span>
<span class="line"><span style="color:#a6accd">| 11 | nova-conductor | linux-node1.example.com | internal | enabled | up    | 2020-02-04T04:48:47.000000 |</span></span>
<span class="line"><span style="color:#a6accd">+----+----------------+-----------------------------+----------+---------+-------+----------------------------+</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h3 id="nova计算节点部署" tabindex="-1">Nova计算节点部署 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#nova计算节点部署" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>根据我们开始实验规划，我们将使用一个控制节点和一个计算节点。计算节点是真正运行虚拟机的服务器，对CPU、内存和硬盘要求都比较高，通常是配置比较强劲的服务器充当。当然在实验的过程中，完全可以和控制节点在一台服务器上。好的，现在请你打开第二台虚拟机并且登陆。</p><ol><li>计算节点安装</li></ol><p>计算节点需要依赖与虚拟化技术来运行虚拟机实例，还记得前面我们讨论OpenStack与虚拟化之间的关系吗？OpenStack默认使用KVM作为Hypervisor，所以在计算节点上需要安装KVM和libvirt。这样OpenStack的控制节点上的nova服务就可以调用Nova-compute进行虚拟机的创建和管理。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node2 ~]# yum install -y openstack-nova-compute</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ul><li><p>复制配置文件</p><p>计算节点的配置文件和控制节点的基本相同，我们可以在控制节点nova.conf的基础上进行增加即可，所以首先请将控制节点的配置文件直接复制过来吧。</p></li></ul><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# scp /etc/nova/nova.conf 192.168.56.12:/etc/nova/nova.conf</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node2 ~]# chown root:nova /etc/nova/nova.conf</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ul><li><p>删除多余的数据配置</p><p>删除数据库配置 相对于控制节点的配置，计算节点需要变更VNC的配置。</p></li><li><p>修改VNC配置</p><p>计算节点需要监听所有IP，同时设置novncproxy的访问地址</p></li></ul><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[vnc]</span></span>
<span class="line"><span style="color:#a6accd">enabled=true</span></span>
<span class="line"><span style="color:#a6accd">server_listen = 0.0.0.0</span></span>
<span class="line"><span style="color:#a6accd">server_proxyclient_address = 192.168.56.12</span></span>
<span class="line"><span style="color:#a6accd">novncproxy_base_url = http://192.168.56.11:6080/vnc_auto.html</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ul><li><p>虚拟化适配</p><p>查看计算节点的服务器是否支持硬件虚拟化。</p></li></ul><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node2 ~]# egrep -c '(vmx|svm)' /proc/cpuinfo</span></span>
<span class="line"><span style="color:#a6accd">如果返回的是0，表示不支持硬件虚拟化，需要在nova.conf里面设置</span></span>
<span class="line"><span style="color:#a6accd">[libvirt]</span></span>
<span class="line"><span style="color:#a6accd">virt_type=qemu</span></span>
<span class="line"><span style="color:#a6accd">如果返回的是非0的值，那么表示计算节点服务器支持硬件虚拟化，需要在nova.conf里面设置</span></span>
<span class="line"><span style="color:#a6accd">[libvirt]</span></span>
<span class="line"><span style="color:#a6accd">virt_type=kvm</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li>启动nova-compute</li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd"># systemctl enable libvirtd.service openstack-nova-compute.service</span></span>
<span class="line"><span style="color:#a6accd"># systemctl start libvirtd.service openstack-nova-compute.service</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li>计算节点加入控制节点</li></ol><p>当添加新的计算节点时，必须在控制器节点上运行nova-manage cell_v2 discover_hosts来注册这些新的计算节点。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# su -s /bin/sh -c "nova-manage cell_v2 discover_hosts --verbose" nova</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>或者，您可以在/etc/nova/nova.conf中设置适当的间隔:</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[scheduler]</span></span>
<span class="line"><span style="color:#a6accd">discover_hosts_in_cells_interval = 300</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li>验证计算节点</li></ol><p>在控制节点再次执行host list，可以发现nova-compute已经成功注册。 再次执行host list可以看到。计算节点已经注册成功。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# openstack host list</span></span>
<span class="line"><span style="color:#a6accd">+------------------------+-------------+----------+</span></span>
<span class="line"><span style="color:#a6accd">| Host Name              | Service     | Zone     |</span></span>
<span class="line"><span style="color:#a6accd">+------------------------+-------------+----------+</span></span>
<span class="line"><span style="color:#a6accd">| linux-node1.example.com | conductor   | internal |</span></span>
<span class="line"><span style="color:#a6accd">| linux-node1.example.com | consoleauth | internal |</span></span>
<span class="line"><span style="color:#a6accd">| linux-node1.example.com | scheduler   | internal |</span></span>
<span class="line"><span style="color:#a6accd">| linux-node2.example.com | compute     | nova     |</span></span>
<span class="line"><span style="color:#a6accd">+------------------------+-------------+----------+</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h3 id="验证nova安装" tabindex="-1">验证Nova安装 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#验证nova安装" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# openstack compute service list</span></span>
<span class="line"><span style="color:#a6accd">+----+------------------+-------------------------+----------+---------+-------+----------------------------+</span></span>
<span class="line"><span style="color:#a6accd">| ID | Binary           | Host                    | Zone     | Status  | State | Updated At                 |</span></span>
<span class="line"><span style="color:#a6accd">+----+------------------+-------------------------+----------+---------+-------+----------------------------+</span></span>
<span class="line"><span style="color:#a6accd">|  1 | nova-consoleauth | linux-node1.example.com | internal | enabled | up    | 2018-02-03T10:38:30.000000 |</span></span>
<span class="line"><span style="color:#a6accd">|  2 | nova-conductor   | linux-node1.example.com | internal | enabled | up    | 2018-02-03T10:38:30.000000 |</span></span>
<span class="line"><span style="color:#a6accd">|  3 | nova-scheduler   | linux-node1.example.com | internal | enabled | up    | 2018-02-03T10:38:30.000000 |</span></span>
<span class="line"><span style="color:#a6accd">|  6 | nova-compute     | linux-node2.example.com | nova     | enabled | up    | None                       |</span></span>
<span class="line"><span style="color:#a6accd">+----+------------------+-------------------------+----------+---------+-------+----------------------------+</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>验证所有的Endpoint API是否正确</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# openstack catalog list</span></span>
<span class="line"><span style="color:#a6accd">+-----------+-----------+--------------------------------------------+</span></span>
<span class="line"><span style="color:#a6accd">| Name      | Type      | Endpoints                                  |</span></span>
<span class="line"><span style="color:#a6accd">+-----------+-----------+--------------------------------------------+</span></span>
<span class="line"><span style="color:#a6accd">| placement | placement | RegionOne                                  |</span></span>
<span class="line"><span style="color:#a6accd">|           |           |   public: http://192.168.56.11:8778        |</span></span>
<span class="line"><span style="color:#a6accd">|           |           | RegionOne                                  |</span></span>
<span class="line"><span style="color:#a6accd">|           |           |   internal: http://192.168.56.11:8778      |</span></span>
<span class="line"><span style="color:#a6accd">|           |           | RegionOne                                  |</span></span>
<span class="line"><span style="color:#a6accd">|           |           |   admin: http://192.168.56.11:8778         |</span></span>
<span class="line"><span style="color:#a6accd">|           |           |                                            |</span></span>
<span class="line"><span style="color:#a6accd">| keystone  | identity  | RegionOne                                  |</span></span>
<span class="line"><span style="color:#a6accd">|           |           |   public: http://192.168.56.11:5000/v3/    |</span></span>
<span class="line"><span style="color:#a6accd">|           |           | RegionOne                                  |</span></span>
<span class="line"><span style="color:#a6accd">|           |           |   admin: http://192.168.56.11:35357/v3/    |</span></span>
<span class="line"><span style="color:#a6accd">|           |           | RegionOne                                  |</span></span>
<span class="line"><span style="color:#a6accd">|           |           |   internal: http://192.168.56.11:35357/v3/ |</span></span>
<span class="line"><span style="color:#a6accd">|           |           |                                            |</span></span>
<span class="line"><span style="color:#a6accd">| glance    | image     | RegionOne                                  |</span></span>
<span class="line"><span style="color:#a6accd">|           |           |   public: http://192.168.56.11:9292        |</span></span>
<span class="line"><span style="color:#a6accd">|           |           | RegionOne                                  |</span></span>
<span class="line"><span style="color:#a6accd">|           |           |   admin: http://192.168.56.11:9292         |</span></span>
<span class="line"><span style="color:#a6accd">|           |           |                                            |</span></span>
<span class="line"><span style="color:#a6accd">| nova      | compute   | RegionOne                                  |</span></span>
<span class="line"><span style="color:#a6accd">|           |           |   admin: http://192.168.56.11:8774/v2.1    |</span></span>
<span class="line"><span style="color:#a6accd">|           |           | RegionOne                                  |</span></span>
<span class="line"><span style="color:#a6accd">|           |           |   public: http://192.168.56.11:8774/v2.1   |</span></span>
<span class="line"><span style="color:#a6accd">|           |           | RegionOne                                  |</span></span>
<span class="line"><span style="color:#a6accd">|           |           |   internal: http://192.168.56.11:8774/v2.1 |</span></span>
<span class="line"><span style="color:#a6accd">|           |           |                                            |</span></span>
<span class="line"><span style="color:#a6accd">+-----------+-----------+--------------------------------------------+</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>检查cells和placement API状态</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# nova-status upgrade check</span></span>
<span class="line"><span style="color:#a6accd">+---------------------------+</span></span>
<span class="line"><span style="color:#a6accd">| Upgrade Check Results     |</span></span>
<span class="line"><span style="color:#a6accd">+---------------------------+</span></span>
<span class="line"><span style="color:#a6accd">| Check: Cells v2           |</span></span>
<span class="line"><span style="color:#a6accd">| Result: Success           |</span></span>
<span class="line"><span style="color:#a6accd">| Details: None             |</span></span>
<span class="line"><span style="color:#a6accd">+---------------------------+</span></span>
<span class="line"><span style="color:#a6accd">| Check: Placement API      |</span></span>
<span class="line"><span style="color:#a6accd">| Result: Success           |</span></span>
<span class="line"><span style="color:#a6accd">| Details: None             |</span></span>
<span class="line"><span style="color:#a6accd">+---------------------------+</span></span>
<span class="line"><span style="color:#a6accd">| Check: Resource Providers |</span></span>
<span class="line"><span style="color:#a6accd">| Result: Success           |</span></span>
<span class="line"><span style="color:#a6accd">| Details: None             |</span></span>
<span class="line"><span style="color:#a6accd">+---------------------------+</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h1 id="_1-7-openstack网络服务neutron" tabindex="-1">1.7 OpenStack网络服务Neutron <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_1-7-openstack网络服务neutron" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h1><p>上一章我们讲到Nova服务提供了OpenStack平台计算资源池，实现了计算即服务。那么仅仅有一台孤立的云主机是无法正常使用的，我们需要把它接入网络，才能正常对外提供服务。</p><h3 id="neutron概述" tabindex="-1">Neutron概述 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#neutron概述" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>OpenStack Networking Services（Neutron），OpenStack 网络服务，OpenStack核心项目之一，由早期的nova-network独立成一个子项目后演变而来，它为OpenStack提供了云计算环境下的虚拟网络功能。</p><p>在OpenStack世界中，网络组件最初叫nova-network，nova-network实现简单，直接采用基于Linux内核的Linux网桥。由于少了很多层抽象，所以比较简单稳定算。但是它的不足之处是支持的插件少（只支持Linux网桥），支持的网络拓扑少（只支持flat, vlan)。</p><p>为了支持更多的插件，支持更多的网络拓扑，于是有了quantum工程。quantum插件不仅支持Linux网桥，也支持OpenvSwitch，以及一些SDN的插件和其他商业公司的插件。在网络拓扑上，除了支持flat，vlan外，还支持gre, vxlan。quantum因为和一家公司的名称冲突，于是，改名为neutron。</p><ol><li>Neutron Server</li></ol><p>这一部分包含守护进程neutron-server和各种插件neutron-*-plugin。neutron-server提供API接口，并把对API的调用请求传给已经配置好的插件进行后续处理。插件需要访问数据库来维护各种配置数据和对应关系，例如路由器、网络、子网、端口、浮动IP、安全组等等。</p><ol><li>ML2(Module Layer2)plugin</li></ol><p>上述整体框架中提到Neutron组件内部有很多个不同的插件（plugin），而在Neutron中，实现一个插件包括两个部分的内容：一部分与数据库db打交道称之为plugin（虽然都是plugin，但是这个是具体实现中的plugin），一部分是调用具体的网络设备真正干活的称之为agent。 与db进行交互的plugin在功能上有很多重复，所以在代码上有很多重复，因此在Neutron中提供了一个公共的plugin叫ML2(ModuleLayer2) plugin。所以ML2 plugin的第一个作用就是：提供与数据库交互的公共plugin。 ML2的第二个作用就是实现支持多种pulgin（原先使用linux bridge，就不能用openvswitch），ML2通过MechanismDriver实现。MechanismDriver的作用是将agent的类型agent_type和vif_type关联，这样vif_type就可以直接通过扩展api设置了。 ML2的第三个作用就是支持不同的网络拓扑，如flat, vlan, gre, vxlan，直接在ml2_conf.ini这个配置文件里都配上即可。</p><ol><li><p>L3-Agent</p><p>上面的ml2解决的只是网络中L2层的问题，对于L3层的路由功能，neturon单独用l3-agent实现，为客户机访问外部网络提供3层转发服务。 L3 层的路由分静态路由和动态路由两种： 在 Linux 中，通过打开 ipv4 forward 特性可以让数据包从一块网卡路由到另外一块网卡上。 动态路由，如内部网关协议 RIP，OSPF；如外部网关协议 BGP。能够自动地学习建立路由表。</p></li></ol><p>目前 Neutron 只支持静态路由，要点如下： 对于不同 tenant 子网通过 namespace 功能进行隔离，在 Linux 中，一个命名空间 namespace 您可以简单理解成 linux 又启动了一个新的 TCP/IP 栈的进程。多个 tenant 意味着多个 namespace，也意味着多个 TCP/IP 栈。 对于同一tenant 中的不同子网的隔离通过 iptables 来做，也意味着，同一tenant中的相同子网的两个虚机不用走 L3 层，直接走 L2 层能通，没问题；但如果同一tenant中的不同子网的两个虚机要通讯的话，必须得绕道 L3-agent 网络节点，这是影响性能的。</p><ol><li>dhcp-agent</li></ol><p>Dhcp-agent主要负责为各个租户网络提供DHCP服务。 L4-L7的Agent 至于再之上的L4-L7层的FwaaS,VPNaaS, DNATaaS, DNSaaS的内容，在neutron又出来一个新的服务框架用于将所有这些除L2层以外的网络服务类似于上述ml2的思想在数据库这块一网打尽。 下面我们通过一个单一扁平的提供者网络FLAT来部署neutron。为虚拟机提供网络资源。</p><h3 id="neutron控制节点部署" tabindex="-1">Neutron控制节点部署 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#neutron控制节点部署" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><ol><li>Neutron安装</li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# yum install -y openstack-neutron openstack-neutron-ml2 \</span></span>
<span class="line"><span style="color:#a6accd">openstack-neutron-linuxbridge ebtables</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li>Neutron配置<ul><li>Neutron数据库配置</li></ul></li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# vim /etc/neutron/neutron.conf</span></span>
<span class="line"><span style="color:#a6accd">[database]</span></span>
<span class="line"><span style="color:#a6accd">connection = mysql+pymysql://neutron:neutron@192.168.56.11:3306/neutron</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ul><li>Keystone连接配置</li></ul><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[DEFAULT]</span></span>
<span class="line"><span style="color:#a6accd">…</span></span>
<span class="line"><span style="color:#a6accd">#注意：该配置默认不存在，需要添加</span></span>
<span class="line"><span style="color:#a6accd">auth_strategy = keystone</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">[keystone_authtoken]</span></span>
<span class="line"><span style="color:#a6accd">www_authenticate_uri = http://192.168.56.11:5000</span></span>
<span class="line"><span style="color:#a6accd">auth_url = http://192.168.56.11:5000</span></span>
<span class="line"><span style="color:#a6accd">memcached_servers = 192.168.56.11:11211</span></span>
<span class="line"><span style="color:#a6accd">auth_type = password</span></span>
<span class="line"><span style="color:#a6accd">project_domain_name = default</span></span>
<span class="line"><span style="color:#a6accd">user_domain_name = default</span></span>
<span class="line"><span style="color:#a6accd">project_name = service</span></span>
<span class="line"><span style="color:#a6accd">username = neutron</span></span>
<span class="line"><span style="color:#a6accd">password = neutron</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ul><li>RabbitMQ相关设置</li></ul><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# vim /etc/neutron/neutron.conf</span></span>
<span class="line"><span style="color:#a6accd">[DEFAULT]</span></span>
<span class="line"><span style="color:#a6accd">transport_url = rabbit://openstack:openstack@192.168.56.11</span></span>
<span class="line"><span style="color:#a6accd">#请注意是在DEFAULT配置栏目下，因为该配置文件有多个transport_url的配置</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ul><li>Neutron网络基础配置</li></ul><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">#注意：该配置默认不存在，需要添加</span></span>
<span class="line"><span style="color:#a6accd">[DEFAULT]</span></span>
<span class="line"><span style="color:#a6accd">core_plugin = ml2</span></span>
<span class="line"><span style="color:#a6accd"># 这个配置没有错，就是设置为空</span></span>
<span class="line"><span style="color:#a6accd">service_plugins =</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ul><li>网络拓扑变化Nova通知配置</li></ul><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[DEFAULT]</span></span>
<span class="line"><span style="color:#a6accd"># 这两行配置需要新增</span></span>
<span class="line"><span style="color:#a6accd">notify_nova_on_port_status_changes = True</span></span>
<span class="line"><span style="color:#a6accd">notify_nova_on_port_data_changes = True</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd"># 在配置文件最后增加以下配置段</span></span>
<span class="line"><span style="color:#a6accd">[nova]</span></span>
<span class="line"><span style="color:#a6accd">auth_url = http://192.168.56.11:5000</span></span>
<span class="line"><span style="color:#a6accd">auth_type = password</span></span>
<span class="line"><span style="color:#a6accd">project_domain_name = default</span></span>
<span class="line"><span style="color:#a6accd">user_domain_name = default</span></span>
<span class="line"><span style="color:#a6accd">region_name = RegionOne</span></span>
<span class="line"><span style="color:#a6accd">project_name = service</span></span>
<span class="line"><span style="color:#a6accd">username = nova</span></span>
<span class="line"><span style="color:#a6accd">password = nova</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ul><li>在 [oslo_concurrency] 部分，配置锁路径：</li></ul><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[oslo_concurrency]</span></span>
<span class="line"><span style="color:#a6accd">lock_path = /var/lib/neutron/tmp</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li>Neutron ML2配置</li></ol><p>还是在控制节点上，我们需要配置Neutron ML2。ML2使用Linux桥接机制为实例创建Layer-2的虚拟网络基础设施。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# vim /etc/neutron/plugins/ml2/ml2_conf.ini</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">#以下所有配置目前均需要新增，默认配置文件已经移除，注意删除注释。</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">[ml2]</span></span>
<span class="line"><span style="color:#a6accd">type_drivers = flat,vlan,gre,vxlan,geneve #支持多选，所以把所有的驱动都选择上。</span></span>
<span class="line"><span style="color:#a6accd">tenant_network_types = flat,vlan,gre,vxlan,geneve #支持多项，所以把所有的网络类型都选择上。</span></span>
<span class="line"><span style="color:#a6accd">mechanism_drivers = linuxbridge,openvswitch,l2population #选择插件驱动，支持多选，开源的有linuxbridge和openvswitch</span></span>
<span class="line"><span style="color:#a6accd">#启用端口安全扩展驱动</span></span>
<span class="line"><span style="color:#a6accd">extension_drivers = port_security,qos </span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">[ml2_type_flat]</span></span>
<span class="line"><span style="color:#a6accd">#设置网络提供</span></span>
<span class="line"><span style="color:#a6accd">flat_networks = provider</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">[securitygroup]</span></span>
<span class="line"><span style="color:#a6accd">#启用ipset</span></span>
<span class="line"><span style="color:#a6accd">enable_ipset = True</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li>Neutron Linuxbridge配置</li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# vim /etc/neutron/plugins/ml2/linuxbridge_agent.ini</span></span>
<span class="line"><span style="color:#a6accd">#以下所有配置目前均需要新增，默认配置文件已经移除，注意删除注释。</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">[linux_bridge]</span></span>
<span class="line"><span style="color:#a6accd">#映射虚拟网络接口和实际网络接口的对应</span></span>
<span class="line"><span style="color:#a6accd">physical_interface_mappings = provider:eth0 </span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">[vxlan]</span></span>
<span class="line"><span style="color:#a6accd">#禁止vxlan网络</span></span>
<span class="line"><span style="color:#a6accd">enable_vxlan = False</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">[securitygroup]</span></span>
<span class="line"><span style="color:#a6accd">#设置使用Linux Bridge桥接的防火墙驱动和安全组</span></span>
<span class="line"><span style="color:#a6accd">firewall_driver = neutron.agent.linux.iptables_firewall.IptablesFirewallDriver</span></span>
<span class="line"><span style="color:#a6accd">enable_security_group = True</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li>Neutron DHCP-Agent配置</li></ol><p>Neutron中的dhcp-agent用来为云主机动态分配IP地址，DHCP Agent默认是通过调用dnsmasq来实现DHCP的分配工作。默认情况下dhcp-agent和后面要介绍的Lbaas-agent都需要依赖namespace,所以开始之前我们需要先介绍下linux的namespache。</p><p><strong>什么是namespace</strong> Namespace(命名空间)，简单的说就是不同的名字空间，打个简单的比方，进程在a空间是叫a进程，在b空间也就可能叫b进程。为什么要有命名空间呢？主要是满足虚拟化的一些需求。试想，如果有一台机器，不管它是个人pc还是服务器，或是 网络交换机，路由器，一般情况下，它也就被一个用户使用。但是如果某一天，另外一个用户也要使用同样的机器，一种方法是再买一个机器，装上同样的 linux系统，但是还有一种更好的方法，就是使用容器（container），每个用户有属于自己的容器，而且容器之间相互隔离，而namespace 就是实现容器的一种手段。Namespace可以实现pid、net、ipc、mnt、uts、user等namespace将容器的进程、网络、消息、文件系统、UTS("UNIX Time-sharing System")和用户空间隔离开。在Docker中，我们会详解讲解namespace。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# vim /etc/neutron/dhcp_agent.ini</span></span>
<span class="line"><span style="color:#a6accd">[DEFAULT]</span></span>
<span class="line"><span style="color:#a6accd">interface_driver = linuxbridge</span></span>
<span class="line"><span style="color:#a6accd">dhcp_driver = neutron.agent.linux.dhcp.Dnsmasq</span></span>
<span class="line"><span style="color:#a6accd">enable_isolated_metadata = True</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li>Neutron metadata配置</li></ol><p>metadata agent为实例提供诸如凭证的配置信息。需要设置Keystone的链接和nova的连接参数。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# vim /etc/neutron/metadata_agent.ini</span></span>
<span class="line"><span style="color:#a6accd">[DEFAULT]</span></span>
<span class="line"><span style="color:#a6accd">nova_metadata_host = 192.168.56.11</span></span>
<span class="line"><span style="color:#a6accd">#注意这个是为元数据代理设置的密码，需要和nova的配置相对应。</span></span>
<span class="line"><span style="color:#a6accd">metadata_proxy_shared_secret = unixhot.com</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li>Neutron相关配置在nova.conf</li></ol><p>Neutron的配置，需要修改nova的配置文件，因为默认大多数OpenStack发行版里nova.conf里面的网络相关的配置都是比较老的，使用的最早的nova-network的配置。所以需要进行修改。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# vim /etc/nova/nova.conf </span></span>
<span class="line"><span style="color:#a6accd">[neutron]</span></span>
<span class="line"><span style="color:#a6accd">auth_url = http://192.168.56.11:5000</span></span>
<span class="line"><span style="color:#a6accd">auth_type = password</span></span>
<span class="line"><span style="color:#a6accd">project_domain_name = default</span></span>
<span class="line"><span style="color:#a6accd">user_domain_name = default</span></span>
<span class="line"><span style="color:#a6accd">region_name = RegionOne</span></span>
<span class="line"><span style="color:#a6accd">project_name = service</span></span>
<span class="line"><span style="color:#a6accd">username = neutron</span></span>
<span class="line"><span style="color:#a6accd">password = neutron</span></span>
<span class="line"><span style="color:#a6accd">service_metadata_proxy = true</span></span>
<span class="line"><span style="color:#a6accd">metadata_proxy_shared_secret = unixhot.com</span></span>
<span class="line"><span style="color:#a6accd">#注意这里设置的共享秘钥需要和之前的对应上。</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li>配置完毕后，我们需要设置一个软连接，因为默认服务会读取plugin.ini。</li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# ln -s /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugin.ini</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li>同步数据库</li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# su -s /bin/sh -c "neutron-db-manage --config-file /etc/neutron/neutron.conf \</span></span>
<span class="line"><span style="color:#a6accd">--config-file /etc/neutron/plugins/ml2/ml2_conf.ini upgrade head" neutron</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li>重启计算API 服务</li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# systemctl restart openstack-nova-api.service</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li>启动网络服务并配置他们开机自启动。</li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd"># systemctl enable neutron-server.service \</span></span>
<span class="line"><span style="color:#a6accd">  neutron-linuxbridge-agent.service neutron-dhcp-agent.service \</span></span>
<span class="line"><span style="color:#a6accd">  neutron-metadata-agent.service</span></span>
<span class="line"><span style="color:#a6accd"># systemctl start neutron-server.service \</span></span>
<span class="line"><span style="color:#a6accd">  neutron-linuxbridge-agent.service neutron-dhcp-agent.service \</span></span>
<span class="line"><span style="color:#a6accd">  neutron-metadata-agent.service</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li>Neutron服务注册</li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">创建service</span></span>
<span class="line"><span style="color:#a6accd"># openstack service create --name neutron --description "OpenStack Networking" network</span></span>
<span class="line"><span style="color:#a6accd">创建endpoint</span></span>
<span class="line"><span style="color:#a6accd"># openstack endpoint create --region RegionOne network public http://192.168.56.11:9696</span></span>
<span class="line"><span style="color:#a6accd"># openstack endpoint create --region RegionOne network internal http://192.168.56.11:9696</span></span>
<span class="line"><span style="color:#a6accd"># openstack endpoint create --region RegionOne network admin http://192.168.56.11:9696</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li>测试Neutron安装**</li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@openstack-compute-node1 neutron]# openstack network agent list</span></span>
<span class="line"><span style="color:#a6accd">WARNING: Failed to import plugin orchestration.</span></span>
<span class="line"><span style="color:#a6accd">WARNING: Failed to import plugin data_processing.</span></span>
<span class="line"><span style="color:#a6accd">+--------------------------------------+--------------------+-------------------------------------+-------------------+-------+-------+---------------------------+</span></span>
<span class="line"><span style="color:#a6accd">| ID                                   | Agent Type         | Host                                | Availability Zone | Alive | State | Binary                    |</span></span>
<span class="line"><span style="color:#a6accd">+--------------------------------------+--------------------+-------------------------------------+-------------------+-------+-------+---------------------------+</span></span>
<span class="line"><span style="color:#a6accd">| c4a78188-a3ad-4cda-8c47-41f854aad375 | Linux bridge agent | openstack-compute-node1.dianjoy.com | None              | :-)   | UP    | neutron-linuxbridge-agent |</span></span>
<span class="line"><span style="color:#a6accd">| e2325b10-281b-447c-b46f-7875c0eda1fc | Metadata agent     | openstack-compute-node1.dianjoy.com | None              | :-)   | UP    | neutron-metadata-agent    |</span></span>
<span class="line"><span style="color:#a6accd">| fb73fabc-2825-42c4-ae24-ea50bb1d37b8 | DHCP agent         | openstack-compute-node1.dianjoy.com | nova              | :-)   | UP    | neutron-dhcp-agent        |</span></span>
<span class="line"><span style="color:#a6accd">+--------------------------------------+--------------------+-------------------------------------+-------------------+-------+-------+---------------------------+</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>如果可以正常输出，基本上Neutron的安装就没有问题。</p><h3 id="neutron计算节点部署" tabindex="-1">Neutron计算节点部署 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#neutron计算节点部署" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><ol><li>安装软件包</li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd"> [root@linux-node2 ~]# yum install -y openstack-neutron openstack-neutron-linuxbridge ebtables</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li>配置计算节点Neutron<ul><li>Keystone连接配置</li></ul></li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node2 ~]# vim /etc/neutron/neutron.conf</span></span>
<span class="line"><span style="color:#a6accd">[DEFAULT]</span></span>
<span class="line"><span style="color:#a6accd">auth_strategy = keystone</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">[keystone_authtoken]</span></span>
<span class="line"><span style="color:#a6accd">www_authenticate_uri = http://192.168.56.11:5000</span></span>
<span class="line"><span style="color:#a6accd">auth_url = http://192.168.56.11:5000</span></span>
<span class="line"><span style="color:#a6accd">memcached_servers = 192.168.56.11:11211</span></span>
<span class="line"><span style="color:#a6accd">auth_type = password</span></span>
<span class="line"><span style="color:#a6accd">project_domain_name = default</span></span>
<span class="line"><span style="color:#a6accd">user_domain_name = default</span></span>
<span class="line"><span style="color:#a6accd">project_name = service</span></span>
<span class="line"><span style="color:#a6accd">username = neutron</span></span>
<span class="line"><span style="color:#a6accd">password = neutron</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ul><li>RabbitMQ相关设置</li></ul><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# vim /etc/neutron/neutron.conf</span></span>
<span class="line"><span style="color:#a6accd">[DEFAULT]</span></span>
<span class="line"><span style="color:#a6accd">transport_url = rabbit://openstack:openstack@192.168.56.11</span></span>
<span class="line"><span style="color:#a6accd">#请注意是在DEFAULT配置栏目下，因为该配置文件有多个transport_url的配置</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ul><li>锁路径</li></ul><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[oslo_concurrency]</span></span>
<span class="line"><span style="color:#a6accd">lock_path = /var/lib/neutron/tmp</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li>配置LinuxBridge配置</li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# scp /etc/neutron/plugins/ml2/linuxbridge_agent.ini 192.168.56.12:/etc/neutron/plugins/ml2/</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li>设置计算节点的nova.conf</li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node2 ~]# vim /etc/nova/nova.conf</span></span>
<span class="line"><span style="color:#a6accd">[neutron]</span></span>
<span class="line"><span style="color:#a6accd">auth_url = http://192.168.56.11:5000</span></span>
<span class="line"><span style="color:#a6accd">auth_type = password</span></span>
<span class="line"><span style="color:#a6accd">project_domain_name = default</span></span>
<span class="line"><span style="color:#a6accd">user_domain_name = default</span></span>
<span class="line"><span style="color:#a6accd">region_name = RegionOne</span></span>
<span class="line"><span style="color:#a6accd">project_name = service</span></span>
<span class="line"><span style="color:#a6accd">username = neutron</span></span>
<span class="line"><span style="color:#a6accd">password = neutron</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li>重启计算服务</li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node2 ~]# systemctl restart openstack-nova-compute.service</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li>启动计算节点linuxbridge-agent</li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node2 ~]# systemctl enable neutron-linuxbridge-agent.service</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node2 ~]# systemctl start neutron-linuxbridge-agent.service</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li>在控制节点上测试Neutron安装</li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# source admin-openstack.sh </span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# openstack network agent list</span></span>
<span class="line"><span style="color:#a6accd">+--------------------------------------+--------------------+-----------------------------+-------------------+-------+-------+---------------------------+</span></span>
<span class="line"><span style="color:#a6accd">| ID                                   | Agent Type         | Host                        | Availability Zone | Alive | State | Binary                    |</span></span>
<span class="line"><span style="color:#a6accd">+--------------------------------------+--------------------+-----------------------------+-------------------+-------+-------+---------------------------+</span></span>
<span class="line"><span style="color:#a6accd">| 11f1de57-836f-412f-b9b4-51dcdc9ae931 | Metadata agent     | linux-node1.example.com | None              | :-)   | UP    | neutron-metadata-agent    |</span></span>
<span class="line"><span style="color:#a6accd">| 197b84cd-7d6e-4ed2-9fa0-46499fe7cdb7 | Linux bridge agent | linux-node2.example.com | None              | :-)   | UP    | neutron-linuxbridge-agent |</span></span>
<span class="line"><span style="color:#a6accd">| 531afd6d-ee81-4b64-a0f4-ec5fd523eff8 | DHCP agent         | linux-node1.example.com | nova              | :-)   | UP    | neutron-dhcp-agent        |</span></span>
<span class="line"><span style="color:#a6accd">| dfc431ff-dede-4a54-8371-653c6e72ffd9 | Linux bridge agent | linux-node1.example.com | None              | :-)   | UP    | neutron-linuxbridge-agent |</span></span>
<span class="line"><span style="color:#a6accd">+--------------------------------------+--------------------+-----------------------------+-------------------+-------+-------+---------------------------+</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>看是否有linux-node2.example.com的Linux bridge agent</p><h3 id="bug解决" tabindex="-1">Bug解决 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#bug解决" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">2020-01-29 14:09:22.599 6346 ERROR neutron.plugins.ml2.drivers.agent._common_agent [req-68a7936c-2bbc-454e-bee4-6785cd3dce7b - - - - -] Error in agent loop. Devices info: {'current': set(['tap07dc4c9c-f3']), 'timestamps': {'tap07dc4c9c-f3': 7}, 'removed': set([]), 'added': set(['tap07dc4c9c-f3']), 'updated': set([])}: KeyError: 'gateway'</span></span>
<span class="line"><span style="color:#a6accd">2020-01-29 14:09:22.599 6346 ERROR neutron.plugins.ml2.drivers.agent._common_agent Traceback (most recent call last):</span></span>
<span class="line"><span style="color:#a6accd">2020-01-29 14:09:22.599 6346 ERROR neutron.plugins.ml2.drivers.agent._common_agent   File "/usr/lib/python2.7/site-packages/neutron/plugins/ml2/drivers/agent/_common_agent.py", line 465, in daemon_loop</span></span>
<span class="line"><span style="color:#a6accd">2020-01-29 14:09:22.599 6346 ERROR neutron.plugins.ml2.drivers.agent._common_agent     sync = self.process_network_devices(device_info)</span></span>
<span class="line"><span style="color:#a6accd">2020-01-29 14:09:22.599 6346 ERROR neutron.plugins.ml2.drivers.agent._common_agent   File "/usr/lib/python2.7/site-packages/osprofiler/profiler.py", line 160, in wrapper</span></span>
<span class="line"><span style="color:#a6accd">2020-01-29 14:09:22.599 6346 ERROR neutron.plugins.ml2.drivers.agent._common_agent     result = f(*args, **kwargs)</span></span>
<span class="line"><span style="color:#a6accd">2020-01-29 14:09:22.599 6346 ERROR neutron.plugins.ml2.drivers.agent._common_agent   File "/usr/lib/python2.7/site-packages/neutron/plugins/ml2/drivers/agent/_common_agent.py", line 214, in process_network_devices</span></span>
<span class="line"><span style="color:#a6accd">2020-01-29 14:09:22.599 6346 ERROR neutron.plugins.ml2.drivers.agent._common_agent     resync_a = self.treat_devices_added_updated(devices_added_updated)</span></span>
<span class="line"><span style="color:#a6accd">2020-01-29 14:09:22.599 6346 ERROR neutron.plugins.ml2.drivers.agent._common_agent   File "/usr/lib/python2.7/site-packages/osprofiler/profiler.py", line 160, in wrapper</span></span>
<span class="line"><span style="color:#a6accd">2020-01-29 14:09:22.599 6346 ERROR neutron.plugins.ml2.drivers.agent._common_agent     result = f(*args, **kwargs)</span></span>
<span class="line"><span style="color:#a6accd">2020-01-29 14:09:22.599 6346 ERROR neutron.plugins.ml2.drivers.agent._common_agent   File "/usr/lib/python2.7/site-packages/neutron/plugins/ml2/drivers/agent/_common_agent.py", line 231, in treat_devices_added_updated</span></span>
<span class="line"><span style="color:#a6accd">2020-01-29 14:09:22.599 6346 ERROR neutron.plugins.ml2.drivers.agent._common_agent     self._process_device_if_exists(device_details)</span></span>
<span class="line"><span style="color:#a6accd">2020-01-29 14:09:22.599 6346 ERROR neutron.plugins.ml2.drivers.agent._common_agent   File "/usr/lib/python2.7/site-packages/neutron/plugins/ml2/drivers/agent/_common_agent.py", line 258, in _process_device_if_exists</span></span>
<span class="line"><span style="color:#a6accd">2020-01-29 14:09:22.599 6346 ERROR neutron.plugins.ml2.drivers.agent._common_agent     device, device_details['device_owner'])</span></span>
<span class="line"><span style="color:#a6accd">2020-01-29 14:09:22.599 6346 ERROR neutron.plugins.ml2.drivers.agent._common_agent   File "/usr/lib/python2.7/site-packages/neutron/plugins/ml2/drivers/linuxbridge/agent/linuxbridge_neutron_agent.py", line 585, in plug_interface</span></span>
<span class="line"><span style="color:#a6accd">2020-01-29 14:09:22.599 6346 ERROR neutron.plugins.ml2.drivers.agent._common_agent     network_segment.mtu)</span></span>
<span class="line"><span style="color:#a6accd">2020-01-29 14:09:22.599 6346 ERROR neutron.plugins.ml2.drivers.agent._common_agent   File "/usr/lib/python2.7/site-packages/neutron/plugins/ml2/drivers/linuxbridge/agent/linuxbridge_neutron_agent.py", line 521, in add_tap_interface</span></span>
<span class="line"><span style="color:#a6accd">2020-01-29 14:09:22.599 6346 ERROR neutron.plugins.ml2.drivers.agent._common_agent     return False</span></span>
<span class="line"><span style="color:#a6accd">2020-01-29 14:09:22.599 6346 ERROR neutron.plugins.ml2.drivers.agent._common_agent   File "/usr/lib/python2.7/site-packages/oslo_utils/excutils.py", line 220, in __exit__</span></span>
<span class="line"><span style="color:#a6accd">2020-01-29 14:09:22.599 6346 ERROR neutron.plugins.ml2.drivers.agent._common_agent     self.force_reraise()</span></span>
<span class="line"><span style="color:#a6accd">2020-01-29 14:09:22.599 6346 ERROR neutron.plugins.ml2.drivers.agent._common_agent   File "/usr/lib/python2.7/site-packages/oslo_utils/excutils.py", line 196, in force_reraise</span></span>
<span class="line"><span style="color:#a6accd">2020-01-29 14:09:22.599 6346 ERROR neutron.plugins.ml2.drivers.agent._common_agent     six.reraise(self.type_, self.value, self.tb)</span></span>
<span class="line"><span style="color:#a6accd">2020-01-29 14:09:22.599 6346 ERROR neutron.plugins.ml2.drivers.agent._common_agent   File "/usr/lib/python2.7/site-packages/neutron/plugins/ml2/drivers/linuxbridge/agent/linuxbridge_neutron_agent.py", line 513, in add_tap_interface</span></span>
<span class="line"><span style="color:#a6accd">2020-01-29 14:09:22.599 6346 ERROR neutron.plugins.ml2.drivers.agent._common_agent     tap_device_name, device_owner, mtu)</span></span>
<span class="line"><span style="color:#a6accd">2020-01-29 14:09:22.599 6346 ERROR neutron.plugins.ml2.drivers.agent._common_agent   File "/usr/lib/python2.7/site-packages/neutron/plugins/ml2/drivers/linuxbridge/agent/linuxbridge_neutron_agent.py", line 546, in _add_tap_interface</span></span>
<span class="line"><span style="color:#a6accd">2020-01-29 14:09:22.599 6346 ERROR neutron.plugins.ml2.drivers.agent._common_agent     mtu):</span></span>
<span class="line"><span style="color:#a6accd">2020-01-29 14:09:22.599 6346 ERROR neutron.plugins.ml2.drivers.agent._common_agent   File "/usr/lib/python2.7/site-packages/neutron/plugins/ml2/drivers/linuxbridge/agent/linuxbridge_neutron_agent.py", line 497, in ensure_physical_in_bridge</span></span>
<span class="line"><span style="color:#a6accd">2020-01-29 14:09:22.599 6346 ERROR neutron.plugins.ml2.drivers.agent._common_agent     physical_interface)</span></span>
<span class="line"><span style="color:#a6accd">2020-01-29 14:09:22.599 6346 ERROR neutron.plugins.ml2.drivers.agent._common_agent   File "/usr/lib/python2.7/site-packages/neutron/plugins/ml2/drivers/linuxbridge/agent/linuxbridge_neutron_agent.py", line 286, in ensure_flat_bridge</span></span>
<span class="line"><span style="color:#a6accd">2020-01-29 14:09:22.599 6346 ERROR neutron.plugins.ml2.drivers.agent._common_agent     if self.ensure_bridge(bridge_name, physical_interface):</span></span>
<span class="line"><span style="color:#a6accd">2020-01-29 14:09:22.599 6346 ERROR neutron.plugins.ml2.drivers.agent._common_agent   File "/usr/lib/python2.7/site-packages/neutron/plugins/ml2/drivers/linuxbridge/agent/linuxbridge_neutron_agent.py", line 455, in ensure_bridge</span></span>
<span class="line"><span style="color:#a6accd">2020-01-29 14:09:22.599 6346 ERROR neutron.plugins.ml2.drivers.agent._common_agent     self.update_interface_ip_details(bridge_name, interface)</span></span>
<span class="line"><span style="color:#a6accd">2020-01-29 14:09:22.599 6346 ERROR neutron.plugins.ml2.drivers.agent._common_agent   File "/usr/lib/python2.7/site-packages/neutron/plugins/ml2/drivers/linuxbridge/agent/linuxbridge_neutron_agent.py", line 417, in update_interface_ip_details</span></span>
<span class="line"><span style="color:#a6accd">2020-01-29 14:09:22.599 6346 ERROR neutron.plugins.ml2.drivers.agent._common_agent     gateway)</span></span>
<span class="line"><span style="color:#a6accd">2020-01-29 14:09:22.599 6346 ERROR neutron.plugins.ml2.drivers.agent._common_agent   File "/usr/lib/python2.7/site-packages/neutron/plugins/ml2/drivers/linuxbridge/agent/linuxbridge_neutron_agent.py", line 401, in _update_interface_ip_details</span></span>
<span class="line"><span style="color:#a6accd">2020-01-29 14:09:22.599 6346 ERROR neutron.plugins.ml2.drivers.agent._common_agent     dst_device.route.add_gateway(gateway=gateway['gateway'],</span></span>
<span class="line"><span style="color:#a6accd">2020-01-29 14:09:22.599 6346 ERROR neutron.plugins.ml2.drivers.agent._common_agent KeyError: 'gateway'</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>分别修改控制节点和计算节点上的Neutron源码，并重启服务</p><ol><li>将priority修改为metric</li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@openstack-compute-node3 ~]# vim +1503 /usr/lib/python2.7/site-packages/neutron/agent/linux/ip_lib.py</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">#'priority': get_attr(route, 'RTA_PRIORITY'),</span></span>
<span class="line"><span style="color:#a6accd">'metric': get_attr(route, 'RTA_PRIORITY'),</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li>将gateway修改为via</li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@openstack-compute-node3 ~]# vim +402 /usr/lib/python2.7/site-packages/neutron/plugins/ml2/drivers/linuxbridge/agent/linuxbridge_neutron_agent.py</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">#dst_device.route.add_gateway(gateway=gateway['gateway'],</span></span>
<span class="line"><span style="color:#a6accd">            dst_device.route.add_gateway(gateway=gateway['via'],</span></span>
<span class="line"><span style="color:#a6accd">                                         metric=metric)</span></span>
<span class="line"><span style="color:#a6accd">            #src_device.route.delete_gateway(gateway=gateway['gateway'])</span></span>
<span class="line"><span style="color:#a6accd">            src_device.route.delete_gateway(gateway=gateway['via'])</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>计算节点重启服务</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@openstack-compute-node3 ~]# systemctl restart neutron-linuxbridge-agent</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>控制节点重启服务</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@openstack-compute-node1 ~]# systemctl restart neutron-server.service   neutron-linuxbridge-agent.s</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h1 id="_1-8-创建第一台openstack云主机" tabindex="-1">1.8 创建第一台OpenStack云主机 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_1-8-创建第一台openstack云主机" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h1><p>好的，到目前为止，你已经完成了启动一台虚拟机所有的必备条件。</p><ul><li>MySQL：为各个服务提供数据存储</li><li>RabbitMQ：为各个服务之间通信提供交通枢纽</li><li>KeyStone：为各个服务器之间通信提供认证和服务注册</li><li>Glance：为虚拟机提供镜像管理</li><li>Nova：为虚拟机提供计算资源</li><li>Neutron：为虚拟机提供网络资源。</li></ul><p>现在你可以着手启动一台虚拟机，剩下的OpenStack服务都可以认为是可选的，不过通常我们会用到Dashboard来通过Web界面来管理，后面的章节我们会讲到，现在我们使用命令来创建我们的第一台虚拟机。</p><h3 id="创建flat网络" tabindex="-1">创建FLAT网络 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#创建flat网络" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>首先我们需要创建一个网络。我们现在为demo租户，创建一个FLAT类型的网络。如下图所示。</p><ol><li>创建Flant网络</li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# source admin-openstack.sh</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# openstack network create  --share --external \</span></span>
<span class="line"><span style="color:#a6accd">  --provider-physical-network provider \</span></span>
<span class="line"><span style="color:#a6accd">  --provider-network-type flat provider</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li>查看网络</li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# openstack network list</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li>创建子网</li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# openstack subnet create --network provider \</span></span>
<span class="line"><span style="color:#a6accd">  --allocation-pool start=192.168.56.100,end=192.168.56.200 \</span></span>
<span class="line"><span style="color:#a6accd">  --dns-nameserver 223.5.5.5  --gateway 192.168.56.2 \</span></span>
<span class="line"><span style="color:#a6accd">  --subnet-range 192.168.56.0/24 provider-subnet</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li>查看子网</li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# openstack subnet list</span></span>
<span class="line"><span style="color:#a6accd">+--------------------------------------+----------+--------------------------------------+-----------------+</span></span>
<span class="line"><span style="color:#a6accd">| ID                                   | Name     | Network                              | Subnet          |</span></span>
<span class="line"><span style="color:#a6accd">+--------------------------------------+----------+--------------------------------------+-----------------+</span></span>
<span class="line"><span style="color:#a6accd">| 297530f8-62b6-4954-ad33-c8a4ba922a3e | provider | 6cf562b4-0537-4232-a2b9-ce5481e6a505 | 192.168.56.0/24 |</span></span>
<span class="line"><span style="color:#a6accd">+--------------------------------------+----------+--------------------------------------+-----------------+</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h3 id="创建云主机" tabindex="-1">创建云主机 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#创建云主机" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><ol><li>创建云主机类型</li></ol><p>由于默认的云主机大小内存最小是512M，考虑到多数人自己PC的实验环境内存有限，我们创建一个比较小的云主机类型。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# openstack flavor create --id 0 --vcpus 1 --ram 64 --disk 1 m1.nano</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li>创建密钥对</li></ol><p>密钥对用来使用登录创建后的虚拟机，OpenStack在创建虚拟机的时候会把公钥放到虚拟机里面，这样我们就可以不使用密码连接。 注意现在切换到demo用户，因为我们要使用demo用户来创建第一台云主机</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# source demo-openstack.sh</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# ssh-keygen -q -N ""</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# openstack keypair create --public-key ~/.ssh/id_rsa.pub mykey</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# openstack keypair list</span></span>
<span class="line"><span style="color:#a6accd">+-------+-------------------------------------------------+</span></span>
<span class="line"><span style="color:#a6accd">| Name  | Fingerprint                                     |</span></span>
<span class="line"><span style="color:#a6accd">+-------+-------------------------------------------------+</span></span>
<span class="line"><span style="color:#a6accd">| mykey | d2:e0:bc:3e:35:58:5f:7b:fd:c1:0a:93:5e:bf:8f:2d |</span></span>
<span class="line"><span style="color:#a6accd">+-------+-------------------------------------------------+</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li>添加安全组规则</li></ol><p>默认情况下，有一个default安全组，这个安全组会拒绝所有访问，所以为了创建虚拟机能够连接，我们需要设置运行ICMP和22端口访问。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# openstack security group rule create --proto icmp default</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# openstack security group rule create --proto tcp --dst-port 22 default</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li>启动实例前准备</li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# source demo-openstack.sh</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ul><li>查看可用的云主机类型</li></ul><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# openstack flavor list</span></span>
<span class="line"><span style="color:#a6accd">+----+---------+-----+------+-----------+-------+-----------+</span></span>
<span class="line"><span style="color:#a6accd">| ID | Name    | RAM | Disk | Ephemeral | VCPUs | Is Public |</span></span>
<span class="line"><span style="color:#a6accd">+----+---------+-----+------+-----------+-------+-----------+</span></span>
<span class="line"><span style="color:#a6accd">| 0  | m1.nano |  64 |    1 |         0 |     1 | True      |</span></span>
<span class="line"><span style="color:#a6accd">+----+---------+-----+------+-----------+-------+-----------+</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ul><li>查看可用的镜像</li></ul><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# openstack image list</span></span>
<span class="line"><span style="color:#a6accd">+--------------------------------------+--------+--------+--------+</span></span>
<span class="line"><span style="color:#a6accd">| ID                                   | Name   | Status | Server |</span></span>
<span class="line"><span style="color:#a6accd">+--------------------------------------+--------+--------+--------+</span></span>
<span class="line"><span style="color:#a6accd">| 68615b2c-d10e-422f-9acb-c724808657ab | cirros | ACTIVE |        |</span></span>
<span class="line"><span style="color:#a6accd">+--------------------------------------+--------+--------+--------+</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ul><li>查看可用的网络</li></ul><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# openstack network list</span></span>
<span class="line"><span style="color:#a6accd">+--------------------------------------+--------+--------------------------------------+</span></span>
<span class="line"><span style="color:#a6accd">| ID                                   | Name   | Subnets                              |</span></span>
<span class="line"><span style="color:#a6accd">+--------------------------------------+--------+--------------------------------------+</span></span>
<span class="line"><span style="color:#a6accd">| 557a00fa-f61d-4244-b350-29ffe0a03125 | public | c003b4b5-23d1-4211-ad2d-a62ec07d8853 |</span></span>
<span class="line"><span style="color:#a6accd">+--------------------------------------+--------+--------------------------------------+</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ul><li>查看可用的安全组</li></ul><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# openstack security group list</span></span>
<span class="line"><span style="color:#a6accd">+--------------------------------------+---------+------------------------+----------------------------------+</span></span>
<span class="line"><span style="color:#a6accd">| ID                                   | Name    | Description            | Project                          |</span></span>
<span class="line"><span style="color:#a6accd">+--------------------------------------+---------+------------------------+----------------------------------+</span></span>
<span class="line"><span style="color:#a6accd">| 74d74309-0660-4eb9-abba-e89fb6fd23ae | default | Default security group | aec911c3ae68464ba989213e5f6060b1 |</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li>创建虚拟机</li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# openstack server create --flavor m1.nano --image cirros \</span></span>
<span class="line"><span style="color:#a6accd">--nic net-id=5c4d0706-24cd-4d42-ba78-36a05b6c81c8 --security-group default \</span></span>
<span class="line"><span style="color:#a6accd">--key-name mykey demo-instance</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h1 id="注意指定网络的时候需要使用id，而不是名称。" tabindex="-1">注意指定网络的时候需要使用ID，而不是名称。 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#注意指定网络的时候需要使用id，而不是名称。" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h1><h3 id="测试云主机" tabindex="-1">测试云主机 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#测试云主机" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><ol><li>查看云主机状态</li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# openstack server list</span></span>
<span class="line"><span style="color:#a6accd">+--------------------------------------+-----------------+--------+------------+-------------+-----------------------+</span></span>
<span class="line"><span style="color:#a6accd">| ID                                   | Name            | Status | Task State | Power State | Networks              |</span></span>
<span class="line"><span style="color:#a6accd">+--------------------------------------+-----------------+--------+------------+-------------+-----------------------+</span></span>
<span class="line"><span style="color:#a6accd">| 7f94df9a-e547-4f56-9887-bbc52c4e0fff | public-instance | ACTIVE | -          | Running     | public=192.168.56.101 |</span></span>
<span class="line"><span style="color:#a6accd">+--------------------------------------+-----------------+--------+------------+-------------+-----------------------+</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li>测试虚拟机连接</li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# ping -c 2 192.168.56.101</span></span>
<span class="line"><span style="color:#a6accd">PING 192.168.56.101 (192.168.56.101) 56(84) bytes of data.</span></span>
<span class="line"><span style="color:#a6accd">64 bytes from 192.168.56.101: icmp_seq=1 ttl=64 time=5.90 ms</span></span>
<span class="line"><span style="color:#a6accd">64 bytes from 192.168.56.101: icmp_seq=2 ttl=64 time=0.484 ms</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">连接虚拟机</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# ssh cirros@192.168.56.101</span></span>
<span class="line"><span style="color:#a6accd">The authenticity of host '192.168.56.101 (192.168.56.101)' can't be established.</span></span>
<span class="line"><span style="color:#a6accd">RSA key fingerprint is 29:4d:76:17:05:1a:4e:e6:f9:81:a5:1d:8c:27:ff:2c.</span></span>
<span class="line"><span style="color:#a6accd">Are you sure you want to continue connecting (yes/no)? yes</span></span>
<span class="line"><span style="color:#a6accd">Warning: Permanently added '192.168.56.101' (RSA) to the list of known hosts.</span></span>
<span class="line"><span style="color:#a6accd">$ ifconfig eth0</span></span>
<span class="line"><span style="color:#a6accd">eth0      Link encap:Ethernet  HWaddr FA:16:3E:E8:B9:C1  </span></span>
<span class="line"><span style="color:#a6accd">          inet addr:192.168.56.101  Bcast:192.168.56.255  Mask:255.255.255.0</span></span>
<span class="line"><span style="color:#a6accd">          inet6 addr: fe80::f816:3eff:fee8:b9c1/64 Scope:Link</span></span>
<span class="line"><span style="color:#a6accd">          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1</span></span>
<span class="line"><span style="color:#a6accd">          RX packets:150 errors:0 dropped:0 overruns:0 frame:0</span></span>
<span class="line"><span style="color:#a6accd">          TX packets:162 errors:0 dropped:0 overruns:0 carrier:0</span></span>
<span class="line"><span style="color:#a6accd">          collisions:0 txqueuelen:1000 </span></span>
<span class="line"><span style="color:#a6accd">          RX bytes:18634 (18.1 KiB)  TX bytes:17264 (16.8 KiB)</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>如果无法ping通虚拟机，还可以登陆虚拟机Web页面查看，是否启动</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# openstack console url show demo-instance</span></span>
<span class="line"><span style="color:#a6accd">+-------+------------------------------------------------------------------------------------+</span></span>
<span class="line"><span style="color:#a6accd">| Type  | Url                                                                                |</span></span>
<span class="line"><span style="color:#a6accd">+-------+------------------------------------------------------------------------------------+</span></span>
<span class="line"><span style="color:#a6accd">| novnc | http://192.168.56.11:6080/vnc_auto.html?token=e97cffb8-cf0c-4c22-8dbc-fa88ff61f453 |</span></span>
<span class="line"><span style="color:#a6accd">+-------+------------------------------------------------------------------------------------+</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>恭喜！你成功的使用OpenStack创建了第一台虚拟机，可能这一路走来有点坎坷，但是你成功了。不过这只是你在OpenStack世界里完成的第一步！</p><h1 id="openstack-dashboard" tabindex="-1">OpenStack Dashboard <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#openstack-dashboard" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h1><p>目前我们已经讲解并安装了OpenStack的Keystone、Glance、Nova、Neutron。而且我们已经使用命令成功的创建了一台虚拟机，并且可以进行连接和访问。我相信大家已经迫不及待的想看到OpenStack的DashBoard了。 那么OpenStack的有三种管理方法Horizon、CLI和API。本小节，我们就来部署Horizon。 Horizon是OpenStack的Dashboard，是一个可以让云管理员和用户管理OpenStack各种资源和服务的web接口。Horizon通过OpenStack API和各个组件进行交互。 Dashboard会根据Keystone中注册的Service去显示相应的功能。所以如果你没有安装相应的服务，却再Keystone中做了注册，登陆Dashboard是会报错的。</p><h3 id="horizon部署" tabindex="-1">Horizon部署 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#horizon部署" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><blockquote><p>需要将Keystone和Hoarizon分开部署，彼此之间有冲突。部署到linux-node2节点上。</p></blockquote><ol><li>安装Horizon</li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node2 ~]# yum install -y openstack-dashboard</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li>Horizon配置</li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node2 ~]# vim /etc/openstack-dashboard/local_settings</span></span>
<span class="line"><span style="color:#a6accd">#允许所有主机访问</span></span>
<span class="line"><span style="color:#a6accd">ALLOWED_HOSTS = ['*', ]</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">#设置Keystone地址</span></span>
<span class="line"><span style="color:#a6accd">OPENSTACK_HOST = "192.168.56.11"</span></span>
<span class="line"><span style="color:#a6accd">OPENSTACK_KEYSTONE_URL = "http://%s:5000/v3" % OPENSTACK_HOST</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">#设置API版本，需要新增</span></span>
<span class="line"><span style="color:#a6accd">OPENSTACK_API_VERSIONS = {</span></span>
<span class="line"><span style="color:#a6accd">    "identity": 3,</span></span>
<span class="line"><span style="color:#a6accd">    "volume": 2,</span></span>
<span class="line"><span style="color:#a6accd">    "compute": 2,</span></span>
<span class="line"><span style="color:#a6accd">}</span></span>
<span class="line"><span style="color:#a6accd">#为通过仪表盘创建的用户配置默认的 user 角色</span></span>
<span class="line"><span style="color:#a6accd">OPENSTACK_KEYSTONE_DEFAULT_ROLE = "user"</span></span>
<span class="line"><span style="color:#a6accd">开启多域支持</span></span>
<span class="line"><span style="color:#a6accd">OPENSTACK_KEYSTONE_MULTIDOMAIN_SUPPORT = True</span></span>
<span class="line"><span style="color:#a6accd">设置默认的域</span></span>
<span class="line"><span style="color:#a6accd">OPENSTACK_KEYSTONE_DEFAULT_DOMAIN = 'Default'</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">#设置Session存储到Memcached</span></span>
<span class="line"><span style="color:#a6accd">SESSION_ENGINE = 'django.contrib.sessions.backends.cache'</span></span>
<span class="line"><span style="color:#a6accd">CACHES = {</span></span>
<span class="line"><span style="color:#a6accd">    'default': {</span></span>
<span class="line"><span style="color:#a6accd">        'BACKEND': 'django.core.cache.backends.memcached.MemcachedCache',</span></span>
<span class="line"><span style="color:#a6accd">        'LOCATION': '192.168.56.11:11211',</span></span>
<span class="line"><span style="color:#a6accd">    }</span></span>
<span class="line"><span style="color:#a6accd">}</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">#启用Web界面上修改密码</span></span>
<span class="line"><span style="color:#a6accd">OPENSTACK_HYPERVISOR_FEATURES = {</span></span>
<span class="line"><span style="color:#a6accd">    'can_set_mount_point': True,</span></span>
<span class="line"><span style="color:#a6accd">    'can_set_password': True,</span></span>
<span class="line"><span style="color:#a6accd">    'requires_keypair': False,</span></span>
<span class="line"><span style="color:#a6accd">}</span></span>
<span class="line"><span style="color:#a6accd">#设置时区</span></span>
<span class="line"><span style="color:#a6accd">TIME_ZONE = "Asia/Shanghai"</span></span>
<span class="line"><span style="color:#a6accd">#禁用自服务网络的一些高级特性</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">OPENSTACK_NEUTRON_NETWORK = {</span></span>
<span class="line"><span style="color:#a6accd">    ...</span></span>
<span class="line"><span style="color:#a6accd">    'enable_router': False,</span></span>
<span class="line"><span style="color:#a6accd">    'enable_quotas': False,</span></span>
<span class="line"><span style="color:#a6accd">    'enable_distributed_router': False,</span></span>
<span class="line"><span style="color:#a6accd">    'enable_ha_router': False,</span></span>
<span class="line"><span style="color:#a6accd">    'enable_lb': False,</span></span>
<span class="line"><span style="color:#a6accd">    'enable_firewall': False,</span></span>
<span class="line"><span style="color:#a6accd">    'enable_vpn': False,</span></span>
<span class="line"><span style="color:#a6accd">    'enable_fip_topology_check': False,</span></span>
<span class="line"><span style="color:#a6accd">}</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li>重新生产Horizon配置文件</li></ol><p>如果根据官方文档，和默认安装的配置，访问/dashboard后跳转到/auth/login并且会提示404 Not Found，需要重新生成配置文件</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node2 ~]# cd /usr/share/openstack-dashboard</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node2 ~]# python manage.py make_web_conf --apache &gt; /etc/httpd/conf.d/openstack-dashboard.conf</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li>启动服务</li></ol><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node2 ~]# systemctl enable httpd.service</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node2 ~]# systemctl restart httpd.service</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>好的，现在你就可以使用<a target="_blank" rel="noreferrer" href="http://192.168.56.11/%E6%9D%A5%E8%AE%BF%E9%97%AE%E4%BB%AA%E8%A1%A8%E7%9B%98%E4%BA%86%E3%80%82%E7%94%A8%E6%88%B7%E5%90%8D%E5%92%8C%E5%AF%86%E7%A0%81%E5%8F%AF%E4%BB%A5%E4%BD%BF%E7%94%A8admin%E6%88%96%E8%80%85demo%E3%80%82%E9%9C%80%E8%A6%81%E4%BD%A0%E4%BA%B2%E8%87%AA%E6%9D%A5%E4%BD%93%E9%AA%8C%E4%BB%96%E4%BB%AC%E5%88%B0%E5%BA%95%E6%9C%89%E4%BB%80%E4%B9%88%E4%B8%8D%E5%90%8C%E3%80%82"><!--[-->http://192.168.56.11/来访问仪表盘了。用户名和密码可以使用admin或者demo。需要你亲自来体验他们到底有什么不同。<!--]--><!----></a></p><h3 id="horizon的session存储" tabindex="-1">Horizon的Session存储 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#horizon的session存储" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>我们经常在负载均衡的环境下，会遇到Session的问题，一般的解决办法有三种：</p><ul><li>Session保持</li><li>Session复制</li><li>Session共享</li></ul><p>Horizon作为一个典型的Django应用，是使用 Django session架构来处理用户的session数据，目前session支持以下几种存储方式：</p><ul><li>本地内存缓存</li><li>键值对存储（Memcached或者Redis）</li><li>数据库存储</li><li>cookies</li></ul><p>通过修改 local_settings 文件的 SESSION_ENGINE 的配置值来自定制session后端</p><p><strong>本地内存缓存</strong></p><p>本地内存缓存是最快速和最简单的session后端配置方法，因为他不需要处理任何的依赖关系。尽管如此，他也拥有以下明显的缺点：</p><ul><li><p>存储为非共享的，不能跨进程</p></li><li><p>进程终止后没有持续性。 单一的Horizon安装的默认为本地内存后端，因为他没有依赖性问题。生产环境是不推荐使用这个后端的，甚至在严肃的开发工作中也不推荐。你仍旧可以这样启用：</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">SESSION_ENGINE = 'django.contrib.sessions.backends.cache'</span></span>
<span class="line"><span style="color:#a6accd">CACHES = {</span></span>
<span class="line"><span style="color:#a6accd">  'BACKEND': 'django.core.cache.backends.locmem.LocMemCache'</span></span>
<span class="line"><span style="color:#a6accd">}</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div></li></ul><p><strong>键值对存储</strong></p><p>你可以使用Memcached 或 Redis 等应用程序作为session的外部缓存机制。这些应用程序为session提供了持久并可以共享的存储，这在小规模或开发环境中的部署中十分有用。</p><ol><li>Memcached</li></ol><p>Memcached 是一个高性能和分布式的内存对象缓存系统，为小块的任意数据提供进驻内存的键值对的存储。 需求： Memcached服务运行并可以访问。 安装了python-memcached 的 Python 模块 通过下面的方法启用：</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">SESSION_ENGINE = 'django.contrib.sessions.backends.cache'</span></span>
<span class="line"><span style="color:#a6accd">CACHES = {</span></span>
<span class="line"><span style="color:#a6accd">    'BACKEND': 'django.core.cache.backends.memcached.MemcachedCache'</span></span>
<span class="line"><span style="color:#a6accd">    'LOCATION': 'my_memcached_host:11211',</span></span>
<span class="line"><span style="color:#a6accd">}</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ol><li>Redis</li></ol><p>Redis是一个开源的，BSD许可的，高级键值对存储。他通常被称为数据结构服务器。 需求： Redis服务运行并可以访问 安装了 redis 以及 django-redis 的python模块 通过以下配置方法启用：</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">SESSION_ENGINE = 'django.contrib.sessions.backends.cache'</span></span>
<span class="line"><span style="color:#a6accd">CACHES = {</span></span>
<span class="line"><span style="color:#a6accd">    "default": {</span></span>
<span class="line"><span style="color:#a6accd">        "BACKEND": "redis_cache.cache.RedisCache",</span></span>
<span class="line"><span style="color:#a6accd">        "LOCATION": "127.0.0.1:6379:1",</span></span>
<span class="line"><span style="color:#a6accd">        "OPTIONS": {</span></span>
<span class="line"><span style="color:#a6accd">            "CLIENT_CLASS": "redis_cache.client.DefaultClient",</span></span>
<span class="line"><span style="color:#a6accd">        }</span></span>
<span class="line"><span style="color:#a6accd">    }</span></span>
<span class="line"><span style="color:#a6accd">}</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h1 id="第三部分-openstack进阶指南" tabindex="-1">第三部分 OpenStack进阶指南 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#第三部分-openstack进阶指南" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h1><p>联系作者----------------------------------------------------------------------</p><h1 id="第四部分-docker和kubernetes基础" tabindex="-1">第四部分 Docker和Kubernetes基础 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#第四部分-docker和kubernetes基础" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h1><p>1.docker基础入门</p><p>在全链路自动化运维体系中，我们已经可以完成自动化系统安装、自动化监控、配置管理、代码部署。通过SaltStack的配置管理可以保证开发、测试、生产的环境一致性；自动化代码部署可以实现一键部署、秒级回滚；但是，你有没有考虑过，SaltStack配置的环境是很难回滚的，就像你安装了一个软件包更新了其它的软件包。想回滚到你安装软件之前的状态，这个可能有点难。再或者你的代码和环境需要一起回滚！</p><p><img src="http://k8s.unixhot.com/docker/media/6b19e69aac9d176124a719ad038e3bb2.png" alt="img"></p><p>那么让我们马上进入容器的世界，虽然容器不仅仅是解决这些问题，但是至少我们遇到了运维痛点？引入一个新的解决方案一定是为了解决痛点。作为运维，我们不能局限于眼前的苟且，一定要高瞻远瞩，及时的跟上潮流。但是对于生产环境，我们必须要慎重的对待。</p><h2 id="docker介绍" tabindex="-1">Docker介绍 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#docker介绍" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h2><p>Docker是Docker.Inc公司开源的一个基于LXC技术之上构建的Container容器引擎，源代码托管在GitHub上,基于Go语言并遵从Apache2.0协议开源。</p><p>Docker是通过内核虚拟化技术（namespaces及cgroups等）来提供容器的资源隔离与安全保障等。由于Docker在操作系统层实现隔离，所以Docker容器在运行时，不需要类似虚拟机（VM）额外的操作系统开销，提高资源利用率。</p><p><strong>Docker运行结构</strong></p><p><img src="http://k8s.unixhot.com/docker/media/3185f40a56d6a22b714b444d515be3f0.png" alt="说明: Docker Engine Components Flow"></p><p>Docker是一个C/S结构的项目，有Docker Client、RESTAPI、Docker Server组成。</p><ul><li>Docker Client：Docker客户端命令工具。</li><li>REST API：提供标准的RESTful接口。</li><li>Docker Server:：Docker daemon的主要组成部分，接收用户从Docker Client调用REST API发送过来的请求。</li></ul><p><strong>Docker 包括三个基本概念：</strong></p><ul><li>镜像（Image）</li><li>容器（Container）</li><li>仓库（Repository）</li></ul><p><strong>Docker镜像</strong></p><p>Docker 镜像就是一个只读的模板。你可以拿KVM虚拟机镜像来对比理解。</p><p>例如：一个镜像可以包含一个完整的CentOS操作系统运行（注意不包含内核），里面仅安装了Nginx或用户需要的其它应用程序。</p><p>镜像可以用来创建Docker容器。就像你使用创建好的镜像直接启动一个虚拟机一样。</p><p>Docker 提供了一个很简单的机制来创建镜像或者更新现有的镜像，用户甚至可以直接从其他人那里下载一个已经做好的镜像来直接使用。</p><p><strong>小提示：为什么在CentOS的宿主机上可以运行一个Ubuntu的Docker容器？因为Docker镜像里面只是包含了容器运行时的文件、库文件等，并不包含Kernel，统一使用宿主机的Linux内核。</strong></p><p><strong>Docker容器</strong></p><p>Docker利用容器来运行应用，就像你运行一个KVM虚拟机一样。容器就是从镜像创建的运行实例，就像我们上面提到了虚拟机实例一样。它可以被启动、开始、停止、删除。每个容器都是相互独立的、隔离的个体。但其实没有虚拟机隔离的那么彻底，这个我们后面再说。</p><p>你可以把容器看做是一个简易版的Linux 环境（包括root用户权限、进程空间、用户空间和网络空间等）和运行在其中的应用程序。</p><p><strong>Docker仓库</strong></p><p>Docker仓库是集中存放镜像文件的场所。就像OpenStack的Glance服务，仓库分为公开仓库（Public）和私有仓库（Private）两种形式。这个有点像Git代码托管，你可以把代码放在GitHub上并公开，也可是使用Gitlab创建一个私有的代码托管平台。</p><p>Docker最大的公开仓库是Docker Hub，存放了数量庞大的镜像供用户下载，后面你也可以创建自己的镜像分享到Docker Hub中。</p><p>同时你可以创建自己的镜像托管平台，Docker有一个叫做Registry的组件，使用Docker Registry可以启动一个私有的镜像托管平台，这个平台可以存放多个仓库，每个仓库中又包含了多个镜像，每个镜像有不同的标签（tag）。</p><p>有时候我们会把仓库和仓库注册服务器（Registry）混为一谈，并不严格区分，实际上不严谨的，不过也不用太纠结这些名词，可以根据上下文来理解。</p><p>当用户创建了自己的镜像之后就可以使用docker push命令将它上传到公有或者私有仓库，这样下次在另外一台机器上使用这个镜像时候，只需要从仓库上docker pull下来就可以了。是的，没错，就像你操作Git那样。</p><h3 id="docker与虚拟化" tabindex="-1">Docker与虚拟化 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#docker与虚拟化" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>Docker是一种容器技术，它和虚拟化是有区别，就像下图所示：</p><p><img src="http://k8s.unixhot.com/docker/media/50b4ac39d02c080a7e3284e9b078e0c2.jpg" alt="0731013"></p><p>可以看到Docker的容器并不需要运行一个Hypervisor。直接是Docker Engine来管理容器，减少了Hypervisor的开销，但是由于Docker容器并不是一个虚拟机，所以也无法提供像虚拟机一样的完全的资源隔离，还有很多隔离层面的问题。所以如果你的业务要求完全的资源隔离，可能Docker并不是一个好的选择。例如我们之前支付业务，需要在物理机上插入U Key，在虚拟机的情况下可以通过USB重定向来实现，但是Docker却并不支持。</p><p>所有的技术都有它适用的场景，只有深入了解Docker，我们才能发挥它的作用，提高工作效率。</p><h3 id="docker与openstack对比" tabindex="-1">Docker与OpenStack对比 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#docker与openstack对比" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>在Docker还没有普及之前，云计算、OpenStack等流行的技术铺面而来。严格来说Docker不应该和OpenStack做对比，他们没有在一个层级上，和KVM比会更好一些。</p><table><thead><tr><th>类别</th><th>Docker</th><th>OpenStack/KVM</th></tr></thead><tbody><tr><td>部署难度</td><td>非常简单</td><td>组件多，部署复杂</td></tr><tr><td>启动速度</td><td>秒级</td><td>分钟级</td></tr><tr><td>执行性能</td><td>和物理系统几乎一致</td><td>VM会占用一些资源</td></tr><tr><td>镜像体积</td><td>镜像是MB级别</td><td>虚拟机镜像GB级别</td></tr><tr><td>管理效率</td><td>管理简单</td><td>组件相互依赖，管理复杂</td></tr><tr><td>隔离性</td><td>隔离性高</td><td>彻底隔离</td></tr><tr><td>可管理性</td><td>单进程、不建议启动SSH</td><td>完整的系统管理</td></tr><tr><td>网络连接</td><td>比较弱</td><td>借助Neutron可以灵活组件各类网络架构</td></tr></tbody></table><h3 id="docker改变了什么？" tabindex="-1">Docker改变了什么？ <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#docker改变了什么？" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>我觉得Docker在不同的层面都改变了我的工作方式：</p><p><strong>面向产品</strong></p><p>增加了产品交付模式。在产品交付方面，之前做产品交付我们都是交付的软件包、安装操作文档等，需要用户进行部署。而现在可以直接交付Docker镜像，直接启动就可以访问。例如现在很多开源的项目都提供了Docker镜像或者Dockerfile，对于只想尝试一下的人员来说，再也不用进行繁琐的安装步骤了，而且往往部署成功后，发现并不能满足我们的要求。</p><p><strong>面向开发</strong></p><p>简化了开发环境配置。公司每次有开发入职，当天除了办手续就是准备开发环境了。如果使用Docker，我们可以给不同项目，构建不同的Docker镜像。开发入职后，就可以快速的进入开发工作。而且这个镜像一次构建，可以在开发、测试、预生产、生产都可以使用。</p><p><strong>面向测试</strong></p><p>Docker实现了多版本测试。之前我公司测试团队，如果想测试一个项目的不同分支，由于测试环境的局限性大家要排队，串行的进行测试。测试人员A再使用某个项目的测试环境时，其它人要排队。现在每个测试人员都可以创建不同分支的Docker容器。让测试团队可以并行测试。</p><p><strong>面向运维</strong></p><p>解决环境一致性、实现运行环境回滚。我们也不需要使用SaltStack进行配置管理了。做好一个Docker镜像之后，开发、测试、生产都用一套运行环境，只是上面的代码不同。当然这个虚拟机也可以实现，但是不够敏捷。而且在生产代码部署的时候，以Docker容器为单位进行发布。如果要进行回滚，直接把上一个版本的Docker容器启动即可。</p><p><strong>面向架构</strong></p><p>SOA服务部署、微服务部署、自动化扩容。之前我们做SOA、微服务的时候最多以虚拟机为单位进行管理。但是现在启动一个Docker容器要远远快速启动一个KVM虚拟机。而且不需要太复杂的操作步骤，你就可以在一个新的服务器上安装上Docker，并下载镜像，启动容器。</p><h2 id="docker部署" tabindex="-1">Docker部署 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#docker部署" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h2><p>目前Docker仅支持Linux和Unix环境，在Windows Server 2016版本支持Windows容器。</p><h3 id="docker-for-centos：" tabindex="-1">Docker for CentOS： <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#docker-for-centos：" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p><strong>第一步：使用官方yum仓库</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# yum install -y yum-utils</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>使用国内Yum源</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# cd /etc/yum.repos.d/</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 yum.repos.d]# wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>第二步：Docker安装：</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# yum install -y docker-ce</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>第三步：启动后台进程：</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# systemctl start docker</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 yum.repos.d]# docker --version</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">Docker version 18.09.2, build 6247962</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h3 id="docker-for-windows" tabindex="-1">Docker for windows: <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#docker-for-windows" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>第一步：下载boot2docker：<a target="_blank" rel="noreferrer" href="https://github.com/boot2docker/windows-installer/releases/latest"><!--[-->https://github.com/boot2docker/windows-installer/releases/latest<!--]--><!----></a></p><p>第二步：运行安装程序，</p><p><img src="http://k8s.unixhot.com/docker/media/ee82a72fe806ac1ae99ab7fa0aac210c.png" alt="img"></p><p>选择需要安装的组件，如果初次安装请选择Full installation。</p><p><img src="http://k8s.unixhot.com/docker/media/920a1b8c46b133f431f0255197ddb3f8.png" alt="img"></p><p><img src="http://k8s.unixhot.com/docker/media/bbe62c1244b68a92383658af3a72b3a0.png" alt="img"></p><p>安装完毕，打开Docker，默认会进行初始化，初始化会创建一个virtualbox的虚拟机。</p><h3 id="docker-hub" tabindex="-1">Docker Hub <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#docker-hub" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>Docker之所以能这么快的火起来，和Docker Hub的作用是分不开的。Docker构建了像GitHub一样的仓库，用来存放大家构建好的Docker镜像，其中已经包括了超过15,000的镜像。大部分需求，都可以通过在 Docker Hub 中直接下载镜像来实现。</p><p><strong>搜索镜像</strong></p><p>搜索，你懂的，想寻找你需要的Docker镜像，先搜索吧，因为可能已经有人做好了，合并重复造轮子呢，尤其是很多是官方提供的，可以非常放心的使用。</p><p><img src="http://k8s.unixhot.com/docker/media/bd38bd717fa1a2f84aa5f62a965a7ecd.png" alt="img"></p><p>输出依次为镜像名字、描述、星级（表示该镜像的受欢迎程度）、是否官方创建、是否自动创建。OFFICIAL为OK的就是官方镜像。</p><p>本章快速的带领读者使用上Docker，了解Docker的基本操作和管理工作。记住，唯有实践才会掌握。</p><h2 id="docker镜像管理" tabindex="-1">Docker镜像管理 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#docker镜像管理" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h2><p>镜像是Docker容器的基础，想运行一个Docker容器就需要有镜像。我们上面已经学会了使用search搜索镜像。那么这个镜像是怎么创建的呢？</p><h3 id="创建镜像" tabindex="-1">创建镜像 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#创建镜像" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>镜像的创建有以下几种方法：</p><p>**使用ISO镜像：**回顾下OpenStack，我们在创建虚拟机的时候，首先使用iso安装定制了镜像上传到Glance上面，对于Docker来说，我们同样可以使用ISO镜像来制作基本镜像。</p><p>**使用工具制作镜像：**不同的操作系统都提供了相应的工具来让用户创建Docker镜像，比如Centos可以使用febootstrap。debian/ubuntu使用debootstrap。</p><p>**基于本地模板导入：**Docker支持从一个操作系统模板文件导入一个镜像。</p><p>一般我们都是使用一些公共的基础镜像，然后在这个基础上，再根据自身需求进行定制。比如后面要讲到的Docker File的方式，来创建镜像。</p><h3 id="获取镜像" tabindex="-1">获取镜像 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#获取镜像" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>既然有了Docker Hub，我们就不要重复造轮子了，直接下载一个官方提供的centos镜像吧。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# docker pull centos</span></span>
<span class="line"><span style="color:#a6accd">Using default tag: latest</span></span>
<span class="line"><span style="color:#a6accd">latest: Pulling from library/centos</span></span>
<span class="line"><span style="color:#a6accd">a02a4930cb5d: Pull complete</span></span>
<span class="line"><span style="color:#a6accd">Digest: sha256:184e5f35598e333bfa7de10d8fb1cebb5ee4df5bc0f970bf2b1e7c7345136426</span></span>
<span class="line"><span style="color:#a6accd">Status: Downloaded newer image for centos:latest</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>Docker镜像文件是有若干层组成的，行首的名称就是各层的ID。这就是Docker镜像的一个特点。其实分层设计是很多设计的最佳实践之一。由于网络原因，国内用户可能需要等待很久。如果下载中断可以再次执行docker pull centos。</p><p>镜像可以有不同的标签，例如v1.0、v1.1这样，默认情况下，如果我们不再centos后面增加任何的标签会下载标签为latest的镜像，也就是最新版本，而且默认是从docker hub上进行下载。</p><h3 id="查看镜像" tabindex="-1">查看镜像 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#查看镜像" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>使用docker images可以查看当前系统中都有哪些镜像。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# docker images</span></span>
<span class="line"><span style="color:#a6accd">REPOSITORY TAG IMAGE ID CREATED SIZE</span></span>
<span class="line"><span style="color:#a6accd">centos latest 1e1148e4cc2c 2 months ago 202MB</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>使用docker images可以列出本地主机上已经存在的镜像，每个镜像都有一个唯一的镜像ID，我们可以看到有以下几个栏目：</p><ul><li>REPOSITORY：镜像来自于哪个仓库</li><li>TAG：镜像的标签。用来标记镜像的不同版本等。</li><li>IMAGE ID：镜像的唯一ID。</li><li>CREATED：镜像创建时间。</li><li>VIRTUAL SIZE：镜像的大小。</li></ul><h3 id="镜像的导入导出" tabindex="-1">镜像的导入导出 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#镜像的导入导出" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>如果你想把自己的镜像传给别人怎么办，Docker提供了导入和导出的命令：</p><p><strong>导出镜像</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# docker save -o centos.tar centos</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>导入镜像</strong></p><p>将导出的镜像scp到另外的一台机器上后做导入。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node2 ~]# docker load --input centos.tar</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>或者使用重定向进行导入</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node2 ~]# docker load &lt; centos.tar</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>Docker镜像这种导入和导出是完整，可以看到镜像ID等都没有发生改变。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node2 ~]# docker images</span></span>
<span class="line"><span style="color:#a6accd">REPOSITORY TAG IMAGE ID CREATED SIZE</span></span>
<span class="line"><span style="color:#a6accd">centos latest 1e1148e4cc2c 2 months ago 202MB</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h3 id="删除镜像" tabindex="-1">删除镜像 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#删除镜像" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>使用docker rmi命令可以删除本地的镜像，可以使用镜像的标签或者镜像的ID。他们两者之间是不同的。</p><p>docker rmi 标签：会删除指定标签的镜像，但是镜像本身不会被删除，因为一个镜像对象可以有多个标签，就像Linux文件的硬链接，当镜像只剩下一个标签的时候，使用标签删除镜像，才会删除镜像本身。</p><p>docker rmi ID：使用镜像的ID删除镜像，会先删除该镜像的所有标签，然后再删除镜像文件本身。</p><p>注意：如果镜像创建的容器存在时，镜像是无法被删除的。不过你可以使用-f的参数强制删除镜像，并不建议这么做。</p><p>看起来Docker的镜像操作是比虚拟机要便捷的多，目前来说使用几个简单的命令，就可以完成镜像的管理工作，不过这还只是基础。</p><h2 id="docker容器管理" tabindex="-1">Docker容器管理 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#docker容器管理" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h2><p>Docker容器相对于OpenStack的云主机实例，虽然他们本质上不同。我们需要基于镜像来创建容器。容器是独立运行的一个或一组应用，以及它们的运行环境。对应的，虚拟机可以理解为模拟运行的一整套操作系统和跑在上面的应用。</p><h3 id="启动容器" tabindex="-1">启动容器 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#启动容器" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>启动容器其实就是创建镜像并启动，启动镜像有两种方式，一种是将已经存在，但是是stopped状态的镜像启动，一种就是基于一个镜像新建一个新的容器并启动。</p><p><strong>新建并启动容器</strong></p><p>让我们先老生常谈，输出一个Hello World吧。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# docker run centos /bin/echo 'Hello world'</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">Hello world</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>很神奇，可以在精通各种语言的Hello World计数器+1了，这个和你在本地系统运行/bin/echo ‘Hello world’ 几乎没有任何区别，但是它是Docker容器输出的，而且输出后，它就完成使命，自动退出了。</p><blockquote><p>注意：这里就是我们学习Docker要面临的第一个疑惑，就是容器只会在前台运行一个任务，任务结束，容器就终止了。</p></blockquote><p>使用docker ps –a可以查看当前启动的容器：</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# docker ps -a</span></span>
<span class="line"><span style="color:#a6accd">CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES</span></span>
<span class="line"><span style="color:#a6accd">789de67bb454 centos "/bin/echo 'Hello wor" 30 seconds ago Exited (0) 29 seconds</span></span>
<span class="line"><span style="color:#a6accd">ago pedantic_kare</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>刚接触Docker到这里还会有第二个疑惑：</p><p>Docker自动帮你生成了一个名字，比如本例中是pedantic_kare。</p><p>很困惑？好吧，让我们来启动一个我们自定义名称，同时可以有终端的容器，就像启动一个虚拟机一样，不过只是像而已，它们本质上完全不同。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# docker run --name mydocker -t -i centos /bin/bash</span></span>
<span class="line"><span style="color:#a6accd">[root@1b0cae722fa0 /]#</span></span>
<span class="line"><span style="color:#a6accd">[root@1b0cae722fa0 /]# ls /</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">anaconda-post.log bin dev etc home lib lib64 lost+found media mnt opt proc root</span></span>
<span class="line"><span style="color:#a6accd">run sbin srv sys tmp usr var</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>上面我们使用了两个选项，-t 选项让Docker分配一个伪终端并绑定到容器的标准输入上， -i 则让容器的标准输入保持打开。</p><p>在交互模式下，用户可以通过所创建的终端来输入命令，例如我们可以查看当前运行的进程，只有/bin/bash和你运行的命令，注意看/bin/bash的PID为1，有意思。先记着这个特殊的地方，随着我们深入学习，再回过头来研究它。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@1b0cae722fa0 /]# ps aux</span></span>
<span class="line"><span style="color:#a6accd">USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND</span></span>
<span class="line"><span style="color:#a6accd">root 1 0.1 0.1 11776 1872 ? Ss 15:30 0:00 /bin/bash</span></span>
<span class="line"><span style="color:#a6accd">root 18 0.0 0.0 47424 1660 ? R+ 15:31 0:00 ps aux</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>当利用 docker run 来创建容器时，Docker 在后台运行的标准操作包括：</p><ul><li>检查本地是否存在指定的镜像，不存在就从公共仓库下载；</li><li>利用镜像创建并启动一个容器；</li><li>分配一个文件系统，并在只读的镜像层外面挂载一层可读写层；</li><li>从宿主主机配置的网桥接口中桥接一个虚拟接口到容器中去；</li><li>从地址池配置一个ip地址给容器；</li><li>执行用户指定的应用程序；</li><li>执行完毕后容器被终止。</li></ul><p>让我们输入exit来退出这个容器，退出后，容器会自动终止运行。为什么呢？请参考我们第一个疑惑，Docker容器在前台运行一个单任务，任务结束，容器就终止。这就是Docker容器的特性！同时你有没有注意到一个小细节，默认docker容器的主机名就是CONTAINER ID。</p><p><strong>让容器容器后台运行</strong></p><p>在使用docker run运行容器更多时候，我们是需要容器在后台运行的，也就是以守护态形式运行。可以通过-d参数来实现。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# docker run -d --name mydocker2 centos /bin/bash</span></span>
<span class="line"><span style="color:#a6accd">38e42accfa2226bb6c7da2e28e12dc95f6b6d6717326442131887a24bb321cdd</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>容器启动后就会在后台运行，然后返回一个容器ID到控制台，而且上面这个容器也终止了，带着问题继续前进。</p><h3 id="终止容器" tabindex="-1">终止容器 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#终止容器" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>在前面的输出Hello World的操作中我们看到了，当Docker容器中指定的应用程序运行完毕，容器也就自动终止了。同时我们可以使用exit命令退出运行/bin/bash的终端，同时我们也可以使用Ctrl+d来实现同样的效果。</p><p>使用docker stop来停止一个容器，默认是先给容器发送SIGTERM信号，然后10秒后发生SIGKILL信号终止容器，可以使用-t或者—time来设置等待的时间，单位是秒</p><p>docker stop 容器名称、容器ID</p><p>对于终止的容器，可以使用docker start来启动，或者使用docker restart来重启。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@bc419cd0b8fa /]# exit</span></span>
<span class="line"><span style="color:#a6accd">Exit</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>可以使用docker ps –a来查看容器的状态，发现已经是停止模式。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# docker ps -a</span></span>
<span class="line"><span style="color:#a6accd">CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES</span></span>
<span class="line"><span style="color:#a6accd">38e42accfa22 centos "/bin/bash" 17 seconds ago Exited (0) 16 seconds ago</span></span>
<span class="line"><span style="color:#a6accd">mydocker2</span></span>
<span class="line"><span style="color:#a6accd">a5ef57e8783f centos "/bin/bash" 3 minutes ago Exited (0) About a minute ago</span></span>
<span class="line"><span style="color:#a6accd">mydocker</span></span>
<span class="line"><span style="color:#a6accd">d132d6e645ad centos "/bin/echo 'Hello wo…" 3 minutes ago Exited (0) 3 minutes</span></span>
<span class="line"><span style="color:#a6accd">ago serene_dijkstra</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>启动已终止容器</strong></p><p>可以使用docker start来开启已经终止的容器，可以通过输入容器的CONTAINER ID，或者NAMES来进行启动。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# docker start mydocker</span></span>
<span class="line"><span style="color:#a6accd">mydocker</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>好的，我们又一次启动了运行/bin/bash的容器，那么问题来了，我们怎么进去呢。</p><h3 id="进入容器" tabindex="-1">进入容器 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#进入容器" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p><strong>Docker attach</strong></p><p>Docker提供了docker attach的命令，用来让我们进入已经启动的容器（如果容器已经终止，你需要使用docker start将它启动。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# docker attach mydocker</span></span>
<span class="line"><span style="color:#a6accd">[root@a5ef57e8783f /]#</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>你可以继续执行一些命令，没错，很多命令都没有。</p><p>注意，在我们使用 attach 进入容器的时候，如果同时有多个窗口 attach 到同一个容器的时候，所有窗口都会同步显示。当某个窗口因命令阻塞时,其他窗口也无法执行操作了。这可怎么办，如果我们是团队作战，可能多个同事需要同时进入容器操作呢?还有一个最关键的问题。你输入exit以后呢？之前运行的容器退出了。</p><p><strong>nsenter进入容器</strong></p><p>nsenter命令被包含在util-linux软件包里面，使用nsenter可以访问另一个进程的名字空间,大多数Linux发行版默认包含了该软件，CentOS默认是有的。如果你的系统里面没有可以使用以下命令进行安装：</p><blockquote><p><strong>Yum安装：</strong></p></blockquote><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@docker ~]# yum install -y util-linux</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>为了连接到容器，你还需要找到容器的第一个进程的 PID，可以通过下面的命令获取。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd"># docker inspect --format "{{ .State.Pid }}" &lt;container ID or NAMES&gt;</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>通过这个 PID，就可以连接到这个容器：</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd"># nsenter --target $PID --mount --uts --ipc --net –pid</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>如果你刚才停止了容器，请启动。</p><p>连接方式如下：</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# PID=$(docker inspect --format "{{ .State.Pid }}"</span></span>
<span class="line"><span style="color:#a6accd">mydocker)</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# echo $PID</span></span>
<span class="line"><span style="color:#a6accd">8029</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>注意如果你的PID变量为0，说明mydocker容器没有启动。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# nsenter --target $PID --mount --uts --ipc --net --pid</span></span>
<span class="line"><span style="color:#a6accd">[root@a5ef57e8783f /]# ps -ef</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">UID PID PPID C STIME TTY TIME CMD</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">root 1 0 0 21:11 pts/0 00:00:00 /bin/bash</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">root 14 0 0 21:12 pts/0 00:00:00 -bash</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">root 27 14 0 21:12 pts/0 00:00:00 ps -ef</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>编写一个脚本用户进入容器</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# vim docker_in.sh</span></span>
<span class="line"><span style="color:#a6accd">#!/bin/bash</span></span>
<span class="line"><span style="color:#a6accd"># Use nsenter to access docker</span></span>
<span class="line"><span style="color:#a6accd">docker_in(){</span></span>
<span class="line"><span style="color:#a6accd">NAME_ID=$1</span></span>
<span class="line"><span style="color:#a6accd">PID=$(docker inspect --format "{{ .State.Pid }}" $NAME_ID)</span></span>
<span class="line"><span style="color:#a6accd">nsenter --target $PID --mount --uts --ipc --net --pid</span></span>
<span class="line"><span style="color:#a6accd">}</span></span>
<span class="line"><span style="color:#a6accd">docker_in $1</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# chmod +x docker_in.sh</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>这么后面的内容，我们就直接使用docker_in.sh这个脚本来进入Docker容器，只要传给它名称或者容器ID即可，就像下面这样：</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# ./docker_in.sh mydocker</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h3 id="不进入容器执行命令" tabindex="-1">不进入容器执行命令 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#不进入容器执行命令" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>或许你的本意不是想进去容器，而是想让容器执行一个命令，docker提供了exec，使用exec可以在容器内运行命令。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# docker exec mydocker whoami</span></span>
<span class="line"><span style="color:#a6accd">root</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>使用exec进入容器</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# docker exec -it mydocker /bin/bash</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>注意，现在你进入容器和其它方法都是不一样的，其实是你执行了一个/bin/bash的命令，所以你现在拥有了一个shell，你现在所在的shell应该是下图中PID为33的进程。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@1b0cae722fa0 /]# ps -ef</span></span>
<span class="line"><span style="color:#a6accd">UID PID PPID C STIME TTY TIME CMD</span></span>
<span class="line"><span style="color:#a6accd">root 1 0 0 15:37 ? 00:00:00 /bin/bash</span></span>
<span class="line"><span style="color:#a6accd">root 33 0 0 15:41 ? 00:00:00 /bin/bash</span></span>
<span class="line"><span style="color:#a6accd">root 46 33 0 15:41 ? 00:00:00 ps -ef</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>或许你也发现使用docker exec、nsenter进入容器后，执行exit退出容器，容器并不会关闭。但是使用docker attach进入容器，输入exit退出容器后，容器也会自动终止。你可以想想为什么。因为除了attach，nsenter和exec实际中都是开了一个新的shell在执行。而attach是使用容器本身启动的/bin/bash，这个shell环境退出了。那么容器就自动退出了。所以Docker的魔咒来了：docker容器只能而且必须在前台运行一个进程，如果进程退出，容器就关闭。当然如果你想在Docker容器中启动多进程也是有办法的，我们后面会讲到。</p><h3 id="删除容器" tabindex="-1">删除容器 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#删除容器" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>可以使用 docker rm 来删除一个处于终止状态的容器。 例如</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@docker ~]# docker rm mydocker</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>如果要删除一个运行中的容器，可以添加-f参数。Docker会发送 SIGKILL信号给容器。</p><h3 id="学习中的小技巧" tabindex="-1">学习中的小技巧 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#学习中的小技巧" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>如果你在学习和测试的过程中，经常因为启动非常多的容易想删除也很难，下面列举了几个小技巧，可以快速的帮我们进行容器的清理。</p><h4 id="容器停止后就自动删除：" tabindex="-1">容器停止后就自动删除： <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#容器停止后就自动删除：" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h4><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">docker run --rm centos /bin/echo "One"</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h4 id="杀死所有正在运行的容器：" tabindex="-1">杀死所有正在运行的容器： <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#杀死所有正在运行的容器：" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h4><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">docker kill $(docker ps -a -q)</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h4 id="删除所有已经停止的容器：" tabindex="-1">删除所有已经停止的容器： <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#删除所有已经停止的容器：" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h4><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">docker rm $(docker ps -a -q)</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h4 id="删除所有未打-dangling-标签的镜像" tabindex="-1">删除所有未打 dangling 标签的镜像 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#删除所有未打-dangling-标签的镜像" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h4><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">docker rmi $(docker images -q -f dangling=true)</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h4 id="删除所有镜像" tabindex="-1">删除所有镜像 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#删除所有镜像" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h4><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">docker rmi $(docker images -q)</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>如果你觉得名称很长，不容易记，还可以为这些命令创建别名。</p><h4 id="杀死所有正在运行的容器" tabindex="-1">杀死所有正在运行的容器. <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#杀死所有正在运行的容器" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h4><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">alias dockerkill='docker kill $(docker ps -a -q)'</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h4 id="删除所有已经停止的容器" tabindex="-1">删除所有已经停止的容器. <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#删除所有已经停止的容器" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h4><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">alias dockerclean='docker rm $(docker ps -a -q)'</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h4 id="删除所有未打标签的镜像" tabindex="-1">删除所有未打标签的镜像. <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#删除所有未打标签的镜像" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h4><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">alias dockercleani='docker rmi $(docker images -q -f dangling=true)'</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h4 id="删除所有已经停止的容器和未打标签的镜像" tabindex="-1">删除所有已经停止的容器和未打标签的镜像. <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#删除所有已经停止的容器和未打标签的镜像" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h4><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">alias dockerclean='dockercleanc || true &amp;&amp; dockercleani'</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><blockquote><p>注意：生产环境一定要慎用！！！</p></blockquote><h2 id="docker网络访问" tabindex="-1">Docker网络访问 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#docker网络访问" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h2><p>现在我们已经可以熟练的使用docker命令操作镜像和容器，并学会了如何进入到容器中去，那么实际的工作中，我们通常是在Docker中部署服务，我们需要在外部通过IP和端口进行访问的，那么如何访问到Docker的内部服务呢？</p><p>在后面我们会有专门的章节来讲解Docker的网络配置，这里我们先学习一个比较简单的通过网络访问容器的方法，就是端口映射。</p><p>容器中可以运行一些网络应用，要让外部也可以访问这些应用，可以通过 -P 或 -p 参数来指定端口映射。</p><h3 id="随机映射" tabindex="-1">随机映射 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#随机映射" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>当使用 -P 标记时，Docker 会随机映射一个 49000~49900 的端口到内部容器开放的网络端口。</p><p>下面我们以一个nginx的容器为例子来测试一下-P的功能，大家可以使用search搜索下nginx镜像，我们使用官方的nginx镜像来启动一个容器。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# docker search nginx</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>这次我们不执行docker pull。直接来启动容器，你会发现docker会先查找你本地是否有该镜像，如果没有它会自动下载后，然后启动容器。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# docker run -d -P nginx</span></span>
<span class="line"><span style="color:#a6accd">Unable to find image 'nginx:latest' locally</span></span>
<span class="line"><span style="color:#a6accd">latest: Pulling from library/nginx</span></span>
<span class="line"><span style="color:#a6accd">6ae821421a7d: Pull complete</span></span>
<span class="line"><span style="color:#a6accd">da4474e5966c: Pull complete</span></span>
<span class="line"><span style="color:#a6accd">eb2aec2b9c9f: Pull complete</span></span>
<span class="line"><span style="color:#a6accd">Digest: sha256:dd2d0ac3fff2f007d99e033b64854be0941e19a2ad51f174d9240dda20d9f534</span></span>
<span class="line"><span style="color:#a6accd">Status: Downloaded newer image for nginx:latest</span></span>
<span class="line"><span style="color:#a6accd">ecf14adb9dec09555ae31673753093e03941c41d917669dd1f254b51fdec7b51</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>我们又学习了一个新的参数-d可以让容器直接在后台运行</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# docker ps -l</span></span>
<span class="line"><span style="color:#a6accd">CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES</span></span>
<span class="line"><span style="color:#a6accd">6819734a680c nginx "nginx -g 'daemon off" About an hour ago Up About an hour</span></span>
<span class="line"><span style="color:#a6accd">0.0.0.0:32769-&gt;80/tcp, 0.0.0.0:32768-&gt;443/tcp hungry_mayer</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>可以看到，随机映射了一个32769端口到容器的80端口。下面就可以直接访问了。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# curl --head http://192.168.56.11:32769/</span></span>
<span class="line"><span style="color:#a6accd">HTTP/1.1 200 OK</span></span>
<span class="line"><span style="color:#a6accd">Server: nginx/1.11.3</span></span>
<span class="line"><span style="color:#a6accd">Date: Fri, 02 Sep 2016 17:44:14 GMT</span></span>
<span class="line"><span style="color:#a6accd">Content-Type: text/html</span></span>
<span class="line"><span style="color:#a6accd">Content-Length: 612</span></span>
<span class="line"><span style="color:#a6accd">Last-Modified: Tue, 26 Jul 2016 14:54:48 GMT</span></span>
<span class="line"><span style="color:#a6accd">Connection: keep-alive</span></span>
<span class="line"><span style="color:#a6accd">ETag: "579779b8-264"</span></span>
<span class="line"><span style="color:#a6accd">Accept-Ranges: bytes</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>同样的，可以通过 docker logs 命令来查看应用的日志信息。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# docker logs hungry_mayer</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">192.168.56.11 - - [02/Sep/2016:17:44:01 +0000] "HEAD / HTTP/1.1" 200 0 "-"</span></span>
<span class="line"><span style="color:#a6accd">"curl/7.29.0" "-"</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">192.168.56.11 - - [02/Sep/2016:17:44:06 +0000] "GET / HTTP/1.1" 200 612 "-"</span></span>
<span class="line"><span style="color:#a6accd">"curl/7.29.0" "-"</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">192.168.56.11 - - [02/Sep/2016:17:44:14 +0000] "HEAD / HTTP/1.1" 200 0 "-"</span></span>
<span class="line"><span style="color:#a6accd">"curl/7.29.0" "-"</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>是不是老使用随机的名字和ID操作容器很不方面，那么下面我们再创建容器的时候，一定要给它定义一个可读的名字。</p><h3 id="指定端口映射" tabindex="-1">指定端口映射 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#指定端口映射" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>-p（小写的）则可以指定要映射的端口，并且，在一个指定端口上只可以绑定一个容器。支持的格式有以下三种：</p><ul><li>hostPort:containerPort。</li><li>ip:hostPort:containerPort</li><li>ip::containerPort</li></ul><p><strong>映射所有IP地址的指定端口</strong></p><p>使用 hostPort:containerPort，将本地的 80 端口映射到Nginx容器的 80 端口</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# docker run -d -p 80:80 --name nginx-demo1 nginx</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>此时默认会绑定本地所有接口上的所有地址。注意镜像名称需要放到最后。</p><p><strong>映射到指定地址的指定端口</strong></p><p>如果你宿主机拥有多个IP地址，可以使用 ip:hostPort:containerPort 格式指定映射使用一个特定地址，比如将Nginx的80端口绑定到本地IP地址的81端口。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# docker run -d -p 192.168.56.11:81:80 --name nginx-demo2</span></span>
<span class="line"><span style="color:#a6accd">nginx</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>映射到指定地址的任意端口</strong></p><p>使用 ip::containerPort 绑定Nginx80端口到本地192.168.56.11的任意端口，本地主机会自动分配一个端口。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# docker run -d -p 192.168.56.11::80 --name nginx-demo3 nginx</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>使用 udp 标记来指定 udp 端口</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# docker run -d -p 192.168.56.11:53:53/udp –name dns-udp</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>绑定多个端口</strong></p><p>如果你想绑定Docker容器里面的多个端口，可以使用多次使用-p 标记。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# docker run -d -p 80:80 -p 443:443 nginx</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>查看映射端口配置</strong></p><p>使用 docker port 来查看当前映射的端口配置，也可以查看到绑定的地址</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# docker port nginx-demo1</span></span>
<span class="line"><span style="color:#a6accd">80/tcp -&gt; 0.0.0.0:80 </span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h2 id="docker数据管理" tabindex="-1">Docker数据管理 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#docker数据管理" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h2><p>为了能够存储持久化数据以及共享容器间的数据，Docker提出了Volume的概念。让我们通过类似mount的方式将宿主机的文件或者目录挂载到容器中。</p><p>在容器中管理数据主要有两种方式：</p><ul><li>数据卷（Data volumes）</li><li>数据卷容器（Data volume containers）</li></ul><h3 id="数据卷" tabindex="-1">数据卷 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#数据卷" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p><strong>创建一个数据卷</strong></p><p>在使用docker run命令的时候，使用-v选项创建一个数据卷并挂载到容器里。</p><p>下面创建一个测试容器nginx，并加载一个数据卷到容器的/data目录。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# docker run -d --name nginx-volume-test1 -v /data nginx</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>容器启动后，登录到容器会发现/data目录是空的。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# ./docker_in.sh nginx-volume-test1</span></span>
<span class="line"><span style="color:#a6accd">root@00741a2a44ad:/# ls -l /data</span></span>
<span class="line"><span style="color:#a6accd">total 0</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>上面的命令会挂载一个data目录到容器中，并绕过联合文件系统，我们可以在主机上直接操作该目录。任何在该镜像/data 路径的文件会将被复制到Volume。那么在我们宿主机上，这个目录到底在哪里呢。</p><p>我们可以使用 docker inspect 命令找到Volume在主机上的存储位置：</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# docker inspect -f {{.Mounts}} nginx-volume-test1</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">[{861cbee5b650461490633fc60bd94a0df0f8b01e4a4dcc4adff4825dff509159</span></span>
<span class="line"><span style="color:#a6accd">/var/lib/docker/volumes/861cbee5b650461490633fc60bd94a0df0f8b01e4a4dcc4adff4825dff509159/_data</span></span>
<span class="line"><span style="color:#a6accd">/data local true }]</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>你会发现，容器中的/data目录实际上挂载到了/var/lib/docker/volumes/容器ID/_data的目录下。</p><p>我们现在在宿主机该目录下创建一个文件。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 _data]# touch mount-test</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>我们再次在容器中查看，发现已经有了。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# ./docker_in.sh nginx-volume-test1</span></span>
<span class="line"><span style="color:#a6accd">root@00741a2a44ad:/# ls -l /data</span></span>
<span class="line"><span style="color:#a6accd">total 0</span></span>
<span class="line"><span style="color:#a6accd">-rw-r--r-- 1 root root 0 Sep 2 18:58 mount-test</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>指定一个宿主机目录作为数据卷</strong></p><p>有的时候，我们需要将宿主机的某个目录挂载到容器中来进行访问，可以通过-v 源路径：目标目录的方式进行。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# mkdir -p /data/docker-volume-nginx</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# docker run -d --name nginx-volume-test2 \</span></span>
<span class="line"><span style="color:#a6accd">-v /data/docker-volume-nginx:/data nginx</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>你会发现，目前容器中的/data目录和宿主机中的/data/ docker-volume-nginx目录是相同的。</p><p><strong>文件挂载</strong></p><p>数据卷也可以挂载一个宿主机的文件作到容器中作为数据卷。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# docker run -d --name nginx-volume-test3 \</span></span>
<span class="line"><span style="color:#a6accd">-v /$HOME/.bash_history:/root/.bash_history nginx</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>上面案例将宿主机的.bash_history挂载到了容器中，这样就可以记录容器中的命令历史记录。</p><p><strong>只读挂载</strong></p><p>Docker 挂载数据卷的默认权限是读写，用户也可以通过 :ro 指定为只读。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# docker run -d --name nginx-volume-test \</span></span>
<span class="line"><span style="color:#a6accd">-v /data/docker-volume-nginx:/data:ro nginx</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h3 id="数据卷生产实践" tabindex="-1">数据卷生产实践 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#数据卷生产实践" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>那么这种将本地的目录挂载到容器中的方法，在生产中有哪些应用场景呢？</p><p><strong>在开发环境中</strong></p><p>在基于Docker的开发环境中，例如我们将本地的/home/code目录挂载到容器中的/data/webroot，然后容器的应用的webroot同样设置为/data/webroot，那么容器启动后，我们就可以自由的本地进行代码的编写和调试，对于解释性的语言，就比较方便了。如果你的Docker运行在虚拟机中，可以先将本地例如D:code目录通过虚拟机软件共享到虚拟机里，目前VirtualBox和Vmware workstation都支持目录映射。在Vmware中需要在虚拟机中安装Vmware Tools。</p><p><img src="http://k8s.unixhot.com/docker/media/03822b520894cafdde1dbbc64b8b99e6.png" alt="img"></p><p><strong>在生产环境中</strong></p><p>如果我们的容器里面运行的程序需要写一些持久化的数据到硬盘，或者共享宿主机的一些文件或者目录，显然这种数据卷的挂载非常的方便。</p><p>不过数据卷最大的缺陷应该是不灵活了，因为Docker的理念是Build、Ship、Run。因为有这个挂载目录的依赖，那么这个Docker镜像在新的宿主机上运行就需要考虑挂载的目录是否存在等问题。</p><h3 id="数据卷容器" tabindex="-1">数据卷容器 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#数据卷容器" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>Docker还支持让一个容器访问另一个容器的Volume，我们可以使用 -volumes-from 参数挂载其它容器的卷。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# docker run -d --name nginx-volume-test4 \</span></span>
<span class="line"><span style="color:#a6accd">--volumes-from nginx-volume-test1 nginx</span></span>
<span class="line"><span style="color:#a6accd">7d650d332e3d9bf4086be35ae5a04b618f9d8e1df03fa2fef5f5766318a4ca12</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# ./docker_in.sh nginx-volume-test4</span></span>
<span class="line"><span style="color:#a6accd">root@7d650d332e3d:~# ls /data</span></span>
<span class="line"><span style="color:#a6accd">mount-test</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>注意：不管数据卷容器nginx-volume-test1是否运行，nginx-volume-test4都可以访问到nginx-volume-test1里面挂载的数据卷。所以说数据卷容器，其实就是一个普通的容器，只不过是专门用来提供数据卷供其它容器挂载使用的。</p><h3 id="数据卷容器生产实践" tabindex="-1">数据卷容器生产实践 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#数据卷容器生产实践" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>在生产中使用数据卷容器的方案有很多，一个非常典型的就是在进行日志收集的时候。我们可以在一台服务器上启动一个logstash或者filebeat的容器log-volume，然后其它所有应用容器启动都使用--volumes-from log-volume，将日志写入对应路径，然后使用统一收集，这样就避免在所有需要进行日志收集的容器中去部署logstash。相对于直接挂载本地目录也更加的方便。</p><h1 id="docker镜像生产构建" tabindex="-1">Docker镜像生产构建 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#docker镜像生产构建" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h1><p>我们已经了解了Docker的基本概念和管理，下面就开始着手创建自己的第一个Docker镜像，这里我们选用最早pull下来的centos作为基础镜像，然后在上面部署Nginx来做案例讲解。</p><h2 id="手动构建镜像" tabindex="-1">手动构建镜像 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#手动构建镜像" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h2><p>手动构建镜像，简单的说就是我们基于一个基础镜像启动一个容器，然后对这个容器进行更改，更改完毕后，进行提交。</p><h3 id="启动容器-1" tabindex="-1">启动容器 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#启动容器-1" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>运行一个CentOS容器，命名为mynginx</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# docker pull centos</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# docker run --name mynginx -it centos</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>在容器里面安装Nginx</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@2e110e00eef4 /]# rpm -ivh http://mirrors.aliyun.com/epel/epel-release-latest-7.noarch.rpm</span></span>
<span class="line"><span style="color:#a6accd">[root@2e110e00eef4 /]# yum install -y nginx</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>想要Docker容器保持活跃的状态，需要其中运行的进程不能中断，默认情况下，Nginx会以守护进程的方式启动，这会导致容器只是短暂运行，在守护进程被fork启动后，发起守护进程的原始进程就会退出，这时容器就会停止运行了。所以我们需要将Nginx放在前端运行。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@2e110e00eef4 /]# vi /etc/nginx/nginx.conf</span></span>
<span class="line"><span style="color:#a6accd">#在配置文件最上面增加下面配置</span></span>
<span class="line"><span style="color:#a6accd">daemon off;</span></span>
<span class="line"><span style="color:#a6accd">[root@2e110e00eef4 /]# exit</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h3 id="提交镜像" tabindex="-1">提交镜像 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#提交镜像" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p><strong>获取容器ID</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# docker ps -a | grep mynginx</span></span>
<span class="line"><span style="color:#a6accd">2e110e00eef4 centos "/bin/bash" 8 minutes ago Exited (0) 2 minutes ago mynginx</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>提交修改后的容器为镜像</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# docker commit -m "My Nginx" 2e110e00eef4 test/mynginx:v1</span></span>
<span class="line"><span style="color:#a6accd">sha256:cfd25da2c9c5dd2bcce9e5d2ef4e316b46b5f03617176b97b60a34f2958a6d70</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ul><li>-m：指定提交的说明信息，类似SVN和Git。</li><li>之后是用来创建镜像的容器的 ID；</li><li>最后指定目标镜像的仓库名和标签信息。</li></ul><p><strong>查看镜像</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# docker images</span></span>
<span class="line"><span style="color:#a6accd">REPOSITORY TAG IMAGE ID CREATED SIZE</span></span>
<span class="line"><span style="color:#a6accd">test/mynginx v1 cfd25da2c9c5 49 seconds ago 373MB</span></span>
<span class="line"><span style="color:#a6accd">nginx latest f09fe80eb0e7 2 weeks ago 109MB</span></span>
<span class="line"><span style="color:#a6accd">centos latest 1e1148e4cc2c 2 months ago 202MB</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>从我们创建的镜像运行一个容器</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# docker run -d -p 91:80 test/mynginx:v1 nginx</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>现在你的手动构建的第一个镜像就完成了。你可以用同样的方式安装任意你需要的软件到镜像里面，然后使用镜像来启动一个容器，只要记住容器启动必须要有一个不会退出的进程在执行即可。</p><h2 id="dockerfile构建" tabindex="-1">Dockerfile构建 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#dockerfile构建" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h2><p>Dockerfile是为快速构建docker image而设计的，当你使用docker build命令的时候，docker 会读取当前目录下的命名为Dockerfile(首字母大写)的纯文本文件并执行里面的指令构建出一个docker image。这比SaltStack的配置管理要简单的多，不过还是要掌握一些简单的指令。</p><p>Dockerfile 由一行行命令语句组成，并且支持以#开头的注释行。指令是不区分大小写的，但是通常我们都大写。</p><p>下面我们通过构建一个Nginx的镜像来学习Dockerfile的编写。</p><h3 id="nginx-dockerfile实战" tabindex="-1">Nginx Dockerfile实战 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#nginx-dockerfile实战" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p><strong>注意：第一个指令必须是FROM。</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# mkdir -p /opt/dockerfile/mynginx</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# cd /opt/dockerfile/mynginx/</span></span>
<span class="line"><span style="color:#a6accd">[root@test-node1 nginx]# vim Dockerfile</span></span>
<span class="line"><span style="color:#a6accd"># This docker file uses the centos image</span></span>
<span class="line"><span style="color:#a6accd"># VERSION 1</span></span>
<span class="line"><span style="color:#a6accd"># Author: Jason Zhao</span></span>
<span class="line"><span style="color:#a6accd"># Base image</span></span>
<span class="line"><span style="color:#a6accd">FROM centos</span></span>
<span class="line"><span style="color:#a6accd"># Maintainer</span></span>
<span class="line"><span style="color:#a6accd">MAINTAINER shundong.zhao zhaoshundong@gmail.com</span></span>
<span class="line"><span style="color:#a6accd">#Commands to update the image</span></span>
<span class="line"><span style="color:#a6accd">RUN rpm -ivh http://mirrors.aliyun.com/epel/epel-release-latest-7.noarch.rpm</span></span>
<span class="line"><span style="color:#a6accd">RUN yum install -y nginx --enablerepo=epel</span></span>
<span class="line"><span style="color:#a6accd">ADD index.html /usr/share/nginx/html/index.html</span></span>
<span class="line"><span style="color:#a6accd">RUN echo "daemon off;" &gt;&gt; /etc/nginx/nginx.conf</span></span>
<span class="line"><span style="color:#a6accd">EXPOSE 80</span></span>
<span class="line"><span style="color:#a6accd">CMD ["nginx"]</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>为了大家更容易学习，我这里总结一个命令的介绍：</p><p>一般的，我们将Dockerfile 分为四部分：</p><ul><li>基础镜像信息</li><li>维护者信息</li><li>镜像操作指令</li><li>容器启动时执行指令</li></ul><h3 id="常用指令的介绍" tabindex="-1">常用指令的介绍 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#常用指令的介绍" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p><img src="http://k8s.unixhot.com/docker/media/74da151d98647093a4d1ade4f222fbe7.png" alt="img"></p><p><strong>构建Dockerfile</strong></p><p>注意：ADD index.html就是放一个文件进去，这个文件需要大家自己准备一下。例如：</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 mynginx]# echo "nginx in docker test" &gt; index.html</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>使用dokcer build命令构建镜像，最后的.表示当前路径</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 mynginx]# docker build -t mynginx:v2 .</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>构建完毕之后，我们就可以Run起来。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# docker run -d -p 92:80 mynginx:v2 nginx</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>测试访问</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 mynginx]# curl http://192.168.56.11:92</span></span>
<span class="line"><span style="color:#a6accd">nginx in docker test</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>现在你应该发现编写Dockerfile有多么的简单了吧，不过我们还是要系统的再来学习一遍。</p><h3 id="dockerfile命令详解" tabindex="-1">Dockerfile命令详解 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#dockerfile命令详解" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>下面我们来分别介绍下上面使用到的命令：</p><p><strong>FROM</strong></p><p>格式：FROM <img src="" alt="img">或FROM <img src="" alt="img">:。 解释：FROM必须是Dockerfile里的第一条指令（注视除外），后面跟有效的镜像名（如果该镜像你的本地仓库没有则会从远程仓库Pull取）。然后后面的其它指令FROM的镜像中执行。</p><p><strong>MAINTAINER</strong></p><p>格式：MAINTAINER 解释：指定维护者的信息。</p><p><strong>RUN</strong></p><p>格式：RUN 或 RUN ["executable", "param1", "param2"]。 解释：运行命令，命令较长使可以使用来换行。推荐使用上面数组的格式</p><p><strong>CMD</strong></p><p>格式：</p><p>CMD ["executable","param1","param2"] 使用 exec 执行，推荐方式； CMD command param1 param2 在 /bin/sh 中执行，提供给需要交互的应用； CMD ["param1","param2"] 提供给ENTRYPOINT的默认参数；</p><p>解释：</p><p>CMD指定容器启动是执行的命令，每个Dockerfile只能有一条CMD命令，如果指定了多条，只有最后一条会被执行。如果你在启动容器的时候也指定的命令，那么会覆盖Dockerfile构建的镜像里面的CMD命令。</p><p><strong>ENTRYPOINT</strong></p><p>格式：</p><p>ENTRYPOINT ["executable", "param1", "param2"] ENTRYPOINT command param1 param2（shell中执行）。 解释：和CMD类似都是配置容器启动后执行的命令，并且不可被 docker run提供的参数覆盖。</p><p>每个 Dockerfile 中只能有一个ENTRYPOINT，当指定多个时，只有最后一个起效。ENTRYPOINT没有CMD的可替换特性，也就是你启动容器的时候增加运行的命令不会覆盖ENTRYPOINT指定的命令。</p><p>所以生产实践中我们可以同时使用ENTRYPOINT和CMD，例如：</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">ENTRYPOINT ["/usr/bin/rethinkdb"]</span></span>
<span class="line"><span style="color:#a6accd">CMD ["--help"]</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>USER</strong></p><p>格式：USER daemon 解释：指定运行容器时的用户名和UID，后续的RUN指令也会使用这里指定的用户。</p><p><strong>EXPOSE</strong></p><p>格式：EXPOSE [...] 解释：设置Docker容器内部暴露的端口号，如果需要外部访问，还需要启动容器时增加-p或者-P参数进行分配。</p><p><strong>ENV</strong></p><p>格式：ENV ENV = ... 解释：设置环境变量，可以在RUN之前使用，然后RUN命令时调用，容器启动时这些环境变量都会被指定</p><p><strong>ADD</strong></p><p>格式： ADD ... ADD ["",... ""] 解释：将指定的复制到容器文件系统中的</p><p>所有拷贝到container中的文件和文件夹权限为0755,uid和gid为0 如果文件是可识别的压缩格式，则docker会帮忙解压缩</p><p><strong>VOLUME</strong></p><p>格式：VOLUME ["/data"] 解释：可以将本地文件夹或者其他container的文件夹挂载到container中。</p><p><strong>WORKDIR</strong></p><p>格式：WORKDIR /path/to/workdir 解释：切换目录，为后续的 RUN、CMD、ENTRYPOINT 指令配置工作目录。可以多次切换(相当于cd命令)，也可以使用多个 WORKDIR指令，后续命令如果参数是相对路径，则会基于之前命令指定的路径。 例如：</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">WORKDIR /a</span></span>
<span class="line"><span style="color:#a6accd">WORKDIR b</span></span>
<span class="line"><span style="color:#a6accd">WORKDIR c</span></span>
<span class="line"><span style="color:#a6accd">RUN pwd</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>则最终路径为 /a/b/c。</p><p><strong>ONBUILD</strong></p><p>ONBUILD 指定的命令在构建镜像时并不执行，而是在它的子镜像中执行</p><p><strong>ARG</strong></p><p>格式：ARG [=] 解释：ARG指定了一个变量在docker build的时候使用，可以使用--build-arg =来指定参数的值，不过如果构建的时候不指定就会报错。</p><h2 id="docker镜像生产规划实践" tabindex="-1">Docker镜像生产规划实践 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#docker镜像生产规划实践" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h2><p>现在我们学会了如何使用Dockerfile来构建镜像，没错，真正生产环境我们也是大规模的使用Dockerfile。那么我们应该如何入手呢？</p><p>首先我们需要参考一些别人编写的Dockerfile，学习一些规范和技巧，可以来这里找找答案：<a target="_blank" rel="noreferrer" href="https://github.com/dockerfile%E3%80%82"><!--[-->https://github.com/dockerfile。<!--]--><!----></a></p><p>可以参考网友编写的Dockerfile的技巧和方法，那么真正的生产环境，肯定要根据自己公司或者团队的技术栈来构建不同的Docker镜像，根据Docker镜像的分层观念，我们可以在这个基础上对我们的镜像进行分层。</p><ul><li>系统层</li><li>运行环境层</li><li>应用服务层</li></ul><p>案例如下：</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# mkdir /opt/dockerfile</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# cd /opt/dockerfile/</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 dockerfile]# mkdir system runtime app</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 dockerfile]# tree</span></span>
<span class="line"><span style="color:#a6accd">.</span></span>
<span class="line"><span style="color:#a6accd">├── app</span></span>
<span class="line"><span style="color:#a6accd">├── runtime</span></span>
<span class="line"><span style="color:#a6accd">└── system</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h2 id="基础系统镜像" tabindex="-1">基础系统镜像 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#基础系统镜像" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h2><h3 id="centos系统镜像" tabindex="-1">CentOS系统镜像 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#centos系统镜像" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>默认的官方CentOS镜像有很多常用的命令并不提供，可以根据企业需求进行定制。需要注意的是使用yum安装完毕后，记得执行yum clean all。</p><p>因为yum会把下载的软件包和header存储在cache中，而不会自动删除。如果我们觉得它们占用了磁盘空间，可以使用yum clean all指令进行清除，可以减少镜像的大小。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# cd /opt/dockerfile/system/</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 system]# mkdir centos</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>1.编写Dockerfile</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 system]# vim centos/Dockerfile</span></span>
<span class="line"><span style="color:#a6accd"># Docker for CentOS</span></span>
<span class="line"><span style="color:#a6accd">#Base image</span></span>
<span class="line"><span style="color:#a6accd">**FROM** centos</span></span>
<span class="line"><span style="color:#a6accd">#Who</span></span>
<span class="line"><span style="color:#a6accd">**MAINTAINER** Jason.Zhao xxx@gmail.com</span></span>
<span class="line"><span style="color:#a6accd">#Base pkg</span></span>
<span class="line"><span style="color:#a6accd">**RUN** rpm -ivh http://mirrors.aliyun.com/epel/epel-release-latest-7.noarch.rpm</span></span>
<span class="line"><span style="color:#a6accd">**RUN** yum install -y wget mysql-devel supervisor git redis tree net-tools sudo</span></span>
<span class="line"><span style="color:#a6accd">psmisc &amp;&amp; yum clean all</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>2.构建镜像</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 system]# docker build -t unixhot/centos ./centos/</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>3.查看镜像</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 system]# docker images | grep unixhot</span></span>
<span class="line"><span style="color:#a6accd">unixhot/centos latest 4edcb790dacf 24 seconds ago 303 MB</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>4.使用镜像创建容器</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 system]# docker run --name mycentos -it unixhot/centos</span></span>
<span class="line"><span style="color:#a6accd">/bin/bash</span></span>
<span class="line"><span style="color:#a6accd">[root@b137b1cdd3ac /]# ps aux</span></span>
<span class="line"><span style="color:#a6accd">USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND</span></span>
<span class="line"><span style="color:#a6accd">root 1 0.3 0.0 11776 1872 ? Ss 03:56 0:00 /bin/bash</span></span>
<span class="line"><span style="color:#a6accd">root 15 0.0 0.0 47424 1668 ? R+ 03:56 0:00 ps aux</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h3 id="centos系统镜像包含ssh" tabindex="-1">CentOS系统镜像包含SSH <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#centos系统镜像包含ssh" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>在很多时候如果你需要在CentOS里面启动sshd服务，那么就需要安装openssh-server并且重新生成SSH的主机密钥。同时如果，你需要给镜像设置一个密码，可以使用chpasswd非交互的方式来进行。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# cd /opt/dockerfile/system/</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 system]# mkdir centos-ssh</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>1.编写Dockerfile</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 system]# vim centos-ssh/Dockerfile</span></span>
<span class="line"><span style="color:#a6accd"># Docker for CentOS</span></span>
<span class="line"><span style="color:#a6accd">#Base image</span></span>
<span class="line"><span style="color:#a6accd">FROM centos</span></span>
<span class="line"><span style="color:#a6accd">#Who</span></span>
<span class="line"><span style="color:#a6accd">MAINTAINER Jason.Zhao xxx@gmail.com</span></span>
<span class="line"><span style="color:#a6accd">#EPEL</span></span>
<span class="line"><span style="color:#a6accd">RUN rpm -ivh http://mirrors.aliyun.com/epel/epel-release-latest-7.noarch.rpm</span></span>
<span class="line"><span style="color:#a6accd">#Base pkg</span></span>
<span class="line"><span style="color:#a6accd">RUN yum install -y openssh-clients openssl-devel openssh-server wget mysql-devel</span></span>
<span class="line"><span style="color:#a6accd">supervisor git redis tree net-tools sudo psmisc &amp;&amp; yum clean all</span></span>
<span class="line"><span style="color:#a6accd"># For SSHD</span></span>
<span class="line"><span style="color:#a6accd">RUN ssh-keygen -t rsa -f /etc/ssh/ssh_host_rsa_key</span></span>
<span class="line"><span style="color:#a6accd">RUN ssh-keygen -t ecdsa -f /etc/ssh/ssh_host_ecdsa_key</span></span>
<span class="line"><span style="color:#a6accd">RUN ssh-keygen -A -t dsa -f /etc/ssh/ssh_host_dsa_key</span></span>
<span class="line"><span style="color:#a6accd"># Set root password</span></span>
<span class="line"><span style="color:#a6accd">RUN echo "root:unixhot.com" | chpasswd</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>2.构建镜像</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 system]# docker build -t unixhot/centos-ssh:v1 ./centos-ssh/</span></span>
<span class="line"><span style="color:#a6accd">Sending build context to Docker daemon 2.048 kB</span></span>
<span class="line"><span style="color:#a6accd">Step 1 : FROM centos</span></span>
<span class="line"><span style="color:#a6accd">---&gt; 980e0e4c79ec</span></span>
<span class="line"><span style="color:#a6accd">Step 2 : MAINTAINER Jason.Zhao xxx@gmail.com</span></span>
<span class="line"><span style="color:#a6accd">---&gt; **Using cache**</span></span>
<span class="line"><span style="color:#a6accd">---&gt; d08da8648d91</span></span>
<span class="line"><span style="color:#a6accd">Step 3 : RUN rpm -ivh</span></span>
<span class="line"><span style="color:#a6accd">http://mirrors.aliyun.com/epel/epel-release-latest-7.noarch.rpm</span></span>
<span class="line"><span style="color:#a6accd">---&gt; **Using cache**</span></span>
<span class="line"><span style="color:#a6accd">---&gt; ad9a05bcfb78</span></span>
<span class="line"><span style="color:#a6accd">…（省略后面输出）</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><blockquote><p>这里有一个小技巧，为了加快构建的速度，注意到上面前三步的输出Using cache，因为Docker镜像的分层原理，已经构建过的layer不会重复构建。</p></blockquote><p><strong>3.查看镜像</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 system]# docker images | grep unixhot</span></span>
<span class="line"><span style="color:#a6accd">unixhot/centos-ssh v1 ff1ab7d7e7f4 19 seconds ago 304 MB</span></span>
<span class="line"><span style="color:#a6accd">unixhot/centos latest 4edcb790dacf 2 hours ago 303 MB</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>4.使用镜像创建容器</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 system]# docker run -d --name centos-ssh-demo -p 8022:22</span></span>
<span class="line"><span style="color:#a6accd">unixhot/centos-ssh:v1 /usr/sbin/sshd -D</span></span>
<span class="line"><span style="color:#a6accd">5f5bbe98a17c620f91a7d3e68a605a4bcbd48621f32aaa095245e59a8691e229</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 system]# docker ps -l</span></span>
<span class="line"><span style="color:#a6accd">CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES</span></span>
<span class="line"><span style="color:#a6accd">5f5bbe98a17c unixhot/centos-ssh:v1 "/usr/sbin/sshd -D" 5 seconds ago Up 2</span></span>
<span class="line"><span style="color:#a6accd">seconds 0.0.0.0:8022-&gt;22/tcp centos-ssh-demo</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>5.使用ssh连接容器</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# ssh -p 8022 root@192.168.56.11</span></span>
<span class="line"><span style="color:#a6accd">root@192.168.56.11's password:</span></span>
<span class="line"><span style="color:#a6accd">[root@5f5bbe98a17c ~]# ps aux</span></span>
<span class="line"><span style="color:#a6accd">USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND</span></span>
<span class="line"><span style="color:#a6accd">root 1 0.0 0.0 82560 3600 ? Ss 05:46 0:00 /usr/sbin/sshd -D</span></span>
<span class="line"><span style="color:#a6accd">root 7 0.0 0.0 11636 1116 ? Ss 05:47 0:00 /bin/bash</span></span>
<span class="line"><span style="color:#a6accd">root 13 0.0 0.0 11636 1116 ? Ss 05:48 0:00 /bin/bash</span></span>
<span class="line"><span style="color:#a6accd">root 63 0.7 0.0 139272 5424 ? Ss 05:51 0:00 sshd: root@pts/0</span></span>
<span class="line"><span style="color:#a6accd">root 65 0.5 0.0 13376 1988 pts/0 Ss 05:51 0:00 -bash</span></span>
<span class="line"><span style="color:#a6accd">root 78 0.0 0.0 49024 1808 pts/0 R+ 05:51 0:00 ps aux</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>Good Job，你现在可以通过ssh连接到自己创建的Docker容器上了。但是有没有发现一个问题，Docker只允许在前台运行一个进程，那已经运行了sshd。那么如何运行别的进程呢。显然我们做一个能ssh的镜像，目的就是为了在上面运行各种服务。怎么办呢？有很多解决方案。<a target="_blank" rel="noreferrer" href="http://xn--start-fg1hyjn9fbll9onm0app1b175dr5f.sh"><!--[-->例如你编写一个脚本start.sh<!--]--><!----></a>。可以在里面写多个服务的启动命令，只要start.sh不执行完毕，即可。当然真正生产推荐的是使用Supervisor。</p><h3 id="使用supervisor管理进程" tabindex="-1">使用Supervisor管理进程 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#使用supervisor管理进程" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>Supervisor (<a target="_blank" rel="noreferrer" href="http://supervisord.org/"><!--[-->http://supervisord.org<!--]--><!----></a>) 是一个用 Python 写的进程管理工具，可以很方便的用来启动、重启、关闭进程（不仅仅是 Python 进程）。除了对单个进程的控制，还可以同时启动、关闭多个进程。</p><p>Supervisor 可以运行在 Linux、Mac OS X 上。如前所述，supervisor 是 Python 编写的，所以安装起来也很方便，可以直接用 pip :</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">pip install supervisor</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>现在让我们重新创建一个新的不启动ssh的容器，来实验下supervisor的安装和相关配置。以便于，我们编写Dockerfile。</p><p><strong>1.启动测试supervisor的docker容器</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 system]# docker run --rm -it unixhot/centos-ssh:v1 /bin/bash</span></span>
<span class="line"><span style="color:#a6accd">[root@98aced104917 /]# ls -l /etc/supervisord*</span></span>
<span class="line"><span style="color:#a6accd">-rw-r--r-- 1 root root 7953 Aug 20 2015 /etc/supervisord.conf</span></span>
<span class="line"><span style="color:#a6accd">/etc/supervisord.d:</span></span>
<span class="line"><span style="color:#a6accd">total 0</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>注意，在上一小节的镜像构建中，我们已经使用yum安装了supervisord。supervisor的配置文件为/etc/supervisord.conf。</p><p><strong>2.Supervisor配置</strong></p><p>Supervisor 相当强大，提供了很丰富的功能，不过我们可能只需要用到其中一小部分。安装完成之后，可以编写配置文件，来满足自己的需求。为了方便，我们把配置分成两部分：supervisord（supervisor 是一个 C/S 模型的程序，这是 server 端，对应的有 client 端：supervisorctl）和应用程序（即我们要管理的程序）。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@98aced104917 /]# vi /etc/supervisord.conf</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">#下面并没有列出完整的supervisord.conf的配置，默认注释掉的内容，请自行查看。</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">**[unix_http_server]**</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd"># file=/var/run/supervisor/supervisor.sock #UNIX socket 文件，supervisorctl</span></span>
<span class="line"><span style="color:#a6accd">会使用</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">**[supervisord]**</span></span>
<span class="line"><span style="color:#a6accd">logfile=/var/log/supervisor/supervisord.log #主要日志文件，默认位置是</span></span>
<span class="line"><span style="color:#a6accd">$CWD/supervisord.log</span></span>
<span class="line"><span style="color:#a6accd">logfile_maxbytes=50MB #日志文件大小，超出会 rotate，默认 50MB</span></span>
<span class="line"><span style="color:#a6accd">logfile_backups=10 #日志文件保留备份数量默认 10</span></span>
<span class="line"><span style="color:#a6accd">loglevel=info #日志级别，默认 info，其它: debug,warn,trace</span></span>
<span class="line"><span style="color:#a6accd">pidfile=/var/run/supervisord.pid #pid文件位置</span></span>
<span class="line"><span style="color:#a6accd">**nodaemon=true #注意需要修改的为此处，将supervisor放在前台运行。**</span></span>
<span class="line"><span style="color:#a6accd">minfds=1024 #可以打开的文件描述符的最小值，默认 1024</span></span>
<span class="line"><span style="color:#a6accd">minprocs=200 #可以打开的进程数的最小值，默认 200</span></span>
<span class="line"><span style="color:#a6accd">**[supervisorctl]**</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">#通过 UNIX socket 连接 supervisord，路径与 unix_http_server 部分的 file 一致</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">serverurl=unix:///var/run/supervisor/supervisor.sock ; use a unix:// URL for a</span></span>
<span class="line"><span style="color:#a6accd">unix socket</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">**[include]**</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">#包含supervisord.d目录下*.ini文件，也就是说，我们可以将需要启动应用程序的配置放在这个目录下，这就是运维标准化。</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">files = supervisord.d/*.ini</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>3. program 配置</strong></p><p>supervisor主配置文件的变更只需要修改nodaemon=true，可以看到关于程序的配置在主配置文件里面都是注释掉的：</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">;[program:theprogramname]</span></span>
<span class="line"><span style="color:#a6accd">;command=/bin/cat ; the program (relative uses PATH, can take args)</span></span>
<span class="line"><span style="color:#a6accd">;process_name=%(program_name)s ; process_name expr (default %(program_name)s)</span></span>
<span class="line"><span style="color:#a6accd">;numprocs=1 ; number of processes copies to start (def 1)</span></span>
<span class="line"><span style="color:#a6accd">;directory=/tmp ; directory to cwd to before exec (def no cwd)</span></span>
<span class="line"><span style="color:#a6accd">;umask=022 ; umask for process (default None)</span></span>
<span class="line"><span style="color:#a6accd">;priority=999 ; the relative start priority (default 999)</span></span>
<span class="line"><span style="color:#a6accd">;autostart=true ; start at supervisord start (default: true)</span></span>
<span class="line"><span style="color:#a6accd">;autorestart=true ; retstart at unexpected quit (default: true)</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">（省略部分输出）</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>我们现在要做的事情，就是编写一个sshd的程序管理配置，放置到/etc/supervisord.d目录下，注意文件后缀是.ini，你可以复制上面注释掉的内容进行修改：</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@98aced104917 /]# vi /etc/supervisord.d/sshd.ini</span></span>
<span class="line"><span style="color:#a6accd">[program:sshd]</span></span>
<span class="line"><span style="color:#a6accd">command=/usr/sbin/sshd -D</span></span>
<span class="line"><span style="color:#a6accd">process_name=%(program_name)s</span></span>
<span class="line"><span style="color:#a6accd">autostart=true</span></span>
<span class="line"><span style="color:#a6accd">stdout_logfile_maxbytes=100MB</span></span>
<span class="line"><span style="color:#a6accd">stdout_logfile_backups=10</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>4.启动supervisord</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@98aced104917 ~]# /usr/bin/supervisord -c /etc/supervisord.conf</span></span>
<span class="line"><span style="color:#a6accd">[root@98aced104917 ~]# netstat -ntlp</span></span>
<span class="line"><span style="color:#a6accd">Active Internet connections (only servers)</span></span>
<span class="line"><span style="color:#a6accd">Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name</span></span>
<span class="line"><span style="color:#a6accd">tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 31/sshd</span></span>
<span class="line"><span style="color:#a6accd">tcp6 0 0 :::22 :::* LISTEN 31/sshd</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>可以看到sshd已经启动了。是由supervisor进行启动的。</p><p><strong>5. 使用 supervisorctl</strong></p><p>Supervisorctl 是 supervisord 的一个命令行客户端工具，可以用来管理supervisord启动的进程。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@98aced104917 ~]# supervisorctl status</span></span>
<span class="line"><span style="color:#a6accd">sshd RUNNING pid 31, uptime 0:02:35</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>停止与启动sshd</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@98aced104917 ~]# supervisorctl stop sshd</span></span>
<span class="line"><span style="color:#a6accd">sshd: stopped</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">[root@98aced104917 ~]# supervisorctl status</span></span>
<span class="line"><span style="color:#a6accd">sshd STOPPED Oct 14 06:45 AM</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">[root@98aced104917 ~]# supervisorctl start sshd</span></span>
<span class="line"><span style="color:#a6accd">sshd: started</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">[root@98aced104917 ~]# supervisorctl status</span></span>
<span class="line"><span style="color:#a6accd">sshd RUNNING pid 42, uptime 0:00:03</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>6..将supervisord.conf配置文件scp到宿主机目录下，和Dockerfile同目录</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@98aced104917 /]# scp /etc/supervisord.conf</span></span>
<span class="line"><span style="color:#a6accd">192.168.56.11:/opt/dockerfile/system/centos-ssh/</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">[root@98aced104917 ~]# scp /etc/supervisord.d/sshd.ini</span></span>
<span class="line"><span style="color:#a6accd">192.168.56.11:/opt/dockerfile/system/centos-ssh/</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>好的，可以退出容器，这个容器的生命周期结束。</p><h3 id="标准化centos系统镜像" tabindex="-1">标准化CentOS系统镜像 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#标准化centos系统镜像" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>现在我们来构建一个全新的包含ssh的centos系统镜像。同时如果你不需要ssh。你依然可以使用supervisor连进行进程管理。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# cd /opt/dockerfile/system/centos-ssh/</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 centos-ssh]# ls -l</span></span>
<span class="line"><span style="color:#a6accd">total 16</span></span>
<span class="line"><span style="color:#a6accd">-rw-r--r-- 1 root root 554 Oct 14 01:40 Dockerfile</span></span>
<span class="line"><span style="color:#a6accd">-rw-r--r-- 1 root root 142 Oct 14 02:53 sshd.ini</span></span>
<span class="line"><span style="color:#a6accd">-rw-r--r-- 1 root root 7953 Oct 14 02:30 supervisord.conf</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>1.重新编写Dockerfile</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 centos-ssh]# vim Dockerfile</span></span>
<span class="line"><span style="color:#a6accd"># Docker for CentOS</span></span>
<span class="line"><span style="color:#a6accd">#Base image</span></span>
<span class="line"><span style="color:#a6accd">FROM centos</span></span>
<span class="line"><span style="color:#a6accd">#Who</span></span>
<span class="line"><span style="color:#a6accd">MAINTAINER Jason.Zhao xxx@gmail.com</span></span>
<span class="line"><span style="color:#a6accd">#EPEL</span></span>
<span class="line"><span style="color:#a6accd">RUN rpm -ivh http://mirrors.aliyun.com/epel/epel-release-latest-7.noarch.rpm</span></span>
<span class="line"><span style="color:#a6accd">#Base pkg</span></span>
<span class="line"><span style="color:#a6accd">RUN yum install -y openssh-clients openssl-devel openssh-server wget mysql-devel</span></span>
<span class="line"><span style="color:#a6accd">supervisor git redis tree net-tools sudo psmisc &amp;&amp; yum clean all</span></span>
<span class="line"><span style="color:#a6accd"># For SSHD</span></span>
<span class="line"><span style="color:#a6accd">RUN ssh-keygen -t rsa -f /etc/ssh/ssh_host_rsa_key</span></span>
<span class="line"><span style="color:#a6accd">RUN ssh-keygen -t ecdsa -f /etc/ssh/ssh_host_ecdsa_key</span></span>
<span class="line"><span style="color:#a6accd">RUN ssh-keygen -A -t dsa -f /etc/ssh/ssh_host_dsa_key</span></span>
<span class="line"><span style="color:#a6accd"># Set root password</span></span>
<span class="line"><span style="color:#a6accd">RUN echo "root:unixhot.com" | chpasswd</span></span>
<span class="line"><span style="color:#a6accd">#Supervisord config</span></span>
<span class="line"><span style="color:#a6accd">ADD supervisord.conf /etc/supervisord.conf</span></span>
<span class="line"><span style="color:#a6accd">ADD sshd.ini /etc/supervisord.d/sshd.ini</span></span>
<span class="line"><span style="color:#a6accd"># Outside Port</span></span>
<span class="line"><span style="color:#a6accd">EXPOSE 22</span></span>
<span class="line"><span style="color:#a6accd">#supervisord start</span></span>
<span class="line"><span style="color:#a6accd">CMD ["/usr/bin/supervisord", "-c", "/etc/supervisord.conf"]</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>2.构建镜像</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 centos-ssh]# docker build -t unixhot/centos-ssh:latest .</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>3.查看镜像</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 centos-ssh]# docker images | grep unixhot</span></span>
<span class="line"><span style="color:#a6accd">unixhot/centos-ssh latest 0d4b39f9100e 31 seconds ago 304 MB</span></span>
<span class="line"><span style="color:#a6accd">unixhot/centos-ssh v1 ff1ab7d7e7f4 About an hour ago 304 MB</span></span>
<span class="line"><span style="color:#a6accd">unixhot/centos latest 4edcb790dacf 3 hours ago 303 MB</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>4.构建容器</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# docker run -d --name centos-ssh-supervisor -p 2222:22</span></span>
<span class="line"><span style="color:#a6accd">unixhot/centos-ssh</span></span>
<span class="line"><span style="color:#a6accd">9ebf53b3cacd093c11a9b5773c5fc62875626061f3f3fe0b5380013c78f2f15b</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# docker ps -l</span></span>
<span class="line"><span style="color:#a6accd">CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES</span></span>
<span class="line"><span style="color:#a6accd">9ebf53b3cacd unixhot/centos-ssh "/usr/bin/supervisord" 6 seconds ago Up 3</span></span>
<span class="line"><span style="color:#a6accd">seconds 0.0.0.0:2222-&gt;22/tcp centos-ssh-supervisor</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>5.使用ssh连接容器</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# ssh root@192.168.56.11 -p 2222</span></span>
<span class="line"><span style="color:#a6accd">root@192.168.56.11's password:</span></span>
<span class="line"><span style="color:#a6accd">[root@9ebf53b3cacd ~]# ps aux</span></span>
<span class="line"><span style="color:#a6accd">USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND</span></span>
<span class="line"><span style="color:#a6accd">root 1 0.3 0.0 117248 14668 ? Ss 07:25 0:00 /usr/bin/python /usr/bin/supervisord</span></span>
<span class="line"><span style="color:#a6accd">-c /etc/supervisord.conf</span></span>
<span class="line"><span style="color:#a6accd">root 9 0.0 0.0 82560 3608 ? S 07:25 0:00 /usr/sbin/sshd -D</span></span>
<span class="line"><span style="color:#a6accd">root 25 1.2 0.0 139272 5428 ? Ss 07:26 0:00 sshd: root@pts/0</span></span>
<span class="line"><span style="color:#a6accd">root 27 0.0 0.0 13376 1984 pts/0 Ss 07:27 0:00 -bash</span></span>
<span class="line"><span style="color:#a6accd">root 40 0.0 0.0 49024 1808 pts/0 R+ 07:27 0:00 ps aux</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>操作系统标准化约定：</strong></p><p>统一使用supervisor进行进程的管理</p><p>所有Docker容器，使用centos-ssh镜像时提供supervisor的ini配置文件。</p><h2 id="运行环境镜像" tabindex="-1">运行环境镜像 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#运行环境镜像" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h2><p>现在有了系统镜像，当然文中的知识案例，具体的情况可以根据你所在企业的具体情况进行相关的配置，现在我们可以在系统镜像的基础上来构建运行环境。</p><h3 id="java-运行环境" tabindex="-1">Java 运行环境 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#java-运行环境" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>Java环境在生产中非常常见，例如会启动一个单一的Java小进程处理队列里面的内容，只需要有JDK即可。那首先我们先构建一个只包含JDK的运行环境：</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# cd /opt/dockerfile/runtime/</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 runtime]# mkdir java</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>Java运行环境要基于centos-ssh的镜像来操作，所以先运行一个临时容器来构建Java运行环境，然后把操作步骤转换为Dockerfile。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# docker run --rm -it unixhot/centos-ssh /bin/bash</span></span>
<span class="line"><span style="color:#a6accd">[root@ec2696c0f9f4 /]# yum install -y java-1.8.0-openjdk</span></span>
<span class="line"><span style="color:#a6accd">java-1.8.0-openjdk-devel</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>使用yum安装的openjdk，默认可以不设置JAVA_HOME，即可执行相关的java程序。</p><p><strong>1.编写Dockerfile</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 runtime]# vim java/Dockerfile</span></span>
<span class="line"><span style="color:#a6accd"># Docker for CentOS</span></span>
<span class="line"><span style="color:#a6accd">#Base image</span></span>
<span class="line"><span style="color:#a6accd">FROM unixhot/centos-ssh</span></span>
<span class="line"><span style="color:#a6accd">#Who</span></span>
<span class="line"><span style="color:#a6accd">MAINTAINER Jason.Zhao xxx@gmail.com</span></span>
<span class="line"><span style="color:#a6accd">#Base pkg</span></span>
<span class="line"><span style="color:#a6accd">RUN yum install -y java-1.8.0-openjdk java-1.8.0-openjdk-devel &amp;&amp; yum clean all</span></span>
<span class="line"><span style="color:#a6accd"># JAVA_HOME</span></span>
<span class="line"><span style="color:#a6accd">ENV JAVA_HOME /usr/lib/jvm/java-1.8.0-openjdk</span></span>
<span class="line"><span style="color:#a6accd"># Outside Port</span></span>
<span class="line"><span style="color:#a6accd">EXPOSE 22</span></span>
<span class="line"><span style="color:#a6accd">#supervisord start</span></span>
<span class="line"><span style="color:#a6accd">CMD ["/usr/bin/supervisord", "-c", "/etc/supervisord.conf"]</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>2.构建镜像</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 runtime]# docker build -t unixhot/runtime-java ./java/</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>3.查看镜像</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# docker images | grep unixhot</span></span>
<span class="line"><span style="color:#a6accd">unixhot/runtime-java latest ff3a2bb0b2a9 2 minutes ago 505.1 MB</span></span>
<span class="line"><span style="color:#a6accd">unixhot/centos-ssh latest 02d375a33cd1 38 minutes ago 304 MB</span></span>
<span class="line"><span style="color:#a6accd">unixhot/centos-ssh v1 ff1ab7d7e7f4 2 hours ago 304 MB</span></span>
<span class="line"><span style="color:#a6accd">unixhot/centos latest 4edcb790dacf 4 hours ago 303 MB</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>当然十分可以使用，需要大家去运行一个小程序进行测试，这里就不演示了。</p><h3 id="tomcat运行环境" tabindex="-1">Tomcat运行环境 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#tomcat运行环境" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p><strong>相对于单一使用Java命令启动Java服务，Tomcat</strong></p><p><strong>1.启动centos-ssh镜像的实例</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# docker run --rm -it unixhot/centos-ssh /bin/bash</span></span>
<span class="line"><span style="color:#a6accd">[root@80f529b965ef /]# yum install -y java-1.8.0-openjdk</span></span>
<span class="line"><span style="color:#a6accd">java-1.8.0-openjdk-devel</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>为什么不直接使用Java的运行环境构建Tomcat？</strong></p><p>首先，确实可以直接使用Java的运行环境来进行Tomcat的构建，但是会破坏我们系统层、运行环境层、业务层的架构，也就是将依赖关系复杂话了，这要根据企业自身情况来做。我先说说我的理由：</p><p>如果Tomcat使用的Java版本要升级到JDK 1.8版本，而Java运行环境的版本是1.7，不能升级，因为有很多服务就是在JDK 1.7的环境下开发的，未进行迁移。这个时候，如果Tomcat是基于Java 1.7的运行环境构建的，就出现依赖关系了。所以，仅仅是个人的建议：让问题简单化！</p><p><strong>2.安装部署Tomcat 8</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@80f529b965ef /]# cd /usr/local/src</span></span>
<span class="line"><span style="color:#a6accd">[root@80f529b965ef src]# wget http://mirrors.cnnic.cn/apache/tomcat/tomcat-8/v8.5.6/bin/apache-tomcat-8.5.6.tar.gz</span></span>
<span class="line"><span style="color:#a6accd">[root@80f529b965ef src]# tar zxf apache-tomcat-8.5.6.tar</span></span>
<span class="line"><span style="color:#a6accd">[root@80f529b965ef src]# mv apache-tomcat-8.5.6 /usr/local/</span></span>
<span class="line"><span style="color:#a6accd">[root@80f529b965ef src]# ln -s /usr/local/apache-tomcat-8.5.6/</span></span>
<span class="line"><span style="color:#a6accd">/usr/local/tomcat</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>3.使用supervisor启动tomcat</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@897cc10b8adf ~]# cd /etc/supervisord.d/</span></span>
<span class="line"><span style="color:#a6accd">[root@897cc10b8adf supervisord.d]# vi tomcat.ini</span></span>
<span class="line"><span style="color:#a6accd">[program:tomcat]</span></span>
<span class="line"><span style="color:#a6accd">command=/usr/local/tomcat/bin/catalina.sh run</span></span>
<span class="line"><span style="color:#a6accd">process_name=%(program_name)s</span></span>
<span class="line"><span style="color:#a6accd">autostart=true</span></span>
<span class="line"><span style="color:#a6accd">stdout_logfile_maxbytes=100MB</span></span>
<span class="line"><span style="color:#a6accd">stdout_logfile_backups=10</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>更新supervisor并查看tomcat状态</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@897cc10b8adf supervisord.d]# supervisorctl update</span></span>
<span class="line"><span style="color:#a6accd">tomcat: stopped</span></span>
<span class="line"><span style="color:#a6accd">tomcat: updated process group</span></span>
<span class="line"><span style="color:#a6accd">[root@897cc10b8adf supervisord.d]# supervisorctl status</span></span>
<span class="line"><span style="color:#a6accd">sshd RUNNING pid 408, uptime 0:06:00</span></span>
<span class="line"><span style="color:#a6accd">tomcat RUNNING pid 704, uptime 0:00:03</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>4.准备Dockerfile的构建环境</strong></p><p>现在我们开始编写Dockerfile，在编写之前，我们先决定一件事情，apache-tomcat的包，我们应该是像实验中使用wget进行下载并解压呢，还是应该提前将包放在宿主机，构建的时候使用Dockerfile ADD进行呢。</p><p>我建议选择后者，因为实际的工作中，我们通常要对Tomcat进行很多定制化的配置，例如进行安全规范的调整等。</p><p>这里我们能演示的就是直接下载，可以想象一下，这个已经是你修改过的软件包。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# cd /opt/dockerfile/runtime/</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 runtime]# mkdir tomcat</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 runtime]# cd tomcat/</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 tomcat]# wget http://mirrors.cnnic.cn/apache/tomcat/tomcat-8/v8.5.6/bin/apache-tomcat-8.5.6.tar.gz</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>5.编写Dockerfile</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd"># Docker for CentOS</span></span>
<span class="line"><span style="color:#a6accd">#Base image</span></span>
<span class="line"><span style="color:#a6accd">FROM unixhot/centos-ssh</span></span>
<span class="line"><span style="color:#a6accd">#Who</span></span>
<span class="line"><span style="color:#a6accd">MAINTAINER Jason.Zhao xxx@gmail.com</span></span>
<span class="line"><span style="color:#a6accd">#Base pkg</span></span>
<span class="line"><span style="color:#a6accd">RUN yum install -y java-1.8.0-openjdk java-1.8.0-openjdk-devel &amp;&amp; yum clean all</span></span>
<span class="line"><span style="color:#a6accd"># JAVA_HOME</span></span>
<span class="line"><span style="color:#a6accd">ENV JAVA_HOME /usr/lib/jvm/java-1.8.0-openjdk</span></span>
<span class="line"><span style="color:#a6accd"># Tomcat</span></span>
<span class="line"><span style="color:#a6accd">ADD apache-tomcat-8.5.6.tar.gz /usr/local/</span></span>
<span class="line"><span style="color:#a6accd">RUN ln -s /usr/local/apache-tomcat-8.5.6 /usr/local/tomcat</span></span>
<span class="line"><span style="color:#a6accd">ADD tomcat.ini /etc/supervisord.d/tomcat.ini</span></span>
<span class="line"><span style="color:#a6accd">ENV TOMCAT_HOME /usr/local/tomcat</span></span>
<span class="line"><span style="color:#a6accd"># Outside Port</span></span>
<span class="line"><span style="color:#a6accd">EXPOSE 22 8080</span></span>
<span class="line"><span style="color:#a6accd">#supervisord start</span></span>
<span class="line"><span style="color:#a6accd">CMD ["/usr/bin/supervisord", "-c", "/etc/supervisord.conf"]</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>6.构建tomcat镜像</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 tomcat]# ls -l</span></span>
<span class="line"><span style="color:#a6accd">total 9096</span></span>
<span class="line"><span style="color:#a6accd">-rw-r--r-- 1 root root 9304958 Oct 6 16:39 apache-tomcat-8.5.6.tar.gz</span></span>
<span class="line"><span style="color:#a6accd">-rw-r--r-- 1 root root 546 Oct 14 05:51 Dockerfile</span></span>
<span class="line"><span style="color:#a6accd">-rw-r--r-- 1 root root 165 Oct 14 06:33 tomcat.ini</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 tomcat]# docker build -t unixhot/runtime-tomcat .</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>7.查看镜像</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 tomcat]# docker images | grep unixhot</span></span>
<span class="line"><span style="color:#a6accd">unixhot/runtime-tomcat latest ef8372a88ad4 6 minutes ago 518.2 MB</span></span>
<span class="line"><span style="color:#a6accd">unixhot/runtime-java latest ff3a2bb0b2a9 2 hours ago 505.1 MB</span></span>
<span class="line"><span style="color:#a6accd">unixhot/centos-ssh latest 02d375a33cd1 3 hours ago 304 MB</span></span>
<span class="line"><span style="color:#a6accd">unixhot/centos-ssh v1 ff1ab7d7e7f4 4 hours ago 304 MB</span></span>
<span class="line"><span style="color:#a6accd">unixhot/centos latest 4edcb790dacf 6 hours ago 303 MB</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>8.构建tomcat-demo容器</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 tomcat]# docker run -d --name tomcat-demo -p 88:8080 -p</span></span>
<span class="line"><span style="color:#a6accd">89:22 unixhot/runtime-tomcat</span></span>
<span class="line"><span style="color:#a6accd">8cea3ef85634210eb0cfab0f65a63b0ebd6961b5a67fc765b762785c01cd2c18</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 tomcat]# docker ps -l</span></span>
<span class="line"><span style="color:#a6accd">CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES</span></span>
<span class="line"><span style="color:#a6accd">8cea3ef85634 unixhot/runtime-tomcat "/usr/bin/supervisord" 5 seconds ago Up 2</span></span>
<span class="line"><span style="color:#a6accd">seconds 0.0.0.0:89-&gt;22/tcp, 0.0.0.0:88-&gt;8080/tcp tomcat-demo</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>9.访问tomcat</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">http://192.168.56.11:88/</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h2 id="应用镜像" tabindex="-1">应用镜像 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#应用镜像" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h2><p>最上层的业务镜像就是在运行环境的基础上，直接使用或者根据业务情况再次进行二次定制。例如对于PHP运行环境，默认只安装基础通用的模块，对于业务需要的时候再次进行二次构建。这里我们拿一个比较简单的Jenkins来模拟。</p><h3 id="jenkins镜像构建" tabindex="-1">Jenkins镜像构建 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#jenkins镜像构建" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>JenkinsJenkins是基于Java开发的一种持续集成工具，它有自己的yum安装方式，但是最直接的还是直接下载war包存放到tomcat目录下。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# cd /opt/dockerfile/app/</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 app]# mkdir jenkins</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 app]# cd jenkins/</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 jenkins]# wget http://mirrors.jenkins-ci.org/war/latest/jenkins.war</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>2.编写Dockerfile</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd"># Docker for CentOS</span></span>
<span class="line"><span style="color:#a6accd">#Base image</span></span>
<span class="line"><span style="color:#a6accd">FROM unixhot/runtime-tomcat</span></span>
<span class="line"><span style="color:#a6accd">#Who</span></span>
<span class="line"><span style="color:#a6accd">MAINTAINER Jason.Zhao xxx@gmail.com</span></span>
<span class="line"><span style="color:#a6accd"># Jenkins</span></span>
<span class="line"><span style="color:#a6accd">ADD jenkins.war /usr/local/tomcat/webapps/</span></span>
<span class="line"><span style="color:#a6accd"># Outside Port</span></span>
<span class="line"><span style="color:#a6accd">EXPOSE 22 8080</span></span>
<span class="line"><span style="color:#a6accd">#supervisord start</span></span>
<span class="line"><span style="color:#a6accd">CMD ["/usr/bin/supervisord", "-c", "/etc/supervisord.conf"]</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>3.构建jenkins镜像</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 jenkins]# docker build -t unixhot/jenkins .</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>4.查看镜像</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 jenkins]# docker images | grep unixhot</span></span>
<span class="line"><span style="color:#a6accd">unixhot/jenkins latest a7b16bc093ed 40 seconds ago 588.7 MB</span></span>
<span class="line"><span style="color:#a6accd">unixhot/runtime-tomcat latest 3b596c9a7696 2 hours ago 518.8 MB</span></span>
<span class="line"><span style="color:#a6accd">unixhot/runtime-java latest 476a6a2fc074 2 hours ago 505.8 MB</span></span>
<span class="line"><span style="color:#a6accd">unixhot/centos-ssh latest d85aa8a9dfd4 3 hours ago 304.6 MB</span></span>
<span class="line"><span style="color:#a6accd">unixhot/centos-ssh v1 3309208ed679 4 hours ago 300.9 MB</span></span>
<span class="line"><span style="color:#a6accd">unixhot/centos latest 83f0491a30e1 4 hours ago 282.3 MB</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>5.创建jenkins-demo容器</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 jenkins]# docker run -d --name jenkins-demo -p 91:22 -p</span></span>
<span class="line"><span style="color:#a6accd">92:8080 unixhot/jenkins</span></span>
<span class="line"><span style="color:#a6accd">6d88ba756befc84ef1820818994be028c12ac13825c2b439d680358d101e922e</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 jenkins]# docker ps -l</span></span>
<span class="line"><span style="color:#a6accd">CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES</span></span>
<span class="line"><span style="color:#a6accd">6d88ba756bef unixhot/jenkins "/usr/bin/supervisord" 7 seconds ago Up 2 seconds</span></span>
<span class="line"><span style="color:#a6accd">0.0.0.0:91-&gt;22/tcp, 0.0.0.0:92-&gt;8080/tcp jenkins-demo</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>6.访问jenkins并进行初始化安装</strong></p><p><img src="http://k8s.unixhot.com/docker/media/e59722d6316d2cc9702c1989e1c9f3f1.png" alt="img"></p><p>当然如果你想构建一个启动后直接就可以使用的Jenkins，可以在安装完毕后，把刚才的容器重新提交成为镜像。</p><p><strong>7.安装Jenkins并提交成为jenkins-login镜像。</strong></p><p>进入镜像或者安装的密码，填入密码框并继续。后面步骤省略。全部安装默认即可。注意安装插件时时间会比较长。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# ./docker_in.sh jenkins-demo</span></span>
<span class="line"><span style="color:#a6accd">[root@b3bbada66693 /]# cat /root/.jenkins/secrets/initialAdminPassword</span></span>
<span class="line"><span style="color:#a6accd">b5257b8891464ebf890b38c00d24de71</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>获取Container ID：</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# docker ps -l</span></span>
<span class="line"><span style="color:#a6accd">CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES</span></span>
<span class="line"><span style="color:#a6accd">b3bbada66693 unixhot/jenkins "/usr/bin/supervisord" 9 minutes ago Up 9 minutes</span></span>
<span class="line"><span style="color:#a6accd">0.0.0.0:91-&gt;22/tcp, 0.0.0.0:92-&gt;8080/tcp jenkins-demo</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>提交新的镜像</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# docker commit -m "jenkins login" b3bbada66693</span></span>
<span class="line"><span style="color:#a6accd">unixhot/jenkins-login</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# docker images | grep jenkins</span></span>
<span class="line"><span style="color:#a6accd">unixhot/jenkins-login latest c453363bb9f7 22 seconds ago 776.6 MB</span></span>
<span class="line"><span style="color:#a6accd">unixhot/jenkins latest a7b16bc093ed 13 minutes ago 588.7 MB</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>8.使用jenkins-login镜像构建容器</strong></p><p>现在可以使用的Jenkins容器诞生了，赶紧创建一个容器来进行使用吧。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# docker run -d --name jenkins-login -p 93:22 -p 94:8080</span></span>
<span class="line"><span style="color:#a6accd">unixhot/jenkins-login</span></span>
<span class="line"><span style="color:#a6accd">af421b4d5b076c756ab58ed8cc95c615a321f9517d30b755667d5d80d1336a63</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# docker ps -l</span></span>
<span class="line"><span style="color:#a6accd">CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES</span></span>
<span class="line"><span style="color:#a6accd">af421b4d5b07 unixhot/jenkins-login "/usr/bin/supervisord" 4 seconds ago Up 2</span></span>
<span class="line"><span style="color:#a6accd">seconds 0.0.0.0:93-&gt;22/tcp, 0.0.0.0:94-&gt;8080/tcp jenkins-login</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>9.访问Jenkins容器，开始持续集成之路</strong></p><p><img src="http://k8s.unixhot.com/docker/media/04b8b149d6c7ce1e18f67796998be9b5.png" alt="img"></p><h1 id="_3-docker私有仓库实践" tabindex="-1">3 Docker私有仓库实践 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_3-docker私有仓库实践" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h1><h2 id="docker-registry" tabindex="-1">Docker Registry <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#docker-registry" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h2><h3 id="创建docker-registry" tabindex="-1">创建Docker Registry <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#创建docker-registry" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>在安装了 Docker 后，可以通过获取官方 registry 镜像来运行。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# docker run -d --name docker-registry -p 5000:5000 registry</span></span>
<span class="line"><span style="color:#a6accd">b822d0a3e77e6a3144408119ccc2e7344f60f7404651adc025ce84561245b2d2</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h3 id="配置docker信任仓库" tabindex="-1">配置Docker信任仓库 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#配置docker信任仓库" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# vim /etc/docker/daemon.json</span></span>
<span class="line"><span style="color:#a6accd">{</span></span>
<span class="line"><span style="color:#a6accd">"registry-mirrors": ["https://tdimi5q1.mirror.aliyuncs.com"],</span></span>
<span class="line"><span style="color:#a6accd">"insecure-registries" : ["http://192.168.56.11:5000"]</span></span>
<span class="line"><span style="color:#a6accd">}</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# systemctl restart docker</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h3 id="push镜像到docker-registry" tabindex="-1">Push镜像到Docker Registry <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#push镜像到docker-registry" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# docker tag system/centos 192.168.56.11:5000/system/centos</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# docker push 192.168.56.11:5000/system/centos</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h2 id="registry项目harbor" tabindex="-1">Registry项目Harbor <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#registry项目harbor" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h2><p>上面我们简单的介绍了Docker Registry的使用，但是真正生产环境，我们无法有效的管理Docker Regisry。官方提供了收费版的Registry，社区有开源版本的Harbor。</p><p><strong>Harbor简介</strong></p><p>Harbor是一个用于存储和分发Docker镜像的企业级Registry服务器，通过添加一些企业必需的功能特性，例如安全、标识和管理等，扩展了开源Docker Distribution。作为一个企业级私有Registry服务器，Harbor提供了更好的性能和安全。提升用户使用Registry构建和运行环境传输镜像的效率。Harbor支持安装在多个Registry节点的镜像资源复制，镜像全部保存在私有Registry中， 确保数据和知识产权在公司内部网络中管控。另外，Harbor也提供了高级的安全特性，诸如用户管理，访问控制和活动审计等。</p><p>基于角色的访问控制 - 用户与Docker镜像仓库通过“项目”进行组织管理，一个用户可以对多个镜像仓库在同一命名空间（project）里有不同的权限。</p><p>镜像复制 - 镜像可以在多个Registry实例中复制（同步）。尤其适合于负载均衡，高可用，混合云和多云的场景。</p><p>图形化用户界面 - 用户可以通过浏览器来浏览，检索当前Docker镜像仓库，管理项目和命名空间。</p><p>AD/LDAP 支持 - Harbor可以集成企业内部已有的AD/LDAP，用于鉴权认证管理。</p><p>审计管理 - 所有针对镜像仓库的操作都可以被记录追溯，用于审计管理。</p><p>国际化 - 已拥有英文、中文、德文、日文和俄文的本地化版本。更多的语言将会添加进来。</p><p>RESTful API - RESTful API 提供给管理员对于Harbor更多的操控, 使得与其它管理软件集成变得更容易。</p><p>部署简单 - 提供在线和离线两种安装工具， 也可以安装到vSphere平台(OVA方式)虚拟设备。</p><h3 id="harbor部署" tabindex="-1">Harbor部署 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#harbor部署" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>Harbor为运维提供了多种部署方法，你可以直接clone最新代码，也可以支持offline的部署方法，直接下载官方构建好的镜像来进行使用。可以在github的releases查看最新版本，本文是1.7.1版本。<em><a target="_blank" rel="noreferrer" href="https://github.com/vmware/harbor/releases"><!--[-->https://github.com/vmware/harbor/releases<!--]--><!----></a></em></p><p>建议下载offline的压缩包，里面包含了harbor启动所用的所有docker镜像，可以快速的部署harbor。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd"># cd /usr/local/src</span></span>
<span class="line"><span style="color:#a6accd"># wget https://storage.googleapis.com/harbor-releases/release-1.7.0/harbor-offline-installer-v1.7.1.tgz</span></span>
<span class="line"><span style="color:#a6accd"># tar zxf harbor-offline-installer-v1.7.1.tgz</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>Harbor的每个组件都是以Docker容器的形式构建的，使用Docker Compose来对它进行部署，你可以查看docker-compose.yml文件。</p><h3 id="harbor配置" tabindex="-1">Harbor配置 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#harbor配置" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p><strong>为Harbor配置HTTP访问</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# cd harbor</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 harbor]# vim harbor.cfg</span></span>
<span class="line"><span style="color:#a6accd">hostname = 192.168.56.11</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>配置Docker信任仓库</strong></p><p>如果使用http的方式配置harbor需要为所有Docker添加信任配置。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# vim /etc/docker/daemon.json</span></span>
<span class="line"><span style="color:#a6accd">{</span></span>
<span class="line"><span style="color:#a6accd">"registry-mirrors": ["https://tdimi5q1.mirror.aliyuncs.com"],</span></span>
<span class="line"><span style="color:#a6accd">"insecure-registries" : ["http://192.168.56.11"]</span></span>
<span class="line"><span style="color:#a6accd">}</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# systemctl restart docker</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>为Harbor配置https访问</strong></p><p>默认情况下Harbor是使用http进行访问，官方提供了自签名证书的方法，不过生产环境还是建议购买SSL证书。</p><p><strong>1.申请证书</strong></p><p>如果你没有SSL证书，那么也不要使用网上复杂的自签名证书的步骤了。目前阿里云提供Symantec 免费型DV SSL证书。申请成功后，下载Nginx版本的证书文件。</p><p><strong>2.Harbor配置</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@docker-node1 harbor]# cd /usr/local/src/harbor/</span></span>
<span class="line"><span style="color:#a6accd">[root@docker-node1 harbor]# vim harbor.cfg</span></span>
<span class="line"><span style="color:#a6accd">hostname = reg.unixhot.com</span></span>
<span class="line"><span style="color:#a6accd">ui_url_protocol = https</span></span>
<span class="line"><span style="color:#a6accd">ssl_cert = /usr/local/src/harbor/1_registry.linuxhot.com_bundle.crt</span></span>
<span class="line"><span style="color:#a6accd">ssl_cert_key = /usr/local/src/harbor/2_registry.linuxhot.com.key</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>请将证书放置在宿主机上，并配置正确的证书路径。</p><p><strong>Docker Compose安装</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]#yum install -y docker-compose</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# docker-compose --version</span></span>
<span class="line"><span style="color:#a6accd">docker-compose version 1.8.0, build 94f7016</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>安装Harbor</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@k8s-harbor-1 ~]# cd /usr/local/src/harbor/</span></span>
<span class="line"><span style="color:#a6accd">[root@k8s-harbor-1 harbor]# ./install.sh</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>查看Harbor状态</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@k8s-harbor-1 harbor]# docker-compose ps</span></span>
<span class="line"><span style="color:#a6accd">Name Command State Ports</span></span>
<span class="line"><span style="color:#a6accd">------------------------------------------------------------------------------------------------------------------------------</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">harbor-adminserver /harbor/start.sh Up</span></span>
<span class="line"><span style="color:#a6accd">harbor-core /harbor/start.sh Up</span></span>
<span class="line"><span style="color:#a6accd">harbor-db /entrypoint.sh postgres Up 5432/tcp</span></span>
<span class="line"><span style="color:#a6accd">harbor-jobservice /harbor/start.sh Up</span></span>
<span class="line"><span style="color:#a6accd">harbor-log /bin/sh -c /usr/local/bin/ ... Up 127.0.0.1:1514-&gt;10514/tcp</span></span>
<span class="line"><span style="color:#a6accd">harbor-portal nginx -g daemon off; Up 80/tcp</span></span>
<span class="line"><span style="color:#a6accd">nginx nginx -g daemon off; Up 0.0.0.0:443-&gt;443/tcp, 0.0.0.0:4443-&gt;4443/tcp,</span></span>
<span class="line"><span style="color:#a6accd">0.0.0.0:80-&gt;80/tcp</span></span>
<span class="line"><span style="color:#a6accd">redis docker-entrypoint.sh redis ... Up 6379/tcp</span></span>
<span class="line"><span style="color:#a6accd">registry /entrypoint.sh /etc/regist ... Up 5000/tcp</span></span>
<span class="line"><span style="color:#a6accd">registryctl /harbor/start.sh Up</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h3 id="使用harbor管理registry" tabindex="-1">使用Harbor管理Registry <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#使用harbor管理registry" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>现在你就可以访问你的域名<a target="_blank" rel="noreferrer" href="http://reg.linuxhot.xn--com-vj3fu53h4lw8bl.xn--admin%2Charbor12345-8l75a8z3f3f3fwrmtj9hrk9d./"><!--[-->http://reg.linuxhot.com进行登录。默认用户admin，密码Harbor12345。<!--]--><!----></a></p><p><img src="http://k8s.unixhot.com/docker/media/366c20c68d5e6467434fb8108baa6081.png" alt="img"></p><p>登录后的第一件事情永远都是修改默认密码。然后你就可以在项目管理中，新建和管理项目了。不过默认情况下创建的项目library是公开的，如果你要使用这个项目，而且域名放在公网上，请取消公开。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# docker login reg.linuxhot.com</span></span>
<span class="line"><span style="color:#a6accd">Username: admin</span></span>
<span class="line"><span style="color:#a6accd">Password:</span></span>
<span class="line"><span style="color:#a6accd">WARNING! Your password will be stored unencrypted in /root/.docker/config.json.</span></span>
<span class="line"><span style="color:#a6accd">Configure a credential helper to remove this warning. See</span></span>
<span class="line"><span style="color:#a6accd">https://docs.docker.com/engine/reference/commandline/login/#credentials-store</span></span>
<span class="line"><span style="color:#a6accd">Login Succeeded</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>提交镜像到Registry</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd"># docker tag centos:latest reg.linuxhot.com/system/centos:latest</span></span>
<span class="line"><span style="color:#a6accd"># docker push reg.linuxhot.com/system/centos:latest</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h1 id="kubernetes快速入门" tabindex="-1">Kubernetes快速入门 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#kubernetes快速入门" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h1><p>学习Kubernetes最权威的知识来源就是Kubernetes官方文档，而且对于初学者来说，官方文档可能不是最佳选择。本章将带你循循序渐进的学习Kubernetes，后面章节会通过大量的实践案例来理解和掌握Kubernetes的知识。</p><ul><li>Kubernetes官方文档：<a target="_blank" rel="noreferrer" href="https://kubernetes.io/docs/home/"><!--[-->https://kubernetes.io/docs/home/<!--]--><!----></a></li><li>Kuernetes Github：<a target="_blank" rel="noreferrer" href="https://github.com/kubernetes/"><!--[-->https://github.com/kubernetes/<!--]--><!----></a></li></ul><h2 id="kubernetes架构介绍" tabindex="-1">Kubernetes架构介绍 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#kubernetes架构介绍" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h2><p>Kubernetes 源于希腊语，意为 “舵手” 或 “飞行员”，是用于自动部署，扩展和管理容器化应用程序的开源系统，由于K和S之间有8个字母，被简称为K8S。Kubernetes 构建在 Google 15 年生产环境经验基础之上，可以将Kubernetes看作为Google内部的容器管理平台Brog的开源版本，当然他们之间是有一些差异的。</p><h3 id="kubernetes系统架构" tabindex="-1">Kubernetes系统架构 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#kubernetes系统架构" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>Kubernetes被设计为Master和Node两个角色，这类似于OpenStack的架构理念，Master为控制节点，Node为计算节点或者叫工作节点，在Master节点上有一个API Server服务，对外提供标准的RestAPI，这也是Kubernetes集群的入口，意外着只要和集群进行交互必须连接到API Server上。</p><p><img src="http://k8s.unixhot.com/kubernetes/media/4f93fc22b605a967fa54a2491557c04e.png" alt="img"></p><p><strong>Master节点介绍</strong></p><p>Kubernetes Master节点主要有4个组件，API Server、Scheduler、Controller、etcd。如下图所示：</p><p><img src="http://k8s.unixhot.com/kubernetes/media/0786901c59be2f756d7b979619a048b2.png" alt="img"></p><ul><li><strong>API Server</strong>：提供Kubernetes API接口，主要处理 Rest操作以及更新Etcd中的对象。是所有资源增删改查的唯一入口。</li><li><strong>Scheduler</strong>：绑定Pod到Node上，主要做资源调度。</li><li><strong>Controller Manager</strong>：所有其他群集级别的功能，目前由控制器Manager执行。资源对象的自动化控制中心，Kubernetes集群有很多控制器。</li><li><strong>Etcd</strong>：所有持久化的状态信息存储在Etcd中，这个是Kubernetes集群的数据库。</li></ul><p><strong>Node节点介绍</strong></p><p>Node节点是Kubernetes集群的工作节点，在Node节点上主要运行了Docker、Kubelet、kube-proxy三个服务（Fluentd请先忽略），如下图所示：</p><p><img src="http://k8s.unixhot.com/kubernetes/media/ff26ae9ea18c1a93e50b8226abfa2fa7.png" alt="img"></p><ul><li><strong>Docker Engine</strong>：负责节点的容器的管理工作，最终创建出来的是一个Docker容器。</li><li><strong>Kubelet</strong>：安装在Node上的代理服务，用来管理Pods以及容器、镜像、Volume等，实现对集群对节点的管理。</li><li><strong>Kube-proxy</strong>：安装在Node上的网络代理服务，提供网络代理以及负载均衡，实现与Service通讯。</li></ul><h3 id="kubernetes逻辑架构" tabindex="-1">Kubernetes逻辑架构 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#kubernetes逻辑架构" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>在上面的介绍中提到像Pod、Service这些概念，在Kubernetes的系统架构图中并没有体现出来，这些可以理解为Kubernetes逻辑架构中的组件。也就是在Master和Node上并不具体存在的一个服务或者进程，但却是Kubernetes的组件，也是我们的管理对象。主要有Pod、Service和各种控制器等。</p><p><strong>Pod</strong></p><p>Pod是Kubernetes的最小管理单元，一个Pod可以包含一组容器和卷。虽然一个Pod里面可以包含一个或者多个容器，但是Pod只有一个IP地址，而且Pod重启后，IP地址会发生变化。</p><p><img src="http://k8s.unixhot.com/kubernetes/media/e2a89d5ac819b578808e62d8fee0e960.png" alt="img"></p><p><strong>Controller</strong></p><p>一个应用如果可以有一个或者多个Pod，就像你给某一个应用做集群，集群中的所有Pod是一模一样的。Kubernetes中有很多控制器可以来管理Pod，例如下图的Replication Controller可以控制Pod的副本数量，从而实现横向扩展。</p><p><img src="http://k8s.unixhot.com/kubernetes/media/c4ae2886ff4fdb51b9a0dd20a14c8e50.png" alt="img"></p><p>Kubernetes中有很多控制器，后面的章节我们会一一讲到，常用的控制器如下：</p><ul><li>Replication Controller（新版本已经被ReplicaSet所替代）</li><li>ReplicaSet（新版本被封装在Deployment中）</li><li>Deployment：封装了Pod的副本管理、部署更新、回滚、扩容、缩容等功能。</li><li>DaemonSet：保证所有的Node上有且只有一个Pod在运行。</li><li>StatefulSet：有状态的应用，为 Pod 提供唯一的标识，它可以保证部署和 scale 的顺序。</li><li>Job：使用Kubernetes运行单一任务。</li><li>CronJob：使用Kubernetes运行定时任务。</li></ul><p><strong>Service</strong></p><p>由于Pod的生命周期是短暂的，而且每次重启Pod的IP地址都会发生变化，而且一个Pod有多个副本，也就是说一个集群中有了多个节点，就需要考虑负载均衡的问题。Kubernetes使用Service来实现Pod的访问，而且Service有一个Cluster IP，通常也称之为VIP，是固定不变的。</p><p><img src="http://k8s.unixhot.com/kubernetes/media/2e61328894d1eaf548ad6ff06d85a6a3.png" alt="img"></p><h3 id="kubernetes网络介绍" tabindex="-1">Kubernetes网络介绍 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#kubernetes网络介绍" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>在Kubernetes集群中存在着三种网络，分别是Node网络、Pod网络和Service网络，这几种网络之间的通信需要依靠网络插件，Kubernetes本身并没有提供，社区提供了像Flannel、Calico、Cannal等，后面章节会详述。</p><p><strong>Node网络</strong></p><p>Node网络指的是Kubernetes Node节点本地的网络，在本实验环境中使用的是192.168.56.0/24这个网段，所有的Node和Master在该网段都可以正常通信。</p><p><strong>Pod网络</strong></p><p>后面创建的Pod，每一个Pod都会有一个IP地址，这个IP地址网络段被称之为Pod网络，如下图所示。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# kubectl get pod -o wide</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">nginx-54458cd494-hpn68 1/1 Running 0 9m7s 10.2.1.2 linux-node2.linuxhot.com</span></span>
<span class="line"><span style="color:#a6accd">&lt;none&gt; &lt;none&gt;</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">nginx-54458cd494-r4mfq 1/1 Running 0 7m46s 10.2.1.3 linux-node2.linuxhot.com</span></span>
<span class="line"><span style="color:#a6accd">&lt;none&gt; &lt;none&gt;</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>Service网络</strong></p><p>Service是为Pod提供访问和负载均衡的网络地址段，如下图所示。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# kubectl get service</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">kubernetes ClusterIP 10.1.0.1 &lt;none&gt; 443/TCP 64m</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">nginx NodePort 10.1.216.23 &lt;none&gt; 80:30893/TCP 8m3s</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>Kubernetes的组件和知识绝非如此，快速入门可以先了解这么多，下一章节，我们先快速的部署一个Kubernetes集群。</p><h2 id="使用kubeadm部署kubernetes-v1-16-4" tabindex="-1">使用kubeadm部署Kubernetes v1.16.4 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#使用kubeadm部署kubernetes-v1-16-4" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h2><p>想要快速的体验Kubernetes的功能，官方提供了非常多的部署方案，可以使用官方提供的kubeadm以容器的方式运行Kubernetes集群，也可以使用二进制方式部署更有利于理解Kubernetes的架构，我们先使用kubeadm快速的部署一个Kubernetes集群后，学习Kubernetes的使用，然后动手使用二进制的方式来深入理解Kubernetes架构。</p><blockquote><p>注意：请不要把目光仅仅放在部署上，要慢慢的了解其本质。</p></blockquote><p>Kubernetesv1.13版本发布后，kubeadm才正式进入GA，可以生产使用。目前Kubernetes的对应镜像仓库，在国内阿里云也有了镜像站点，使用kubeadm部署Kubernetes集群变得简单并且容易了很多，本文使用kubeadm带领大家快速部署Kubernetes v1.16.2版本。</p><p><strong>实验环境准备</strong></p><p>在本书的实验环境的基础上，我们如下来分配角色：</p><table><thead><tr><th>主机名</th><th>IP地址（NAT）</th><th>最低配置</th><th>描述</th></tr></thead><tbody><tr><td><a target="_blank" rel="noreferrer" href="http://linux-node1.linuxhot.com"><!--[-->linux-node1.linuxhot.com<!--]--><!----></a></td><td>eth0:192.168.56.11</td><td>1CPU/1G内存</td><td>Kubernets Master/Etcd节点</td></tr><tr><td><a target="_blank" rel="noreferrer" href="http://linux-node2.linuxhot.com"><!--[-->linux-node2.linuxhot.com<!--]--><!----></a></td><td>eth0:192.168.56.12</td><td>1CPU/1G内存</td><td>Kubernets Node节点</td></tr><tr><td><a target="_blank" rel="noreferrer" href="http://linux-node3.linuxhot.com"><!--[-->linux-node3.linuxhot.com<!--]--><!----></a></td><td>eth0:192.168.56.13</td><td>1CPU/1G内存</td><td>Kubernets Node节点</td></tr><tr><td>Service网段</td><td>10.1.0.0/16</td><td></td><td></td></tr><tr><td>Pod网段</td><td>10.2.0.0/16</td><td></td><td></td></tr><tr><td>备注</td><td>如果有条件可以部署多个Kubernets node，实验效果更佳。</td><td></td><td></td></tr></tbody></table><h3 id="部署docker" tabindex="-1">部署Docker <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#部署docker" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>首先需要在所有Kubernetes集群的节点中安装Docker和kubeadm。</p><p><strong>1.设置使用国内Yum源</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# cd /etc/yum.repos.d/</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 yum.repos.d]# wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>2.安装指定的Docker版本</strong></p><p>由于kubeadm对Docker的版本是有要求的，需要安装与Kubernetes匹配的版本，这个对应关系一般在每次发布的Changlog中可以找到，例如1.16.2的CHANGELOG如下：<a target="_blank" rel="noreferrer" href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.16.md"><!--[-->CHANGELOG<!--]--><!----></a></p><p>当前v1.16.2支持的Docker版本有v1.13.1, 17.03, 17.06, 17.09, 18.06, 18.09,可以通过下面命令查看：</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# yum list docker-ce.x86_64 --showduplicates | sort -r</span></span>
<span class="line"><span style="color:#a6accd"> * updates: mirror.jdcloud.com</span></span>
<span class="line"><span style="color:#a6accd">Loading mirror speeds from cached hostfile</span></span>
<span class="line"><span style="color:#a6accd">Loaded plugins: fastestmirror</span></span>
<span class="line"><span style="color:#a6accd"> * extras: mirror.jdcloud.com</span></span>
<span class="line"><span style="color:#a6accd"> * epel: mirrors.njupt.edu.cn</span></span>
<span class="line"><span style="color:#a6accd">docker-ce.x86_64            3:19.03.4-3.el7                     docker-ce-stable</span></span>
<span class="line"><span style="color:#a6accd">docker-ce.x86_64            3:19.03.3-3.el7                     docker-ce-stable</span></span>
<span class="line"><span style="color:#a6accd">docker-ce.x86_64            3:19.03.2-3.el7                     docker-ce-stable</span></span>
<span class="line"><span style="color:#a6accd">docker-ce.x86_64            3:19.03.1-3.el7                     docker-ce-stable</span></span>
<span class="line"><span style="color:#a6accd">docker-ce.x86_64            3:19.03.0-3.el7                     docker-ce-stable</span></span>
<span class="line"><span style="color:#a6accd">docker-ce.x86_64            3:18.09.9-3.el7                     docker-ce-stable</span></span>
<span class="line"><span style="color:#a6accd">docker-ce.x86_64            3:18.09.8-3.el7                     docker-ce-stable</span></span>
<span class="line"><span style="color:#a6accd">docker-ce.x86_64            3:18.09.7-3.el7                     docker-ce-stable</span></span>
<span class="line"><span style="color:#a6accd">docker-ce.x86_64            3:18.09.6-3.el7                     docker-ce-stable</span></span>
<span class="line"><span style="color:#a6accd">docker-ce.x86_64            3:18.09.5-3.el7                     docker-ce-stable</span></span>
<span class="line"><span style="color:#a6accd">docker-ce.x86_64            3:18.09.4-3.el7                     docker-ce-stable</span></span>
<span class="line"><span style="color:#a6accd">docker-ce.x86_64            3:18.09.3-3.el7                     docker-ce-stable</span></span>
<span class="line"><span style="color:#a6accd">docker-ce.x86_64            3:18.09.2-3.el7                     docker-ce-stable</span></span>
<span class="line"><span style="color:#a6accd">docker-ce.x86_64            3:18.09.1-3.el7                     docker-ce-stable</span></span>
<span class="line"><span style="color:#a6accd">docker-ce.x86_64            3:18.09.0-3.el7                     docker-ce-stable</span></span>
<span class="line"><span style="color:#a6accd">docker-ce.x86_64            18.06.3.ce-3.el7                    docker-ce-stable</span></span>
<span class="line"><span style="color:#a6accd">docker-ce.x86_64            18.06.2.ce-3.el7                    docker-ce-stable</span></span>
<span class="line"><span style="color:#a6accd">docker-ce.x86_64            18.06.1.ce-3.el7                    docker-ce-stable</span></span>
<span class="line"><span style="color:#a6accd">docker-ce.x86_64            18.06.0.ce-3.el7                    docker-ce-stable</span></span>
<span class="line"><span style="color:#a6accd">docker-ce.x86_64            18.03.1.ce-1.el7.centos             docker-ce-stable</span></span>
<span class="line"><span style="color:#a6accd">docker-ce.x86_64            18.03.0.ce-1.el7.centos             docker-ce-stable</span></span>
<span class="line"><span style="color:#a6accd">docker-ce.x86_64            17.12.1.ce-1.el7.centos             docker-ce-stable</span></span>
<span class="line"><span style="color:#a6accd">docker-ce.x86_64            17.12.0.ce-1.el7.centos             docker-ce-stable</span></span>
<span class="line"><span style="color:#a6accd">docker-ce.x86_64            17.09.1.ce-1.el7.centos             docker-ce-stable</span></span>
<span class="line"><span style="color:#a6accd">docker-ce.x86_64            17.09.0.ce-1.el7.centos             docker-ce-stable</span></span>
<span class="line"><span style="color:#a6accd">docker-ce.x86_64            17.06.2.ce-1.el7.centos             docker-ce-stable</span></span>
<span class="line"><span style="color:#a6accd">docker-ce.x86_64            17.06.1.ce-1.el7.centos             docker-ce-stable</span></span>
<span class="line"><span style="color:#a6accd">docker-ce.x86_64            17.06.0.ce-1.el7.centos             docker-ce-stable</span></span>
<span class="line"><span style="color:#a6accd">docker-ce.x86_64            17.03.3.ce-1.el7                    docker-ce-stable</span></span>
<span class="line"><span style="color:#a6accd">docker-ce.x86_64            17.03.2.ce-1.el7.centos             docker-ce-stable</span></span>
<span class="line"><span style="color:#a6accd">docker-ce.x86_64            17.03.1.ce-1.el7.centos             docker-ce-stable</span></span>
<span class="line"><span style="color:#a6accd">docker-ce.x86_64            17.03.0.ce-1.el7.centos             docker-ce-stable</span></span>
<span class="line"><span style="color:#a6accd"> * base: mirrors.neusoft.edu.cn</span></span>
<span class="line"><span style="color:#a6accd">Available Packages</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>3.安装Docker18.09版本</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# yum -y install docker-ce-18.09.9-3.el7 docker-ce-cli-18.09.9-3.el7</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>4.设置cgroup驱动使用systemd</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# mkdir /etc/docker</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# cat &gt; /etc/docker/daemon.json &lt;&lt;EOF</span></span>
<span class="line"><span style="color:#a6accd">    {</span></span>
<span class="line"><span style="color:#a6accd">      "registry-mirrors": ["https://dx5z2hy7.mirror.aliyuncs.com"],</span></span>
<span class="line"><span style="color:#a6accd">      "exec-opts": ["native.cgroupdriver=systemd"]</span></span>
<span class="line"><span style="color:#a6accd">    }</span></span>
<span class="line"><span style="color:#a6accd">EOF</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>5.启动后台进程</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# systemctl enable docker &amp;&amp; systemctl start docker</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>6.查看Docker版本</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# docker --version</span></span>
<span class="line"><span style="color:#a6accd">Docker version 18.09.9, build 039a7df9ba</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h3 id="部署kubadm和kubelet" tabindex="-1">部署kubadm和kubelet <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#部署kubadm和kubelet" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>在Kubernetes集群的所有节点上部署完毕Docker后，还需要全部部署kubeadm和kubelet，其中kubeadm是管理工具，kubelet是一个服务，用于启动Kubernetes对应的服务。</p><p><strong>1.设置kubernetes YUM仓库</strong></p><p>这里在官方文档的基础上修改为了国内阿里云的yum仓库，</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# vim /etc/yum.repos.d/kubernetes.repo</span></span>
<span class="line"><span style="color:#a6accd">[kubernetes]</span></span>
<span class="line"><span style="color:#a6accd">name=Kubernetes</span></span>
<span class="line"><span style="color:#a6accd">baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/</span></span>
<span class="line"><span style="color:#a6accd">enabled=1</span></span>
<span class="line"><span style="color:#a6accd">gpgcheck=1</span></span>
<span class="line"><span style="color:#a6accd">repo_gpgcheck=1</span></span>
<span class="line"><span style="color:#a6accd">gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><blockquote><p>注意：最下面一行gpgkey的两个URL地址之间是空格，因为排版问题可能导致换行。</p></blockquote><p><strong>2.安装软件包</strong></p><p>由于版本更新频繁，请指定对应的版本号，本文采用1.16.2版本，其它版本未经测试，如果不指定版本默认安装最新版本。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# yum install -y kubelet-1.16.2 kubeadm-1.16.2 kubectl-1.16.2 ipvsadm</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>3.配置kubelet</strong></p><p>默认情况下，Kubelet不允许所在的主机存在交换分区，后期规划的时候，可以考虑在系统安装的时候不创建交换分区，针对已经存在交换分区的可以设置忽略禁止使用Swap的限制，不然无法启动Kubelet。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# vim /etc/sysconfig/kubelet</span></span>
<span class="line"><span style="color:#a6accd">KUBELET_CGROUP_ARGS="--cgroup-driver=systemd"</span></span>
<span class="line"><span style="color:#a6accd">KUBELET_EXTRA_ARGS="--fail-swap-on=false"</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>在所有节点上关闭SWAP</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# swapoff -a</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>4.设置内核参数</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# cat &lt;&lt;EOF &gt;  /etc/sysctl.d/k8s.conf</span></span>
<span class="line"><span style="color:#a6accd">net.bridge.bridge-nf-call-ip6tables = 1</span></span>
<span class="line"><span style="color:#a6accd">net.bridge.bridge-nf-call-iptables = 1</span></span>
<span class="line"><span style="color:#a6accd">net.ipv4.ip_forward = 1</span></span>
<span class="line"><span style="color:#a6accd">EOF</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>使配置生效</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# sysctl --system</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>5.启动kubelet并设置开机启动</strong></p><p>注意，此时kubelet是无法正常启动的，可以查看/var/log/messages有报错信息，等待执行初始化之后即可正常，为正常现象。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# systemctl enable kubelet &amp;&amp; systemctl start kubelet</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>6.使用IPVS进行负载均衡</strong></p><p>在Kubernetes集群中Kube-Proxy组件负载均衡的功能，默认使用iptables，生产环境建议使用ipvs进行负载均衡。在所有节点启用ipvs模块</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# vim /etc/sysconfig/modules/ipvs.modules</span></span>
<span class="line"><span style="color:#a6accd">#!/bin/bash</span></span>
<span class="line"><span style="color:#a6accd">modprobe -- ip_vs</span></span>
<span class="line"><span style="color:#a6accd">modprobe -- ip_vs_rr</span></span>
<span class="line"><span style="color:#a6accd">modprobe -- ip_vs_wrr</span></span>
<span class="line"><span style="color:#a6accd">modprobe -- ip_vs_sh</span></span>
<span class="line"><span style="color:#a6accd">modprobe -- nf_conntrack_ipv4</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# chmod +x /etc/sysconfig/modules/ipvs.modules</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# source /etc/sysconfig/modules/ipvs.modules</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>查看模块是否加载正常</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# lsmod | grep -e ip_vs -e nf_conntrack_ipv4</span></span>
<span class="line"><span style="color:#a6accd">ip_vs_sh               12688  0 </span></span>
<span class="line"><span style="color:#a6accd">ip_vs_wrr              12697  0 </span></span>
<span class="line"><span style="color:#a6accd">ip_vs_rr               12600  0 </span></span>
<span class="line"><span style="color:#a6accd">ip_vs                 145497  6 ip_vs_rr,ip_vs_sh,ip_vs_wrr</span></span>
<span class="line"><span style="color:#a6accd">nf_conntrack_ipv4      15053  15 </span></span>
<span class="line"><span style="color:#a6accd">nf_defrag_ipv4         12729  1 nf_conntrack_ipv4</span></span>
<span class="line"><span style="color:#a6accd">nf_conntrack          133095  7 ip_vs,nf_nat,nf_nat_ipv4,xt_conntrack,nf_nat_masquerade_ipv4,nf_conntrack_netlink,nf_conntrack_ipv4</span></span>
<span class="line"><span style="color:#a6accd">libcrc32c              12644  4 xfs,ip_vs,nf_nat,nf_conntrack</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ul><li>以上步骤请在Kubernetes的所有节点上执行，本实验环境是需要在linux-node1、linux-node2、linux-node3这三台机器上均安装Docker、kubeadm、kubelet，对于以上操作需要自动化可以参考我使用SaltStack完成的salt-kubeadm项目：<a target="_blank" rel="noreferrer" href="https://github.com/unixhot/salt-kubeadm"><!--[-->https://github.com/unixhot/salt-kubeadm<!--]--><!----></a></li></ul><h3 id="初始化集群部署master" tabindex="-1">初始化集群部署Master <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#初始化集群部署master" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>在所有节点上安装完毕后，在linux-node1这台Master节点上进行集群的初始化工作。</p><p><strong>1.导出所有默认的配置</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# kubeadm config print init-defaults &gt; kubeadm.yaml</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>上面的命令会生成一个默认配置的kubeadm配置文件，然后在此基础上进行修改即可。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# cat kubeadm.yaml </span></span>
<span class="line"><span style="color:#a6accd">apiVersion: kubeadm.k8s.io/v1beta2</span></span>
<span class="line"><span style="color:#a6accd">bootstrapTokens:</span></span>
<span class="line"><span style="color:#a6accd">- groups:</span></span>
<span class="line"><span style="color:#a6accd">  - system:bootstrappers:kubeadm:default-node-token</span></span>
<span class="line"><span style="color:#a6accd">  token: abcdef.0123456789abcdef</span></span>
<span class="line"><span style="color:#a6accd">  ttl: 24h0m0s</span></span>
<span class="line"><span style="color:#a6accd">  usages:</span></span>
<span class="line"><span style="color:#a6accd">  - signing</span></span>
<span class="line"><span style="color:#a6accd">  - authentication</span></span>
<span class="line"><span style="color:#a6accd">kind: InitConfiguration</span></span>
<span class="line"><span style="color:#a6accd">localAPIEndpoint:</span></span>
<span class="line"><span style="color:#a6accd">  advertiseAddress: 192.168.56.11  #修改为API Server的地址</span></span>
<span class="line"><span style="color:#a6accd">  bindPort: 6443</span></span>
<span class="line"><span style="color:#a6accd">nodeRegistration:</span></span>
<span class="line"><span style="color:#a6accd">  criSocket: /var/run/dockershim.sock</span></span>
<span class="line"><span style="color:#a6accd">  name: linux-node1.example.com</span></span>
<span class="line"><span style="color:#a6accd">  taints:</span></span>
<span class="line"><span style="color:#a6accd">  - effect: NoSchedule</span></span>
<span class="line"><span style="color:#a6accd">    key: node-role.kubernetes.io/master</span></span>
<span class="line"><span style="color:#a6accd">---</span></span>
<span class="line"><span style="color:#a6accd">apiServer:</span></span>
<span class="line"><span style="color:#a6accd">  timeoutForControlPlane: 4m0s</span></span>
<span class="line"><span style="color:#a6accd">apiVersion: kubeadm.k8s.io/v1beta2</span></span>
<span class="line"><span style="color:#a6accd">certificatesDir: /etc/kubernetes/pki</span></span>
<span class="line"><span style="color:#a6accd">clusterName: kubernetes</span></span>
<span class="line"><span style="color:#a6accd">controllerManager: {}</span></span>
<span class="line"><span style="color:#a6accd">dns:</span></span>
<span class="line"><span style="color:#a6accd">  type: CoreDNS</span></span>
<span class="line"><span style="color:#a6accd">etcd:</span></span>
<span class="line"><span style="color:#a6accd">  local:</span></span>
<span class="line"><span style="color:#a6accd">    dataDir: /var/lib/etcd</span></span>
<span class="line"><span style="color:#a6accd">imageRepository: registry.aliyuncs.com/google_containers  #修改为阿里云镜像仓库</span></span>
<span class="line"><span style="color:#a6accd">kind: ClusterConfiguration</span></span>
<span class="line"><span style="color:#a6accd">kubernetesVersion: v1.16.2  #修改为具体的版本</span></span>
<span class="line"><span style="color:#a6accd">networking:</span></span>
<span class="line"><span style="color:#a6accd">  dnsDomain: cluster.local</span></span>
<span class="line"><span style="color:#a6accd">  serviceSubnet: 10.1.0.0/16   #修改Service的网络</span></span>
<span class="line"><span style="color:#a6accd">  podSubnet: 10.2.0.0/16      #新增Pod的网络</span></span>
<span class="line"><span style="color:#a6accd">scheduler: {}</span></span>
<span class="line"><span style="color:#a6accd">---   #下面有增加的三行配置，用于设置Kubeproxy使用LVS</span></span>
<span class="line"><span style="color:#a6accd">apiVersion: kubeproxy.config.k8s.io/v1alpha1</span></span>
<span class="line"><span style="color:#a6accd">kind: KubeProxyConfiguration</span></span>
<span class="line"><span style="color:#a6accd">mode: ipvs</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>2. 执行初始化操作</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# kubeadm init --config kubeadm.yaml</span></span>
<span class="line"><span style="color:#a6accd">[init] Using Kubernetes version: v1.16.2</span></span>
<span class="line"><span style="color:#a6accd">[preflight] Running pre-flight checks</span></span>
<span class="line"><span style="color:#a6accd">error execution phase preflight: [preflight] Some fatal errors occurred:</span></span>
<span class="line"><span style="color:#a6accd">        [ERROR NumCPU]: the number of available CPUs 1 is less than the required 2</span></span>
<span class="line"><span style="color:#a6accd">        [ERROR Swap]: running with swap on is not supported. Please disable swa</span></span>
<span class="line"><span style="color:#a6accd">[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`</span></span>
<span class="line"><span style="color:#a6accd">To see the stack trace of this error execute with --v=5 or higher</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>如果遇到上面这样的报错，是因为在实验环境开启了交换分区，以及CPU的核数小于2造成的，可以使用--ignore-preflight-errors=进行忽略。 --ignore-preflight-errors=：忽略运行时的错误，例如上面目前存在[ERROR NumCPU]和[ERROR Swap]，忽略这两个报错就是增加--ignore-preflight-errors=NumCPU 和--ignore-preflight-errors=Swap的配置即可。</p><p>再次执行初始化操作：</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# kubeadm init --config kubeadm.yaml \</span></span>
<span class="line"><span style="color:#a6accd">  --ignore-preflight-errors=Swap,NumCPU </span></span>
<span class="line"><span style="color:#a6accd">[init] Using Kubernetes version: v1.16.2</span></span>
<span class="line"><span style="color:#a6accd">[preflight] Running pre-flight checks</span></span>
<span class="line"><span style="color:#a6accd">        [WARNING NumCPU]: the number of available CPUs 1 is less than the required 2</span></span>
<span class="line"><span style="color:#a6accd">        [WARNING Swap]: running with swap on is not supported. Please disable swap</span></span>
<span class="line"><span style="color:#a6accd">[preflight] Pulling images required for setting up a Kubernetes cluster</span></span>
<span class="line"><span style="color:#a6accd">[preflight] This might take a minute or two, depending on the speed of your internet connection</span></span>
<span class="line"><span style="color:#a6accd">[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>执行完毕后，会在当前输出下停留，等待下载Kubernetes组件的Docker镜像。根据你的网络情况，可以持续1-5分钟，你也可以使用docker images查看下载的镜像。镜像下载完毕之后，就会进行初始操作：</p><p>这里省略了所有输出，初始化操作主要经历了下面15个步骤，每个阶段均输出均使用[步骤名称]作为开头：</p><ol><li>[init]：指定版本进行初始化操作</li><li>[preflight] ：初始化前的检查和下载所需要的Docker镜像文件。</li><li>[kubelet-start]：生成kubelet的配置文件”/var/lib/kubelet/config.yaml”，没有这个文件kubelet无法启动，所以初始化之前的kubelet实际上启动失败。</li><li>[certificates]：生成Kubernetes使用的证书，存放在/etc/kubernetes/pki目录中。</li><li>[kubeconfig] ：生成 KubeConfig文件，存放在/etc/kubernetes目录中，组件之间通信需要使用对应文件。</li><li>[control-plane]：使用/etc/kubernetes/manifest目录下的YAML文件，安装 Master组件。</li><li>[etcd]：使用/etc/kubernetes/manifest/etcd.yaml安装Etcd服务。</li><li>[wait-control-plane]：等待control-plan部署的Master组件启动。</li><li>[apiclient]：检查Master组件服务状态。</li><li>[uploadconfig]：更新配置</li><li>[kubelet]：使用configMap配置kubelet。</li><li>[patchnode]：更新CNI信息到Node上，通过注释的方式记录。</li><li>[mark-control-plane]：为当前节点打标签，打了角色Master，和不可调度标签，这样默认就不会使用Master节点来运行Pod。</li><li>[bootstrap-token]：生成token记录下来，后边使用kubeadm join往集群中添加节点时会用到</li><li>[addons]：安装附加组件CoreDNS和kube-proxy</li></ol><p>成功执行之后，你会看到下面的输出：</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">Your Kubernetes master has initialized successfully!</span></span>
<span class="line"><span style="color:#a6accd">To start using your cluster, you need to run the following as a regular user:</span></span>
<span class="line"><span style="color:#a6accd">mkdir -p $HOME/.kube</span></span>
<span class="line"><span style="color:#a6accd">sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span></span>
<span class="line"><span style="color:#a6accd">sudo chown $(id -u):$(id -g) $HOME/.kube/config</span></span>
<span class="line"><span style="color:#a6accd">You should now deploy a pod network to the cluster.</span></span>
<span class="line"><span style="color:#a6accd">Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:</span></span>
<span class="line"><span style="color:#a6accd">https://kubernetes.io/docs/concepts/cluster-administration/addons/</span></span>
<span class="line"><span style="color:#a6accd">You can now join any number of machines by running the following on each node</span></span>
<span class="line"><span style="color:#a6accd">as root:</span></span>
<span class="line"><span style="color:#a6accd">kubeadm join 192.168.56.11:6443 --token 19fhhl.3mzkyk16tcgp6vga --discovery-token-ca-cert-hash sha256:76a88c38b673d3b2ac73e33127a809688cb3e58c533512ac6d92ecb66aa57a45</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>如果执行失败，那意味着之前的操作存在问题，检查顺序如下：</p><ul><li>基础环境</li><li>主机名是否可以解析，SELinux，iptables是否关闭。</li><li>交换分区是否存在free -m查看</li><li>内核参数是否修改、IPVS是否修改（目前阶段不会造成失败）</li><li>基础软件</li><li>Docker是否安装并启动</li><li>Kubelet是否安装并启动</li><li>执行kubeadm是否有别的报错是否忽略</li><li>systemctl status kubelet查看kubelet是否启动</li><li>如果kubelet无法启动，查看日志有什么报错，并解决报错。</li><li>以上都解决完毕，需要重新初始化</li><li>kubeadm reset 进行重置（生产千万不要执行，会直接删除集群）</li><li>根据kubeadm reset 提升，清楚iptables和LVS。</li></ul><p>请根据上面输出的要求配置kubectl命令来访问集群。</p><p><strong>3.为kubectl准备Kubeconfig文件。</strong></p><p>kubectl默认会在执行的用户家目录下面的.kube目录下寻找config文件。这里是将在初始化时[kubeconfig]步骤生成的admin.conf拷贝到.kube/config。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# mkdir -p $HOME/.kube</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# chown $(id -u):$(id -g) $HOME/.kube/config</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>在该配置文件中，记录了API Server的访问地址，所以后面直接执行kubectl命令就可以正常连接到API Server中。</p><p><strong>4.使用kubectl命令查看组件状态</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# kubectl get cs</span></span>
<span class="line"><span style="color:#a6accd">NAME STATUS MESSAGE ERROR</span></span>
<span class="line"><span style="color:#a6accd">scheduler Healthy ok</span></span>
<span class="line"><span style="color:#a6accd">controller-manager Healthy ok</span></span>
<span class="line"><span style="color:#a6accd">etcd-0 Healthy {"health": "true"}</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>**知识回顾：为什么上面的输出没有显示API Server组件的状态</p><p>因为API Server是Kubernetes集群的入口，所有和Kubernetes集群的交互都必须经过APIServer，kubectl命令也是连接到API Server上进行交互，所以如果能够正常使用kubectl执行命令，意味着API Server运行正常。</p><p><strong>5.使用kubectl获取Node信息</strong></p><p>目前只有一个节点，角色是Master，状态是NotReady。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# kubectl get node</span></span>
<span class="line"><span style="color:#a6accd">NAME STATUS ROLES AGE VERSION</span></span>
<span class="line"><span style="color:#a6accd">linux-node1.unixhot.com NotReady master 14m v1.16.2</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h3 id="部署网络插件" tabindex="-1">部署网络插件 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#部署网络插件" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>Master节点NotReady的原因就是因为没有使用任何的网络插件，此时Node和Master的连接还不正常。目前最流行的Kubernetes网络插件有Flannel、Calico、Canal，这里分别列举了Canal和Flannel，你可以选择其中之一进行部署。 因为基础的Kubernetes集群已经配置完毕，后面的增加组件等操作，几乎都可以使用kubectl和一个YAML配置文件来完成。</p><p>【部署canal网络插件】</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# kubectl apply -f https://docs.projectcalico.org/v3.3/getting-started/kubernetes/installation/hosted/canal/rbac.yaml</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# kubectl apply -f https://docs.projectcalico.org/v3.3/getting-started/kubernetes/installation/hosted/canal/canal.yaml</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>【部署Flannel网络插件】（推荐） 部署Flannel网络插件需要修改Pod的IP地址段，修改为和你初始化一直的网段，可以先下载Flannel的YAML文件修改后，再执行。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# git clone --depth 1 https://github.com/coreos/flannel.git</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# cd flannel/Documentation/</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 Documentation]# vim kube-flannel.yml</span></span>
<span class="line"><span style="color:#a6accd"># 修改"Network": "10.244.0.0/16"为"Network": "10.2.0.0/16",</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">74   net-conf.json: |</span></span>
<span class="line"><span style="color:#a6accd">75     {</span></span>
<span class="line"><span style="color:#a6accd">76       "Network": "10.2.0.0/16",</span></span>
<span class="line"><span style="color:#a6accd">77       "Backend": {</span></span>
<span class="line"><span style="color:#a6accd">78         "Type": "vxlan"</span></span>
<span class="line"><span style="color:#a6accd">79       }</span></span>
<span class="line"><span style="color:#a6accd">80     }</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd"># 请注意，Flannel的镜像拉取速度会比较慢，可以替换为国内镜像</span></span>
<span class="line"><span style="color:#a6accd"># image: quay.io/coreos/flannel:v0.10.0-amd64</span></span>
<span class="line"><span style="color:#a6accd">image: quay-mirror.qiniu.com/coreos/flannel:v0.11.0-amd64</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd"># 如果Node中有多个网卡，可以使用--iface来指定对应的网卡参数。</span></span>
<span class="line"><span style="color:#a6accd">containers:</span></span>
<span class="line"><span style="color:#a6accd">      - name: kube-flannel</span></span>
<span class="line"><span style="color:#a6accd">        image: quay-mirror.qiniu.com/coreos/flannel:v0.11.0-amd64</span></span>
<span class="line"><span style="color:#a6accd">        command:</span></span>
<span class="line"><span style="color:#a6accd">        - /opt/bin/flanneld</span></span>
<span class="line"><span style="color:#a6accd">        args:</span></span>
<span class="line"><span style="color:#a6accd">        - --ip-masq</span></span>
<span class="line"><span style="color:#a6accd">        - --kube-subnet-mgr</span></span>
<span class="line"><span style="color:#a6accd">        - --iface=eth0</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>部署Flannel</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 Documentation]# kubectl create -f kube-flannel.yml</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>查看Pod状态</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 Documentation]# kubectl get pod -n kube-system</span></span>
<span class="line"><span style="color:#a6accd">NAME                                              READY   STATUS     RESTARTS   AGE</span></span>
<span class="line"><span style="color:#a6accd">coredns-58cc8c89f4-cjhdv                          0/1     Pending    0          41m</span></span>
<span class="line"><span style="color:#a6accd">coredns-58cc8c89f4-vdfn2                          0/1     Pending    0          41m</span></span>
<span class="line"><span style="color:#a6accd">etcd-linux-node1.unixhot.com                      1/1     Running    0          41m</span></span>
<span class="line"><span style="color:#a6accd">kube-apiserver-linux-node1.unixhot.com            1/1     Running    0          40m</span></span>
<span class="line"><span style="color:#a6accd">kube-controller-manager-linux-node1.unixhot.com   1/1     Running    1          40m</span></span>
<span class="line"><span style="color:#a6accd">kube-flannel-ds-amd64-bwsxl                       0/1     Init:0/1   0          20s</span></span>
<span class="line"><span style="color:#a6accd">kube-proxy-5qrmh                                  1/1     Running    0          41m</span></span>
<span class="line"><span style="color:#a6accd">kube-scheduler-linux-node1.unixhot.com            1/1     Running    1          41m</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>可以看到此时CoreDNS处于Pending状态，需要等待网络插件canal或者Flannel的Pod状态变成Running之后CoreDNS也会正常。所有Pod的状态都变成Running之后，这个时候再次获取Node，会发现节点变成了Ready状态。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# kubectl get node</span></span>
<span class="line"><span style="color:#a6accd">NAME STATUS ROLES AGE VERSION</span></span>
<span class="line"><span style="color:#a6accd">linux-node1.unixhot.com Ready master 29m v1.16.2</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><em>kubeadm其实使用Kubernetes部署Kubernetes，这样就存在先有鸡还是先有蛋的问题，所以，我们首先手动部署了Docker和kubelet，然后kubeadm调用kubelet以静态Pod的方式部署了Kubernetes集群中的其它组件。静态Pod在后面的章节会讲到。</em></p><h3 id="部署node节点" tabindex="-1">部署Node节点 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#部署node节点" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>Master节点部署完毕之后，就可以部署Node节点，首先请遵循部署Docker和kubeadm章节为Node节点部署安装好docker、kubeadm和kubelet，此过程这里不再重复列出。</p><p><strong>1.在Master节点输出增加节点的命令</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# kubeadm token create --print-join-command</span></span>
<span class="line"><span style="color:#a6accd">kubeadm join 192.168.56.11:6443 --token isggqa.xjwsm3i6nex91d2x --discovery-token-ca-cert-hash sha256:718827895a9a5e63dfa9ff54e16ad6dc0c493139c9c573b67ad66968036cd569</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>2.在Node节点执行</strong></p><p>注意如果节点有交换分区，需要增加--ignore-preflight-errors=Swap。</p><p>部署linux-node2</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node2 ~]# kubeadm join 192.168.56.11:6443 --token isggqa.xjwsm3i6nex91d2x --discovery-token-ca-cert-hash sha256:718827895a9a5e63dfa9ff54e16ad6dc0c493139c9c573b67ad66968036cd569 --ignore-preflight-errors=Swap</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>部署linux-node3</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node3 ~]# kubeadm join 192.168.56.11:6443 --tokenisggqa.xjwsm3i6nex91d2x --discovery-token-ca-cert-hash sha256:718827895a9a5e63dfa9ff54e16ad6dc0c493139c9c573b67ad66968036cd569 --ignore-preflight-errors=Swap</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>这个时候kubernetes会使用DaemonSet在所有节点上都部署canal/flannel和kube-proxy。部署完毕之后节点即部署完毕。DaemonSet的内容后面会讲解。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">    [root@linux-node1 ~]# kubectl get daemonset --all-namespaces</span></span>
<span class="line"><span style="color:#a6accd">    NAMESPACE NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE</span></span>
<span class="line"><span style="color:#a6accd">    kube-system canal 2 2 1 2 1 beta.kubernetes.io/os=linux 17m</span></span>
<span class="line"><span style="color:#a6accd">    kube-system kube-proxy 2 2 2 2 2 &lt;none&gt; 47m</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>待所有Pod全部启动完毕之后，节点就恢复Ready状态。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">    [root@linux-node1 ~]# kubectl get pod --all-namespaces</span></span>
<span class="line"><span style="color:#a6accd">    NAMESPACE NAME READY STATUS RESTARTS AGE</span></span>
<span class="line"><span style="color:#a6accd">    kube-system canal-lv92w 3/3 Running 0 8m45s</span></span>
<span class="line"><span style="color:#a6accd">    kube-system canal-rq5n5 3/3 Running 0 23m</span></span>
<span class="line"><span style="color:#a6accd">    kube-system coredns-78d4cf999f-5k4sg 1/1 Running 0 53m</span></span>
<span class="line"><span style="color:#a6accd">    kube-system coredns-78d4cf999f-bnbgf 1/1 Running 0 53m</span></span>
<span class="line"><span style="color:#a6accd">    kube-system etcd-linux-node1.linuxhot.com 1/1 Running 0 52m</span></span>
<span class="line"><span style="color:#a6accd">    kube-system kube-apiserver-linux-node1.linuxhot.com 1/1 Running 0 52m</span></span>
<span class="line"><span style="color:#a6accd">    kube-system kube-controller-manager-linux-node1.linuxhot.com 1/1 Running 0 52m</span></span>
<span class="line"><span style="color:#a6accd">    kube-system kube-proxy-sddlp 1/1 Running 0 53m</span></span>
<span class="line"><span style="color:#a6accd">    kube-system kube-proxy-tw96b 1/1 Running 0 8m45s</span></span>
<span class="line"><span style="color:#a6accd">    kube-system kube-scheduler-linux-node1.linuxhot.com 1/1 Running 0 52m</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>查看所有节点</strong> `` [root@linux-node1 ~]# kubectl get node NAME STATUS ROLES AGE VERSION <a target="_blank" rel="noreferrer" href="http://linux-node1.linuxhot.com"><!--[-->linux-node1.linuxhot.com<!--]--><!----></a> Ready master 49m v1.13.2 <a target="_blank" rel="noreferrer" href="http://linux-node2.linuxhot.com"><!--[-->linux-node2.linuxhot.com<!--]--><!----></a> Ready 4m48s v1.13.2</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">**如何给Node加上Roles标签**</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">使用kubectl get node能够看到linux-node1.linuxhot.com的ROLES是master这个是在进行集群初始化的时候[mark-control-plane]进行标记的。</span></span>
<span class="line"><span style="color:#a6accd">[mark-control-plane] Marking the node linux-node1.linuxhot.com as control-plane</span></span>
<span class="line"><span style="color:#a6accd">by adding the label "node-role.kubernetes.io/master=''"</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">[mark-control-plane] Marking the node linux-node1.linuxhot.com as control-plane</span></span>
<span class="line"><span style="color:#a6accd">by adding the taints [node-role.kubernetes.io/master:NoSchedule]</span></span>
<span class="line"><span style="color:#a6accd">1.查看节点的标签</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# kubectl get nodes --show-labels</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">NAME STATUS ROLES AGE VERSION LABELS</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">linux-node1.linuxhot.com Ready master 48m v1.13.3</span></span>
<span class="line"><span style="color:#a6accd">beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=linux-node1.linuxhot.com,node-role.kubernetes.io/master=</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">linux-node2.linuxhot.com Ready &lt;none&gt; 7m13s v1.13.3</span></span>
<span class="line"><span style="color:#a6accd">beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=linux-node2.linuxhot.com</span></span>
<span class="line"><span style="color:#a6accd">2.增加标签</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# kubectl label nodes linux-node2.linuxhot.com</span></span>
<span class="line"><span style="color:#a6accd">node-role.kubernetes.io/node=</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">node/linux-node2.linuxhot.com labeled</span></span>
<span class="line"><span style="color:#a6accd">3.查看效果</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# kubectl get nodes</span></span>
<span class="line"><span style="color:#a6accd">NAME STATUS ROLES AGE VERSION</span></span>
<span class="line"><span style="color:#a6accd">linux-node1.linuxhot.com Ready master 50m v1.13.3</span></span>
<span class="line"><span style="color:#a6accd">linux-node2.linuxhot.com Ready node 8m41s v1.13.3</span></span>
<span class="line"><span style="color:#a6accd">### 测试Kubernetes集群 {#test}</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">在上面的步骤中，我们创建了一个Kubernetes集群，1个Master和2个Node节点，在生产环境需要考虑Master的高可用，这里先不用考虑，后面会讲到。</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">**1.创建一个单Pod的Nginx应用**</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# kubectl create deployment nginx --image=nginx:alpine</span></span>
<span class="line"><span style="color:#a6accd">deployment.apps/nginx created</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# kubectl get pod</span></span>
<span class="line"><span style="color:#a6accd">NAME READY STATUS RESTARTS AGE</span></span>
<span class="line"><span style="color:#a6accd">nginx-54458cd494-9j7ql 0/1 ContainerCreating 0 10s</span></span>
<span class="line"><span style="color:#a6accd">**2.查看Pod详细信息**</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">待Pod的状态为Running后，可以获取Pod的IP地址，这个IP地址是从Master节点初始化的--pod-network-cidr=10.2.0.0/16地址段中分配的。</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# kubectl get pod -o wide</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">nginx-54458cd494-9j7ql 1/1 Running 0 59s 10.2.1.2 linux-node2.linuxhot.com</span></span>
<span class="line"><span style="color:#a6accd">&lt;none&gt; &lt;none&gt;</span></span>
<span class="line"><span style="color:#a6accd">**3.测试Nginx访问**</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# curl --head http://10.2.1.2</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">HTTP/1.1 200 OK</span></span>
<span class="line"><span style="color:#a6accd">Server: nginx/1.15.8</span></span>
<span class="line"><span style="color:#a6accd">Date: Sun, 13 Jan 2019 01:16:36 GMT</span></span>
<span class="line"><span style="color:#a6accd">Content-Type: text/html</span></span>
<span class="line"><span style="color:#a6accd">Content-Length: 612</span></span>
<span class="line"><span style="color:#a6accd">Last-Modified: Wed, 26 Dec 2018 23:21:49 GMT</span></span>
<span class="line"><span style="color:#a6accd">Connection: keep-alive</span></span>
<span class="line"><span style="color:#a6accd">ETag: "5c240d0d-264"</span></span>
<span class="line"><span style="color:#a6accd">Accept-Ranges: bytes</span></span>
<span class="line"><span style="color:#a6accd">**4.测试扩容**</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">现在将Nginx应用的Pod副本数量拓展到2个节点</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# kubectl scale deployment nginx --replicas=2</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">deployment.extensions/nginx scaled</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# kubectl get pod</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">NAME READY STATUS RESTARTS AGE</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">nginx-54458cd494-9j7ql 1/1 Running 0 2m13s</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">nginx-54458cd494-vnm4f 1/1 Running 0 5s</span></span>
<span class="line"><span style="color:#a6accd">**5.为Nginx增加Service**</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">为Nginx增加Service，会创建一个Cluster</span></span>
<span class="line"><span style="color:#a6accd">IP，从Master初始化的--service-cidr=10.1.0.0/16地址段中进行分配，</span></span>
<span class="line"><span style="color:#a6accd">并开启NodePort是在Node节点上进行端口映射，进行外部访问。</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# kubectl expose deployment nginx --port=80</span></span>
<span class="line"><span style="color:#a6accd">--type=NodePort</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">service/nginx exposed</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# kubectl get service</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE</span></span>
<span class="line"><span style="color:#a6accd">kubernetes ClusterIP 10.1.0.1 &lt;none&gt; 443/TCP 88m</span></span>
<span class="line"><span style="color:#a6accd">nginx NodePort 10.1.147.204 &lt;none&gt; 80:30599/TCP 67m</span></span>
<span class="line"><span style="color:#a6accd">**6.测试Service的VIP**</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# curl --head http://10.1.147.204/</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">HTTP/1.1 200 OK </span></span>
<span class="line"><span style="color:#a6accd">Server: nginx/1.15.8</span></span>
<span class="line"><span style="color:#a6accd">Date: Sun, 13 Jan 2019 01:26:21 GMT</span></span>
<span class="line"><span style="color:#a6accd">Content-Type: text/html</span></span>
<span class="line"><span style="color:#a6accd">Content-Length: 612</span></span>
<span class="line"><span style="color:#a6accd">Last-Modified: Wed, 26 Dec 2018 23:21:49 GMT</span></span>
<span class="line"><span style="color:#a6accd">Connection: keep-alive</span></span>
<span class="line"><span style="color:#a6accd">ETag: "5c240d0d-264"</span></span>
<span class="line"><span style="color:#a6accd">Accept-Ranges: bytes</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>``` <strong>7.测试NodePort，外部访问。</strong></p><p><img src="http://k8s.unixhot.com/kubernetes/media/1f9d523f359ce6d49515d04703d8e941.png" alt="img"></p><p>这一切看起来似乎不是十分完美，但是现在你已经拥有了一个Kubernetes集群，接下来就可以继续探索Kubernetes的世界了。</p><h2 id="公有云中的kubernetes" tabindex="-1">公有云中的Kubernetes <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#公有云中的kubernetes" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h2><p>截止2019年2月，大多数公有云已经提供了容器Kubernetes的产品服务，对于使用公有云的用户来说，最佳实践是直接购买公有云产品，而非自己部署Kubernetes集群，主要是因为公有云已经将网络和存储与Kubernetes集成好了，解决了生产应用的难题。</p><h3 id="阿里云中的kubernetes" tabindex="-1">阿里云中的Kubernetes <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#阿里云中的kubernetes" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>国内阿里云提供了容器服务 Kubernetes 版（简称 ACK）提供高性能可伸缩的容器应用管理能力，支持企业级 Kubernetes 容器化应用的全生命周期管理。容器服务 Kubernetes 版简化集群的搭建和扩容等工作，整合阿里云虚拟化、存储、网络和安全能力，打造云端最佳的 Kubernetes 容器化应用运行环境。</p><p><strong>阿里云Kubernetes模式</strong></p><p>容器服务Kubernetes版包含了经典Dedicated Kubernetes以及Serverless两种形态，方便您按需选择。</p><ul><li>经典Dedicated Kubernetes模式：您可以对集群基础设施和容器应用进行更细粒度的控制，比如选择宿主机实例规格和操作系统，指定Kubernetes 版本、自定义 Kubernetes 特性开关设置等。阿里云 Kubernetes 服务负责为集群创建底层云资源，升级等自动化运维操作。而您需要规划、维护、升级服务器集群，手动或自动在集群中添加或删除服务器。</li><li>Serverless 模式：您无需创建底层虚拟化资源，可以利用 Kubernetes 命令指明应用容器镜像、CPU 和内存要求以及对外服务方式，直接启动。</li></ul><p><strong>阿里云Kubernetes产品架构</strong></p><p><img src="http://k8s.unixhot.com/kubernetes/media/43688d33a81f2bd6af354d0715b6b297.png" alt="C:UsersjasonDesktop15447553537457_zh-CN.png"></p><p><strong>阿里云Kubernetes创建</strong></p><p>默认情况下可以在阿里云中自行创建5个集群，每个集群最多可以添加 40 个节点。如需更高配额，需要提交工单申请。</p><p><img src="http://k8s.unixhot.com/kubernetes/media/4c165d0ebc842862595f9fd0f4650d43.png" alt="img"></p><p>阿里云将网络、存储、安全等方面已经进行了深度集成，在创建Kubernetes集群时，阿里云容器服务提供两种网络插件：Terway和Flannel：</p><ul><li>Flannel：使用的是简单稳定的社区的Flannel CNI插件，配合阿里云的VPC的高速网络，能给集群高性能和稳定的容器网络体验，但功能偏简单，支持的特性少，例如：不支持基于Kubernetes标准的Network Policy。</li><li>Terway：是阿里云容器服务自研的网络插件，功能上完全兼容Flannel，支持将阿里云的弹性网卡分配给容器，支持基于Kubernetes标准的NetworkPolicy来定义容器间的访问策略，支持对单个容器做带宽的限流。对于不需要使用Network Policy的用户，可以选择Flannel，其他情况建议选择Terway。了解更多Terway网络插件的相关内容，请参见Terway网络插件。</li></ul><p>最终阿里云会使用kubeadm帮你创建一个指定版本的Kubernetes集群。</p><h3 id="私有云中的kubernetes" tabindex="-1">私有云中的Kubernetes <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#私有云中的kubernetes" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p><strong>内网部署Kubernetes</strong></p><p>很多企业需要内网部署Kubernetes但是内网又无法访问外网，就需要本地化部署，无忘了本地话部署主要包括两个方面，一个是软件仓库的准备，一个是Kubernetes镜像的准备。</p><ol><li>准备内网YUM仓库（略）</li><li>准备Docker Registry（请参考Harbor章节）</li><li>下载并提交镜像到Harbor中，然后将Harbor的镜像部署直接COPY到生产环境中</li></ol><p>下载脚本如下，请根据需求自行修改：</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# vim k8s-images.sh</span></span>
<span class="line"><span style="color:#a6accd">#!/bin/bash</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd"># EVN</span></span>
<span class="line"><span style="color:#a6accd">ALIYUN_REG="registry.aliyuncs.com/google_containers"</span></span>
<span class="line"><span style="color:#a6accd">FLANNEL_REG="quay-mirror.qiniu.com/coreos"</span></span>
<span class="line"><span style="color:#a6accd">LOCAL_REG="192.168.56.11/kubernetes"</span></span>
<span class="line"><span style="color:#a6accd">K8S_VER=v1.15.5</span></span>
<span class="line"><span style="color:#a6accd">PAUSE_VER=3.1</span></span>
<span class="line"><span style="color:#a6accd">ETCD_VER=3.3.10</span></span>
<span class="line"><span style="color:#a6accd">COREDNS_VER=1.3.1</span></span>
<span class="line"><span style="color:#a6accd">FLANNEL_VER=v0.11.0-amd64</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd"># Kubernetes Docker Images</span></span>
<span class="line"><span style="color:#a6accd">IMAGES=(</span></span>
<span class="line"><span style="color:#a6accd">  kube-proxy:${K8S_VER}</span></span>
<span class="line"><span style="color:#a6accd">  kube-scheduler:${K8S_VER}</span></span>
<span class="line"><span style="color:#a6accd">  kube-controller-manager:${K8S_VER}</span></span>
<span class="line"><span style="color:#a6accd">  kube-apiserver:${K8S_VER}</span></span>
<span class="line"><span style="color:#a6accd">  pause:${PAUSE_VER}</span></span>
<span class="line"><span style="color:#a6accd">  etcd-amd64:${ETCD_VER}</span></span>
<span class="line"><span style="color:#a6accd">  coredns:${COREDNS_VER}</span></span>
<span class="line"><span style="color:#a6accd">)</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">for IMAGE in ${IMAGES[@]}</span></span>
<span class="line"><span style="color:#a6accd">do</span></span>
<span class="line"><span style="color:#a6accd">  docker pull ${ALIYUN_REG}/${IMAGE}</span></span>
<span class="line"><span style="color:#a6accd">  docker tag ${ALIYUN_REG}/${IMAGE} ${LOCAL_REG}/${IMAGE}</span></span>
<span class="line"><span style="color:#a6accd">  #docker push ${LOCAL_REG}/${IMAGES}</span></span>
<span class="line"><span style="color:#a6accd">done</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd"># Flannel</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">docker pull ${FLANNEL_REG}/flannel:${FLANNEL_VER}</span></span>
<span class="line"><span style="color:#a6accd">docker tag ${FLANNEL_REG}/flannel:${FLANNEL_VER} ${LOCAL_REG}/flannel:${FLANNEL_VER}</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h1 id="第五部分-将应用迁移至kubernetes" tabindex="-1">第五部分 将应用迁移至Kubernetes <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#第五部分-将应用迁移至kubernetes" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h1><h1 id="_6-第一步：将应用封装进容器中" tabindex="-1">6 第一步：将应用封装进容器中 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_6-第一步：将应用封装进容器中" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h1><h2 id="第一步：将应用封装进容器中" tabindex="-1">第一步：将应用封装进容器中 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#第一步：将应用封装进容器中" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h2><p>在之前的容器镜像实战中，我们已经学习了如何将应用容器化，这里我们将下载使用两个官方的Nginx镜像来完成接下来的实验。</p><h3 id="部署harbor镜像仓库" tabindex="-1">部署Harbor镜像仓库 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#部署harbor镜像仓库" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>生产环境中可以使用Harbor来管理Docker镜像，请参考之前章节的内容完成Harbor镜像仓库的部署工作，并在Harbor中创建一个devopsedu的项目。</p><p><img src="http://k8s.unixhot.com/kubernetes/media/093b4b731c05b54a1b8426e924f93d45.png" alt="img"></p><h3 id="制作实验用的docker镜像" tabindex="-1">制作实验用的Docker镜像 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#制作实验用的docker镜像" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>这里不再演示Docker镜像的构建，直接下载两个官方镜像作为案例。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# docker pull nginx:1.13.12</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# docker pull nginx:1.14.0</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h3 id="配置docker仓库" tabindex="-1">配置Docker仓库 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#配置docker仓库" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root\@linux-node1 ~]# vim /etc/docker/daemon.json</span></span>
<span class="line"><span style="color:#a6accd">{</span></span>
<span class="line"><span style="color:#a6accd">    "registry-mirrors": ["https://tdimi5q1.mirror.aliyuncs.com"],</span></span>
<span class="line"><span style="color:#a6accd">    "insecure-registries" : ["http://192.168.56.11"]</span></span>
<span class="line"><span style="color:#a6accd">}</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# systemctl restart docker</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h3 id="登录harbor镜像仓库" tabindex="-1">登录Harbor镜像仓库 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#登录harbor镜像仓库" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# docker login 192.168.56.11</span></span>
<span class="line"><span style="color:#a6accd">Username: admin</span></span>
<span class="line"><span style="color:#a6accd">Password:</span></span>
<span class="line"><span style="color:#a6accd">WARNING! Your password will be stored unencrypted in /root/.docker/config.json.</span></span>
<span class="line"><span style="color:#a6accd">    Configure a credential helper to remove this warning. See</span></span>
<span class="line"><span style="color:#a6accd">    https://docs.docker.com/engine/reference/commandline/login/\#credentials-store</span></span>
<span class="line"><span style="color:#a6accd">    Login Succeeded</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h3 id="提交镜像到registry" tabindex="-1">提交镜像到Registry <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#提交镜像到registry" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# docker tag nginx:1.13.12</span></span>
<span class="line"><span style="color:#a6accd">192.168.56.11/devopsedu/nginx:1.13.12</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# docker tag nginx:1.14.0</span></span>
<span class="line"><span style="color:#a6accd">192.168.56.11/devopsedu/nginx:1.14.0</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# docker push 192.168.56.11/devopsedu/nginx:1.13.12</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# docker push 192.168.56.11/devopsedu/nginx:1.14.0</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>在上面的步骤中，模拟了生产环境如何构建和提交镜像，如果对于构建和提交镜像有疑问可以复习第3章的内容。</p><h1 id="_7-第二步：将容器封装到pod中" tabindex="-1">7 第二步：将容器封装到Pod中 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_7-第二步：将容器封装到pod中" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h1><p>Pod是Kubernetes最小的管理单元，一个Pod可以代表一个运行在集群里的进程。之前是在宿主机上运行不同的进程，现在是运行不同的Pod。前面介绍过Pod是一个逻辑架构的组件，Pod里封装了一个（或者多个）应用容器，存储资源和IP地址。</p><p><strong>为什么要造一个Pod出来？</strong> 学习Kubernetes遇到的最多的名称可能就是Pod了，其它开源的容器管理平台例如Mesos直接管理和调度的是容器，但是Kubernetes确是Pod，它在容器上面做了一层封装，方便用户将一组紧耦合的容器，放置在一个共享资源的单元中。对于很多没有此类场景的初学者，可以暂时将Pod看做是容器的一个壳，你也完全可以只在Pod中运行一个容器，随着学习的深入再慢慢理解。 Kubernetes运行Pod的两种方式：</p><ul><li>Pod里只运行一个单独容器，是Kubernetes最常见的使用场景；在这种情况下，可以把Pod看做是一个单独容器的连接器，Kubernetes通过Pod去管理容器，作为使用者几乎不用关心容器。</li><li>Pod里运行多个有关系容器。例如如果使用Nginx+Tomcat运行Java应用，可以制作一个镜像里面包含了Nginx+Tomcat，也可以分别制作两个镜像Nginx镜像和Tomcat镜像，如果使用Kubernetes就需要使用Pod，如果将Nginx和Tomcat单独放在两个Pod里面来管理，就会面临很多很多问题。这个时候，就可以把这两个容器放置在一个Pod中。</li></ul><h2 id="pod管理" tabindex="-1">Pod管理 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#pod管理" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h2><p>在Kubernetes中使用YAML格式来描述一个Pod。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# vim nginx-pod.yaml </span></span>
<span class="line"><span style="color:#a6accd">apiVersion: v1</span></span>
<span class="line"><span style="color:#a6accd">kind: Pod</span></span>
<span class="line"><span style="color:#a6accd">metadata:</span></span>
<span class="line"><span style="color:#a6accd">  name: nginx-pod</span></span>
<span class="line"><span style="color:#a6accd">  labels:</span></span>
<span class="line"><span style="color:#a6accd">    app: nginx</span></span>
<span class="line"><span style="color:#a6accd">spec:</span></span>
<span class="line"><span style="color:#a6accd">  containers:</span></span>
<span class="line"><span style="color:#a6accd">  - name: nginx</span></span>
<span class="line"><span style="color:#a6accd">    image: nginx:1.13.12</span></span>
<span class="line"><span style="color:#a6accd">    ports:</span></span>
<span class="line"><span style="color:#a6accd">- containerPort: 80</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>Pod的YAML描述内容还有很多，在使用kubeadm部署Kubernetes的时候，就是使用静态Pod的方式运行的相关服务，YAML文件存放在，当然现在很多配置还是看不懂的，带着问题继续学习。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# ls -l /etc/kubernetes/manifests/</span></span>
<span class="line"><span style="color:#a6accd">total 16</span></span>
<span class="line"><span style="color:#a6accd">-rw------- 1 root root 2041 Feb 11 20:33 etcd.yaml</span></span>
<span class="line"><span style="color:#a6accd">-rw------- 1 root root 2700 Feb 11 20:33 kube-apiserver.yaml</span></span>
<span class="line"><span style="color:#a6accd">-rw------- 1 root root 2345 Feb 11 20:33 kube-controller-manager.yaml</span></span>
<span class="line"><span style="color:#a6accd">-rw------- 1 root root 1080 Feb 11 20:33 kube-scheduler.yaml</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>创建Pod</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# kubectl create -f nginx-pod.yaml </span></span>
<span class="line"><span style="color:#a6accd">pod "nginx-pod" created</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>查看Pod</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# kubectl get pod</span></span>
<span class="line"><span style="color:#a6accd">NAME                                READY     STATUS    RESTARTS   AGE</span></span>
<span class="line"><span style="color:#a6accd">nginx-pod                           1/1       Running   0          49s</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>查看Pod更多信息</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# kubectl get pod -o wide</span></span>
<span class="line"><span style="color:#a6accd">NAME                                READY     STATUS    RESTARTS   AGE       IP           NODE</span></span>
<span class="line"><span style="color:#a6accd">nginx-pod  1/1       Running   0          1m        10.2.53.18   192.168.56.13</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>查看Pod详情</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# kubectl describe pod nginx-pod</span></span>
<span class="line"><span style="color:#a6accd">Name:         nginx-pod</span></span>
<span class="line"><span style="color:#a6accd">Namespace:    default</span></span>
<span class="line"><span style="color:#a6accd">Node:         192.168.56.13/192.168.56.13</span></span>
<span class="line"><span style="color:#a6accd">Start Time:   Sat, 02 Jun 2018 06:42:53 +0800</span></span>
<span class="line"><span style="color:#a6accd">Labels:       app=nginx</span></span>
<span class="line"><span style="color:#a6accd">Annotations:  &lt;none&gt;</span></span>
<span class="line"><span style="color:#a6accd">Status:       Running</span></span>
<span class="line"><span style="color:#a6accd">IP:           10.2.53.18</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>查看Pod日志</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# kubectl logs pod/nginx-pod</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>Pod中的镜像拉取策略</strong> 当kubelet尝试拉取指定的镜像时，[imagePullPolicy]和镜像的标签会生效。</p><ul><li>imagePullPolicy: IfNotPresent：仅当镜像在本地不存在时镜像才被拉取。</li><li>imagePullPolicy: Always：每次启动 pod 的时候都会拉取镜像。</li></ul><p>省略imagePullPolicy，镜像标签为:latest或被省略，Always被应用。 imagePullPolicy被省略，并且镜像的标签被指定且不是:latest，IfNotPresent被应用。 imagePullPolicy: Never：镜像被假设存在于本地。 没有尝试拉取镜像。</p><h1 id="_7-3-第三步：使用controllers管理pod" tabindex="-1">7.3 第三步：使用Controllers管理Pod <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_7-3-第三步：使用controllers管理pod" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h1><p>在实际的生产环境中，我们其实很少单独创建Pod，而是通过控制器来进行Pod的管理，Kubernetes提供了很多的控制器，一个 Controllers 可以创建和管理很多个 Pod, 也提供复制、初始化，以及提供集群范围的自我恢复的功能。比如说： 如果一个节点宕机，Controller 将调度一个在其他节点上完全相同的 pod 来自动取代当前的 pod。</p><h1 id="_8-1-replication-controller控制器" tabindex="-1">8.1 Replication Controller控制器 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_8-1-replication-controller控制器" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h1><h1 id="_8-2-replica-sets控制器" tabindex="-1">8.2 Replica Sets控制器 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_8-2-replica-sets控制器" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h1><h1 id="_8-3-deployment控制器" tabindex="-1">8.3 Deployment控制器 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_8-3-deployment控制器" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h1><h1 id="_8-4-daemonset控制器" tabindex="-1">8.4 DaemonSet控制器 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_8-4-daemonset控制器" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h1><p>DaemonSet 确保全部（或者一些）Node 上运行一个 Pod 的副本。当有 Node 加入集群时，也会为他们新增一个 Pod 。当有 Node 从集群移除时，这些 Pod 也会被回收。删除 DaemonSet 将会删除它创建的所有 Pod。 使用 DaemonSet 的一些典型用法：</p><ul><li>运行集群存储 daemon，例如在每个 Node 上运行 glusterd、ceph。</li><li>在每个 Node 上运行日志收集 daemon，例如filebeat、logstash。</li><li>在每个 Node 上运行监控 daemon，例如 Prometheus Node Exporter、collectd、Datadog 代理、Zabbix Agent。</li></ul><p><strong>创建DaemonSet</strong></p><p>DaemonSet的描述文件和Deployment非常相似，只需要修改Kind，并去掉副本数量的配置即可</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 example]# vim nginx-daemonset.yaml</span></span>
<span class="line"><span style="color:#a6accd">apiVersion: apps/v1</span></span>
<span class="line"><span style="color:#a6accd">kind: DaemonSet</span></span>
<span class="line"><span style="color:#a6accd">metadata:</span></span>
<span class="line"><span style="color:#a6accd">  name: nginx-daemonset</span></span>
<span class="line"><span style="color:#a6accd">  labels:</span></span>
<span class="line"><span style="color:#a6accd">    app: nginx</span></span>
<span class="line"><span style="color:#a6accd">spec:</span></span>
<span class="line"><span style="color:#a6accd">  selector:</span></span>
<span class="line"><span style="color:#a6accd">    matchLabels:</span></span>
<span class="line"><span style="color:#a6accd">      app: nginx</span></span>
<span class="line"><span style="color:#a6accd">  template:</span></span>
<span class="line"><span style="color:#a6accd">    metadata:</span></span>
<span class="line"><span style="color:#a6accd">      labels:</span></span>
<span class="line"><span style="color:#a6accd">        app: nginx</span></span>
<span class="line"><span style="color:#a6accd">    spec:</span></span>
<span class="line"><span style="color:#a6accd">      containers:</span></span>
<span class="line"><span style="color:#a6accd">      - name: nginx</span></span>
<span class="line"><span style="color:#a6accd">        image: nginx:1.13.12</span></span>
<span class="line"><span style="color:#a6accd">        ports:</span></span>
<span class="line"><span style="color:#a6accd">        - containerPort: 80</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>查看Pod在Node上的分布</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# kubectl get pod -o wide</span></span>
<span class="line"><span style="color:#a6accd">NAME                    READY     STATUS    RESTARTS   AGE       IP           NODE</span></span>
<span class="line"><span style="color:#a6accd">nginx-daemonset-hk28q   1/1       Running   0          1m        10.2.56.10   192.168.56.12</span></span>
<span class="line"><span style="color:#a6accd">nginx-daemonset-wtt68   1/1       Running   0          1m        10.2.53.10   192.168.56.13</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# kubectl get daemonset</span></span>
<span class="line"><span style="color:#a6accd">NAME              DESIRED   CURRENT   READY     UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE</span></span>
<span class="line"><span style="color:#a6accd">nginx-daemonset   2         2         2         2            2           &lt;none&gt;          1m</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h1 id="_7-4-第四步：使用service管理pod访问" tabindex="-1">7.4 第四步：使用Service管理Pod访问 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_7-4-第四步：使用service管理pod访问" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h1><p>在上面的小节，我们通过Deployment可以为一个应用创建多个Pod，而且可以动态的进行增加、或者删除多余的Pod，Kubernetes Pod 是有生命周期的，它们可以被创建，也可以被销毁，但是每次Pod的IP地址就会发生变化，外面如何访问到该Pod呢？总不能每次Pod重启就修改访问的IP地址吧。 每个 Pod 都会获取它自己的 IP 地址，但是每次即使这些 IP 地址不总是稳定可依赖的。 这会导致一个问题：在 Kubernetes 集群中，如果一组 Pod（称为 backend）为其它 Pod （称为 frontend）提供服务，那么那些 frontend 该如何发现，并连接到这组 Pod 中的哪些 backend 呢？</p><h1 id="_9-1-service介绍和管理" tabindex="-1">9.1 Service介绍和管理 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_9-1-service介绍和管理" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h1><h3 id="创建service" tabindex="-1">创建Service <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#创建service" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>继续我们Nginx的案例，我们为之前的应用创建一个Service</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# cat nginx-service.yaml </span></span>
<span class="line"><span style="color:#a6accd">kind: Service</span></span>
<span class="line"><span style="color:#a6accd">apiVersion: v1</span></span>
<span class="line"><span style="color:#a6accd">metadata:</span></span>
<span class="line"><span style="color:#a6accd">  name: nginx-service</span></span>
<span class="line"><span style="color:#a6accd">spec:</span></span>
<span class="line"><span style="color:#a6accd">  selector:</span></span>
<span class="line"><span style="color:#a6accd">    app: nginx</span></span>
<span class="line"><span style="color:#a6accd">  ports:</span></span>
<span class="line"><span style="color:#a6accd">  - protocol: TCP</span></span>
<span class="line"><span style="color:#a6accd">    port: 80</span></span>
<span class="line"><span style="color:#a6accd">    targetPort: 80</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ul><li>第1行：定义资源类型为Service</li><li>第2行：定义当前Service API的版本</li><li>第3行：metadata设置</li><li>第4行：设置Service的名称为nginx-service</li><li>第5行：spec: 开始设置Service的内容</li><li>第6行：selector: 为该Service指定一个匹配的标签</li><li>第7行：app: nginx 所有带有标签app ：nginx的Pod将使用该Service</li><li>第8行：ports: 指定Service需要对外的端口</li><li>第9行：设置端口协议：支持TCP和UDP</li><li>第10行：设置Service的端口</li><li>第11行：设置Pod的端口，Kubernetes会将发送给Service端口的连接，转发到Pod的端口上。</li></ul><p>创建Nginx Service</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# kubectl create -f nginx-service.yaml </span></span>
<span class="line"><span style="color:#a6accd">service "nginx-service" created</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>查看Nginx Service</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# kubectl get service</span></span>
<span class="line"><span style="color:#a6accd">NAME            TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)   AGE</span></span>
<span class="line"><span style="color:#a6accd">kubernetes      ClusterIP   10.1.0.1      &lt;none&gt;        443/TCP   7h</span></span>
<span class="line"><span style="color:#a6accd">nginx-service   ClusterIP   10.1.184.53   &lt;none&gt;        80/TCP    25s</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>访问Servce IP</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# curl --head 10.1.181.45</span></span>
<span class="line"><span style="color:#a6accd">HTTP/1.1 200 OK</span></span>
<span class="line"><span style="color:#a6accd">Server: nginx/1.11.10</span></span>
<span class="line"><span style="color:#a6accd">Date: Tue, 21 Feb 2017 08:20:42 GMT</span></span>
<span class="line"><span style="color:#a6accd">Content-Type: text/html</span></span>
<span class="line"><span style="color:#a6accd">Content-Length: 612</span></span>
<span class="line"><span style="color:#a6accd">Last-Modified: Tue, 14 Feb 2017 15:36:04 GMT</span></span>
<span class="line"><span style="color:#a6accd">Connection: keep-alive</span></span>
<span class="line"><span style="color:#a6accd">ETag: "58a323e4-264"</span></span>
<span class="line"><span style="color:#a6accd">Accept-Ranges: bytes</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h1 id="_9-2-service和endpoint" tabindex="-1">9.2 Service和Endpoint <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_9-2-service和endpoint" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h1><p>Service作为Kubernetes中为Pod实现负载均衡的组件，几乎在所有的文章中为了方便初学者理解，基本上说的是Service会来监听Pod的变化，然后来更新Pod的IP地址。其实这个事情不是Service干的，而是有一个幕后英雄：Endpoint Endpoints表示了一个Service对应的所有Pod副本的访问地址，而Endpoints Controller负责生成和维护所有Endpoints对象的控制器。它负责监听Service和对应的Pod副本的变化。</p><ul><li>如果监测到Service被删除，则删除和该Service同名的Endpoints对象；</li><li>如果监测到新的Service被创建或修改，则根据该Service信息获得相关的Pod列表，然后创建或更新Service对应的Endpoints对象。</li><li>如果监测到Pod的事件，则更新它对应的Service的Endpoints对象。</li></ul><p>kube-proxy进程获取每个Service的Endpoints，实现Service的负载均衡功能。</p><h3 id="创建一个headless-service" tabindex="-1">创建一个Headless Service <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#创建一个headless-service" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>编写一个Service不使用clusterip</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# cat mysql-service.yaml </span></span>
<span class="line"><span style="color:#a6accd">kind: Service</span></span>
<span class="line"><span style="color:#a6accd">apiVersion: v1</span></span>
<span class="line"><span style="color:#a6accd">metadata:</span></span>
<span class="line"><span style="color:#a6accd">  name: mysql-service</span></span>
<span class="line"><span style="color:#a6accd">spec:</span></span>
<span class="line"><span style="color:#a6accd">  ports:</span></span>
<span class="line"><span style="color:#a6accd">  - protocol: TCP</span></span>
<span class="line"><span style="color:#a6accd">    port: 3306</span></span>
<span class="line"><span style="color:#a6accd">    targetPort: 3306</span></span>
<span class="line"><span style="color:#a6accd">  clusterIP: None</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# kubectl create -f mysql-service.yaml </span></span>
<span class="line"><span style="color:#a6accd">service "mysql-service" created</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>查看Service，可以放心CLUSTER-IP为None</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# kubectl get service mysql-service</span></span>
<span class="line"><span style="color:#a6accd">NAME            TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)    AGE</span></span>
<span class="line"><span style="color:#a6accd">mysql-service   ClusterIP   None         &lt;none&gt;        3306/TCP   17s</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>2.创建一个Endpoint</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# vim mysql-endpoint.yaml </span></span>
<span class="line"><span style="color:#a6accd">apiVersion: v1</span></span>
<span class="line"><span style="color:#a6accd">kind: Endpoints</span></span>
<span class="line"><span style="color:#a6accd">metadata:</span></span>
<span class="line"><span style="color:#a6accd"> name: mysql-service</span></span>
<span class="line"><span style="color:#a6accd">subsets:</span></span>
<span class="line"><span style="color:#a6accd">- addresses:</span></span>
<span class="line"><span style="color:#a6accd">  - ip: 192.168.56.13</span></span>
<span class="line"><span style="color:#a6accd">  ports:</span></span>
<span class="line"><span style="color:#a6accd">  - port: 3306</span></span>
<span class="line"><span style="color:#a6accd">    protocol: TCP</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# kubectl create -f mysql-endpoint.yaml </span></span>
<span class="line"><span style="color:#a6accd">endpoints "mysql-service" created</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>3.查看Service和Endpoint的关联</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# kubectl get ep mysql-service</span></span>
<span class="line"><span style="color:#a6accd">NAME            ENDPOINTS            AGE</span></span>
<span class="line"><span style="color:#a6accd">mysql-service   192.168.56.13:3306   42s</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# kubectl describe svc mysql-service</span></span>
<span class="line"><span style="color:#a6accd">Name:              mysql-service</span></span>
<span class="line"><span style="color:#a6accd">Namespace:         default</span></span>
<span class="line"><span style="color:#a6accd">Labels:            &lt;none&gt;</span></span>
<span class="line"><span style="color:#a6accd">Annotations:       &lt;none&gt;</span></span>
<span class="line"><span style="color:#a6accd">Selector:          &lt;none&gt;</span></span>
<span class="line"><span style="color:#a6accd">Type:              ClusterIP</span></span>
<span class="line"><span style="color:#a6accd">IP:                None</span></span>
<span class="line"><span style="color:#a6accd">Port:              &lt;unset&gt;  3306/TCP</span></span>
<span class="line"><span style="color:#a6accd">TargetPort:        3306/TCP</span></span>
<span class="line"><span style="color:#a6accd">Endpoints:         192.168.56.13:3306</span></span>
<span class="line"><span style="color:#a6accd">Session Affinity:  None</span></span>
<span class="line"><span style="color:#a6accd">Events:            &lt;none&gt;</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h1 id="_7-5-第五步：使用ingress提供外部访问" tabindex="-1">7.5 第五步：使用Ingress提供外部访问 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_7-5-第五步：使用ingress提供外部访问" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h1><p>通过Service可以将Kubernetes集群中的服务以IP：Port的方式暴露出来，我们称之为4层的负载均衡，因为这个是OSI七层模型中传输层的功能。</p><p>那么如何实现七层的负载均衡呢，例如像Nginx那样，可以灵活的进行反向代理的设置，根据不同的URL进行转发等，难道我需要自己部署一个Nginx来做这个事情吗？首先，如果你有这个想法，并没有错，甚至你完全可以自己独立部署一个Nginx来完成，但是Kubernetes提供了更好的解决方案就是Ingress。 Ingress就是从kubernetes集群外访问集群的入口，将用户的URL请求转发到不同的service上。ingress相当于nginx反向代理服务器，它包括的规则定义就是URL的路由信息。</p><h2 id="_10-1-ingress-controller" tabindex="-1">10.1 Ingress Controller <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_10-1-ingress-controller" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h2><p>在学习Service的时候，我们知道最终的负载均衡由kube-proxy和LVS来完成，那么Ingress靠什么来实现7层的路由机制呢？答案是Ingress Controller。</p><p>Ingress Controller 实质上可以理解为是个监视器，Ingress Controller 通过不断地跟 kubernetes API 打交道，实时的感知后端 service、pod 等变化，比如新增和减少 pod，service 增加与减少等；当得到这些变化信息后，Ingress Controller 再结合下文的 Ingress 生成配置，然后更新反向代理负载均衡器，并刷新其配置，达到服务发现的作用。</p><p>Ingress Controller目前有两大开源项目，一个是Nginx Controller，一个是目前比较流行的Traefik，Traefik是一款开源的反向代理与负载均衡工具。它最大的优点是能够与常见的微服务系统直接整合，可以实现自动化动态配置。目前支持Docker, Swarm, Mesos/Marathon, Mesos, Kubernetes, Consul, Etcd, Zookeeper, BoltDB, Rest API等等后端模型。</p><h2 id="ingress-controller-traefik" tabindex="-1">Ingress Controller Traefik <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#ingress-controller-traefik" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h2><h3 id="部署treafik" tabindex="-1">部署Treafik <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#部署treafik" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# kubectl label nodes 192.168.56.12 edgenode=true</span></span>
<span class="line"><span style="color:#a6accd">node "192.168.56.12" labeled</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# kubectl create -f /srv/addons/ingress/</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h1 id="_11-第六步：使用pv和pvc管理数据存储" tabindex="-1">11 第六步：使用PV和PVC管理数据存储 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_11-第六步：使用pv和pvc管理数据存储" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h1><p>截止目前我们所启动Pod的容器中的数据存储都是临时的，因此Pod重启或者被删除的时候，产生在容器中的数据会发生丢失。实际应用中，我们有些应用是无状态，有些应用则需要保持状态数据，确保Pod重启之后能够读取到之前的状态数据，有些应用则作为集群提供服务。这三种服务归纳为无状态服务、有状态服务以及有状态的集群服务，其中后面两个存在数据保存与共享的需求，因此就要采用容器外的存储方案。 Kubernetes中存储中有四个重要的概念：Volume、PersistentVolume（PV）、PersistentVolumeClaim （PVC）、StorageClass。掌握了这四个概念，就掌握了Kubernetes中存储系统的核心。</p><h1 id="_11-1-kubernetes-volume" tabindex="-1">11.1 Kubernetes Volume <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_11-1-kubernetes-volume" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h1><h1 id="_11-2-persistentvolume（pv）" tabindex="-1">11.2 PersistentVolume（PV） <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_11-2-persistentvolume（pv）" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h1><p>PersistentVolume（PV）是由管理员设置的存储，它是群集的一部分。就像节点是集群中的资源一样，PV 也是集群中的资源。 PV 是 Volume 之类的卷插件，但具有独立于使用 PV 的 Pod 的生命周期。此 API 对象包含存储实现的细节，即 NFS、iSCSI 或特定于云供应商的存储系统。</p><p><strong>1.安装并配置NFS</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# yum install -y nfs-utils rpcbind</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# mkdir -p /data/k8s-nfs</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# vim /etc/exports</span></span>
<span class="line"><span style="color:#a6accd">/data/k8s-nfs *(rw,sync,no_root_squash)</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>启动NFS</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# systemctl enable rpcbind nfs</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# systemctl start rpcbind nfs</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>2.创建并查看PV</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# vim nfs-pv.yaml</span></span>
<span class="line"><span style="color:#a6accd">apiVersion: v1</span></span>
<span class="line"><span style="color:#a6accd">kind: PersistentVolume</span></span>
<span class="line"><span style="color:#a6accd">metadata:</span></span>
<span class="line"><span style="color:#a6accd">  name: pv-demo</span></span>
<span class="line"><span style="color:#a6accd">spec:</span></span>
<span class="line"><span style="color:#a6accd">  capacity:</span></span>
<span class="line"><span style="color:#a6accd">storage: 1Gi</span></span>
<span class="line"><span style="color:#a6accd">  volumeMode: Filesystem</span></span>
<span class="line"><span style="color:#a6accd">  accessModes:</span></span>
<span class="line"><span style="color:#a6accd">    - ReadWriteOnce</span></span>
<span class="line"><span style="color:#a6accd">  persistentVolumeReclaimPolicy: Recycle</span></span>
<span class="line"><span style="color:#a6accd">  storageClassName: nfs</span></span>
<span class="line"><span style="color:#a6accd">  nfs:</span></span>
<span class="line"><span style="color:#a6accd">    path: /data/k8s-nfs/pv-demo</span></span>
<span class="line"><span style="color:#a6accd">    server: 192.168.56.11</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# kubectl create -f nfs-pv.yaml </span></span>
<span class="line"><span style="color:#a6accd">persistentvolume "pv-demo" created</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>查看创建的PV</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# kubectl get pv</span></span>
<span class="line"><span style="color:#a6accd">NAME      CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM     STORAGECLASS   REASON    AGE</span></span>
<span class="line"><span style="color:#a6accd">pv-demo   1Gi        RWO            Recycle          Available             nfs                      15s</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h1 id="_11-3-persistentvolumeclaim（pvc）" tabindex="-1">11.3 PersistentVolumeClaim（PVC） <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_11-3-persistentvolumeclaim（pvc）" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h1><p>PersistentVolumeClaim（PVC）是用户存储的请求。它与 Pod 相似。Pod 消耗节点资源，PVC 消耗 PV 资源。Pod 可以请求特定级别的资源（CPU 和内存）。声明可以请求特定的大小和访问模式（例如，可以以读/写一次或 只读多次模式挂载）。</p><p>1.创建PVC</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# vim nfs-pvc.yaml</span></span>
<span class="line"><span style="color:#a6accd">apiVersion: v1</span></span>
<span class="line"><span style="color:#a6accd">kind: PersistentVolumeClaim</span></span>
<span class="line"><span style="color:#a6accd">metadata:</span></span>
<span class="line"><span style="color:#a6accd">  name: pvc-demo</span></span>
<span class="line"><span style="color:#a6accd">spec:</span></span>
<span class="line"><span style="color:#a6accd">  accessModes:</span></span>
<span class="line"><span style="color:#a6accd">    - ReadWriteOnce</span></span>
<span class="line"><span style="color:#a6accd">  resources:</span></span>
<span class="line"><span style="color:#a6accd">    requests:</span></span>
<span class="line"><span style="color:#a6accd">      storage: 1Gi</span></span>
<span class="line"><span style="color:#a6accd">  storageClassName: nfs</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>创建并查看PVC</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# kubectl create -f nfs-pvc.yaml </span></span>
<span class="line"><span style="color:#a6accd">persistentvolumeclaim "pvc-demo" created</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# kubectl get pvc</span></span>
<span class="line"><span style="color:#a6accd">NAME       STATUS    VOLUME    CAPACITY   ACCESS MODES   STORAGECLASS   AGE</span></span>
<span class="line"><span style="color:#a6accd">pvc-demo   Bound     pv-demo   1Gi        RWO            nfs            6s</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>2.使用PVC</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# vim nginx-deployment-pvc.yaml</span></span>
<span class="line"><span style="color:#a6accd">apiVersion: apps/v1</span></span>
<span class="line"><span style="color:#a6accd">kind: Deployment</span></span>
<span class="line"><span style="color:#a6accd">metadata:</span></span>
<span class="line"><span style="color:#a6accd">  name: nginx-deployment</span></span>
<span class="line"><span style="color:#a6accd">  labels:</span></span>
<span class="line"><span style="color:#a6accd">    app: nginx</span></span>
<span class="line"><span style="color:#a6accd">spec:</span></span>
<span class="line"><span style="color:#a6accd">  replicas: 3</span></span>
<span class="line"><span style="color:#a6accd">  selector:</span></span>
<span class="line"><span style="color:#a6accd">    matchLabels:</span></span>
<span class="line"><span style="color:#a6accd">      app: nginx</span></span>
<span class="line"><span style="color:#a6accd">  template:</span></span>
<span class="line"><span style="color:#a6accd">    metadata:</span></span>
<span class="line"><span style="color:#a6accd">      labels:</span></span>
<span class="line"><span style="color:#a6accd">        app: nginx</span></span>
<span class="line"><span style="color:#a6accd">    spec:</span></span>
<span class="line"><span style="color:#a6accd">      containers:</span></span>
<span class="line"><span style="color:#a6accd">      - name: nginx</span></span>
<span class="line"><span style="color:#a6accd">        image: nginx:1.13.12</span></span>
<span class="line"><span style="color:#a6accd">        ports:</span></span>
<span class="line"><span style="color:#a6accd">        - containerPort: 80</span></span>
<span class="line"><span style="color:#a6accd">        volumeMounts:</span></span>
<span class="line"><span style="color:#a6accd">        - mountPath: "/usr/share/nginx/html"</span></span>
<span class="line"><span style="color:#a6accd">          name: pvc-demo</span></span>
<span class="line"><span style="color:#a6accd">      volumes:</span></span>
<span class="line"><span style="color:#a6accd">      - name: pvc-demo</span></span>
<span class="line"><span style="color:#a6accd">        persistentVolumeClaim:</span></span>
<span class="line"><span style="color:#a6accd">          claimName: pvc-demo</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h1 id="_11-4-storageclass" tabindex="-1">11.4 StorageClass <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_11-4-storageclass" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h1><h1 id="_7-第七步：使用rancher管理kubernetes集群" tabindex="-1">7 第七步：使用Rancher管理Kubernetes集群 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_7-第七步：使用rancher管理kubernetes集群" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h1><p>现在我们已经具备把应用迁移到Kubernetes中来的能力，那么现在，迁移后，日常的运维工作就发生的变化，截止目前，在生产环境中，我们很少使用官方自带的Dashbaord来完成日常的运维工作，而是使用第三方的运维工具Rancher。</p><h2 id="_7-1-rancher快速入门" tabindex="-1">7.1 Rancher快速入门 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_7-1-rancher快速入门" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h2><p>快速部署单机版Rancher</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# mkdir /opt/rancher</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# docker run -d --restart=unless-stopped -v /opt/rancher:/var/lib/rancher/ -p 80:80 -p 443:443 rancher/rancher</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h2 id="_7-2-使用rancher进行日常管理" tabindex="-1">7.2 使用Rancher进行日常管理 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_7-2-使用rancher进行日常管理" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h2><h2 id="_7-3-rancher生产集群部署" tabindex="-1">7.3 Rancher生产集群部署 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_7-3-rancher生产集群部署" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h2><h1 id="第六部分-管理kubernetes中的应用" tabindex="-1">第六部分 管理Kubernetes中的应用 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#第六部分-管理kubernetes中的应用" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h1><h1 id="_13-应用的资源限制和健康检查" tabindex="-1">13 应用的资源限制和健康检查 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_13-应用的资源限制和健康检查" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h1><h1 id="_13-1-应用的资源限制" tabindex="-1">13.1 应用的资源限制 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_13-1-应用的资源限制" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h1><h1 id="_13-2-应用的健康检查" tabindex="-1">13.2 应用的健康检查 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_13-2-应用的健康检查" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h1><h3 id="liveness探测" tabindex="-1">Liveness探测 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#liveness探测" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>Kubernetes有两种探测机制，Liveness和Readiness，配置都是相似的，只是实现的功能不同。 Liveness探测是针对Pod健康状态的探测，类似于集群中的健康检查，用户可以自定义这个健康检查的条件，如果探测失败，Kubernetes将会重启容器。 如果您希望容器在探测失败时被杀死并重新启动，那么请指定一个Liveness配置，并指定restartPolicy 为 Always 或 OnFailure。 配置案例</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">livenessProbe:</span></span>
<span class="line"><span style="color:#a6accd">exec:</span></span>
<span class="line"><span style="color:#a6accd">    command:</span></span>
<span class="line"><span style="color:#a6accd">    - ps aux | grep nginx</span></span>
<span class="line"><span style="color:#a6accd">initialDelaySeconds: 10</span></span>
<span class="line"><span style="color:#a6accd">periodSeconds: 5</span></span>
<span class="line"><span style="color:#a6accd">timeoutSeconds: 3</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h3 id="readiness探测" tabindex="-1">Readiness探测 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#readiness探测" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>Readiness探测是探测Pod是否准备好对外提供访问，例如我们在Pod里面运行一个Tomcat的容器，里面运行了一个Jenkins的应用，那么等Jenkins完全启动能提供服务可能需要1分钟，所以在在1分钟之前是不能提供给用户访问的，也就是不能加入Service的负载均衡中，这个就靠Readiness探测来控制。 Readiness用来控制告诉Kubernetes什么时间可以将容器加入到Service的负载均衡中，配置方法和Liveness一样，只需要修改livennessProbe替换为readinessProbe即可。</p><h2 id="健康检查的方法" tabindex="-1">健康检查的方法 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#健康检查的方法" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h2><p>Kubernetes的健康检查是由运行在各个Node上的kubelet来完成的，kubelet目前支持以下三种健康检查的方法：</p><ul><li>ExecAction：在容器中执行指定的命令。如果命令退出时状态码为0，则认为诊断成功。</li><li>TCPSocketAction:对指定端口上容器的IP地址执行TCP检查。如果端口是打开的，则认为诊断是成功的。</li><li>HTTPGetAction:对指定端口和路径上容器的IP地址执行HTTP Get请求。如果响应的状态码大于或等于200，小于400，则认为诊断是成功的。</li></ul><p>以上三种健康检查的方法会有三种返回结果：</p><ul><li><p>Success：成功，容器通过诊断。</p></li><li><p>Failure：失败，容器诊断失败。</p></li><li><p>Unknown：探测失败，无法执行探测，所以不应该采取任何行动。</p><p>针对于探针有以下配置参数，需要注意不管是Liveness还是Readiness探测，探针的使用都是相同的，唯一的不同是探测完毕后，执行操作的不同。</p></li><li><p>initialDelaySeconds: 探测的延迟时间，单位是秒。也就是说在容器启动多少秒之后开始进行第一次探测，例如你启动一个Java的应用需要50秒，那么这个值就需要大于50s。所以这个值是需要根据应用的具体情况来设置。</p></li><li><p>periodSeconds：探测执行的周期时间，单位是秒。是指每隔多长时间执行一次探测，频率越高，发现故障的时间也就越短，并不是越短越好。如果应用服务不够稳定，太高的频率反而会导致很多你认为的“误报”。默认是10秒，最小值是1秒。</p></li><li><p>timeoutSeconds: 探测超时时间，单位是秒，执行探测如果超过这个时间没有返回结果，变意味着探测的结果是失败。默认为1秒。最小值是1秒。</p></li><li><p>failureThreshold：探测成功后，最少连续探测失败多少次才被认定为失败。这个是Kubernetes将在放弃之前尝试失败阈值时间。放弃生命探测意味着重新启动Pod。一旦准备就绪，Pod将被标记为未准备就绪。默认为3。最小值是1。</p></li><li><p>successThreshold: 在探测失败后，最少连续探测成功多少次才被认定为成功。默认为1，也就是必须探测成功1次，才能认为状态恢复，最小值是1。</p><h1 id="管理应用的dns访问" tabindex="-1">管理应用的DNS访问 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#管理应用的dns访问" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h1></li></ul><h1 id="_14-1-kubernetes中的dns" tabindex="-1"><a target="_blank" rel="noreferrer" href="http://k8s.unixhot.com/"><!--[-->14.1 Kubernetes中的DNS<!--]--><!----></a> <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_14-1-kubernetes中的dns" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h1><h1 id="应用的dns管理" tabindex="-1">应用的DNS管理 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#应用的dns管理" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h1><h3 id="pod的域名解析策略" tabindex="-1">Pod的域名解析策略 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#pod的域名解析策略" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# kubectl run dns-test --generator=run-pod/v1 --image=alpine --replicas=1 sleep 360000</span></span>
<span class="line"><span style="color:#a6accd">pod/dns-test created</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>查看Pod</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# kubectl get pod dns-test</span></span>
<span class="line"><span style="color:#a6accd">NAME       READY   STATUS    RESTARTS   AGE</span></span>
<span class="line"><span style="color:#a6accd">dns-test   1/1     Running   0          79s</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>Pod默认的DNS配置</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# kubectl exec -it dns-test /bin/sh</span></span>
<span class="line"><span style="color:#a6accd">/ # cat /etc/resolv.conf </span></span>
<span class="line"><span style="color:#a6accd">nameserver 10.1.0.10</span></span>
<span class="line"><span style="color:#a6accd">search default.svc.cluster.local svc.cluster.local cluster.local</span></span>
<span class="line"><span style="color:#a6accd">options ndots:5</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>如何访问Service名称</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">/ # ping -c 3 wordpress-service.default.svc.cluster.local</span></span>
<span class="line"><span style="color:#a6accd">PING wordpress-service.default.svc.cluster.local (10.1.92.244): 56 data bytes</span></span>
<span class="line"><span style="color:#a6accd">64 bytes from 10.1.92.244: seq=0 ttl=64 time=0.074 ms</span></span>
<span class="line"><span style="color:#a6accd">64 bytes from 10.1.92.244: seq=1 ttl=64 time=0.141 ms</span></span>
<span class="line"><span style="color:#a6accd">64 bytes from 10.1.92.244: seq=2 ttl=64 time=0.187 ms</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">--- wordpress-service.default.svc.cluster.local ping statistics ---</span></span>
<span class="line"><span style="color:#a6accd">3 packets transmitted, 3 packets received, 0% packet loss</span></span>
<span class="line"><span style="color:#a6accd">round-trip min/avg/max = 0.074/0.134/0.187 ms</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>DNS查询策略</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# kubectl get pod dns-test -o yaml | grep dnsPolicy</span></span>
<span class="line"><span style="color:#a6accd">  dnsPolicy: ClusterFirst</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><ul><li>Default: Pod从其运行的节点中继承名称解析配置。</li><li>ClusterFirst:（默认策略）与配置的群集域名后缀不匹配的任何DNS查询都将转发到从节点继承的上游名称服务器。</li><li>ClusterFirstWithHostNet: 如果Pod使用了hostNetwork（例如Ingress Controller Treafik就是使用了hostNetwok），应显式设置其DNS策略为“ClusterFirstWithHostNet”。</li><li>None: 它允许Pod忽略Kubernetes环境中的DNS设置，这时候会使用Pod Spec中的dnsConfig字段提供的DNS设置。</li></ul><h2 id="应用的dns管理-1" tabindex="-1">应用的DNS管理 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#应用的dns管理-1" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h2><h2 id="_15-1-使用configmap管理应用配置" tabindex="-1">15.1 使用ConfigMap管理应用配置 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_15-1-使用configmap管理应用配置" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h2><h3 id="通过kubectl命令创建configmap" tabindex="-1">通过kubectl命令创建ConfigMap <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#通过kubectl命令创建configmap" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p><strong>创建一个名称为cmd-config的ConfigMap</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@k8s-master1 ~]# kubectl create configmap cmd-config --from-literal=host=www.unixhot.com</span></span>
<span class="line"><span style="color:#a6accd">configmap/cmd-config created</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>查看ConfigMap</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@k8s-master1 ~]# kubectl get configmap</span></span>
<span class="line"><span style="color:#a6accd">NAME         DATA   AGE</span></span>
<span class="line"><span style="color:#a6accd">cmd-config   1      63s</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>查看ConfigMap内容</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@k8s-master1 ~]# kubectl describe configmap cmd-config</span></span>
<span class="line"><span style="color:#a6accd">Name:         cmd-config</span></span>
<span class="line"><span style="color:#a6accd">Namespace:    default</span></span>
<span class="line"><span style="color:#a6accd">Labels:       &lt;none&gt;</span></span>
<span class="line"><span style="color:#a6accd">Annotations:  &lt;none&gt;</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">Data</span></span>
<span class="line"><span style="color:#a6accd">====</span></span>
<span class="line"><span style="color:#a6accd">host:</span></span>
<span class="line"><span style="color:#a6accd">----</span></span>
<span class="line"><span style="color:#a6accd">www.unixhot.com</span></span>
<span class="line"><span style="color:#a6accd">Events:  &lt;none&gt;</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>ConfigMap中包含多个键值对</strong> 可以多次使用--from-literal为一个ConfigMap创建多个键值对，中间用空格分隔</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@k8s-master1 ~]# kubectl create configmap mcmd-config --from-literal=host=www.unixhot.com --from-literal=port=443 --from-literal=ssl=on</span></span>
<span class="line"><span style="color:#a6accd">configmap/mcmd-config created</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h3 id="通过yaml文件创建configmap" tabindex="-1">通过YAML文件创建ConfigMap <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#通过yaml文件创建configmap" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>查看已创建的ConfigMap生成的YAML文件</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@k8s-master1 ~]# kubectl get configmap mcmd-config -o yaml</span></span>
<span class="line"><span style="color:#a6accd">apiVersion: v1</span></span>
<span class="line"><span style="color:#a6accd">data:</span></span>
<span class="line"><span style="color:#a6accd">  host: www.unixhot.com</span></span>
<span class="line"><span style="color:#a6accd">  port: "443"</span></span>
<span class="line"><span style="color:#a6accd">  ssl: "on"</span></span>
<span class="line"><span style="color:#a6accd">kind: ConfigMap</span></span>
<span class="line"><span style="color:#a6accd">metadata:</span></span>
<span class="line"><span style="color:#a6accd">  creationTimestamp: "2019-11-05T01:45:13Z"</span></span>
<span class="line"><span style="color:#a6accd">  name: mcmd-config</span></span>
<span class="line"><span style="color:#a6accd">  namespace: default</span></span>
<span class="line"><span style="color:#a6accd">  resourceVersion: "5394993"</span></span>
<span class="line"><span style="color:#a6accd">  selfLink: /api/v1/namespaces/default/configmaps/mcmd-config</span></span>
<span class="line"><span style="color:#a6accd">  uid: 02012d69-e324-4e9d-ba04-7132e9f6ecd8</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>只需要将metadata中无需指定的字段去掉即可生成一个YAML文件。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@k8s-master1 ~]# kubectl get configmap mcmd-config -o yaml &gt; mcmd-config-v2.yaml</span></span>
<span class="line"><span style="color:#a6accd">[root@k8s-master1 ~]# vim mcmd-config-v2.yaml </span></span>
<span class="line"><span style="color:#a6accd">apiVersion: v1</span></span>
<span class="line"><span style="color:#a6accd">data:</span></span>
<span class="line"><span style="color:#a6accd">  host: www.unixhot.com</span></span>
<span class="line"><span style="color:#a6accd">  port: "443"</span></span>
<span class="line"><span style="color:#a6accd">  ssl: "on"</span></span>
<span class="line"><span style="color:#a6accd">kind: ConfigMap</span></span>
<span class="line"><span style="color:#a6accd">metadata:</span></span>
<span class="line"><span style="color:#a6accd">  name: mcmd-config-v2</span></span>
<span class="line"><span style="color:#a6accd">  namespace: default</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><a target="_blank" rel="noreferrer" href="http://xn--metadata-0n3mp82lcujlhxj58f8qvb.name"><!--[-->注意需要修改metadata.name<!--]--><!----></a>,修改完毕后直接创建即可</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@k8s-master1 ~]# kubectl create -f mcmd-config-v2.yaml </span></span>
<span class="line"><span style="color:#a6accd">configmap/mcmd-config-v2 created</span></span>
<span class="line"><span style="color:#a6accd">[root@k8s-master1 ~]# kubectl get configmap</span></span>
<span class="line"><span style="color:#a6accd">NAME             DATA   AGE</span></span>
<span class="line"><span style="color:#a6accd">cmd-config       1      24m</span></span>
<span class="line"><span style="color:#a6accd">mcmd-config      3      16m</span></span>
<span class="line"><span style="color:#a6accd">mcmd-config-v2   3      9s</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h3 id="通过文件创建configmap" tabindex="-1">通过文件创建ConfigMap <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#通过文件创建configmap" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>ConfigMap除了可以存储单个或者多个键值对之外，可以存储完整的配置文件，将单个配置文件直接转换为ConfigMap在生产中十分常用</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@k8s-master1 ~]# kubectl create configmap file-config --from-file=/etc/hosts</span></span>
<span class="line"><span style="color:#a6accd">configmap/file-config created</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>可以看到ConfigMap直接存储了文件的内容，Key名称为文件名hosts，也可以手动指定Key的名称。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@k8s-master1 ~]# kubectl describe configmap file-config</span></span>
<span class="line"><span style="color:#a6accd">Name:         file-config</span></span>
<span class="line"><span style="color:#a6accd">Namespace:    default</span></span>
<span class="line"><span style="color:#a6accd">Labels:       &lt;none&gt;</span></span>
<span class="line"><span style="color:#a6accd">Annotations:  &lt;none&gt;</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">Data</span></span>
<span class="line"><span style="color:#a6accd">====</span></span>
<span class="line"><span style="color:#a6accd">hosts:</span></span>
<span class="line"><span style="color:#a6accd">----</span></span>
<span class="line"><span style="color:#a6accd">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span></span>
<span class="line"><span style="color:#a6accd">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span></span>
<span class="line"><span style="color:#a6accd">192.168.99.27 k8s-master1 k8s-master1.dianjoy.com </span></span>
<span class="line"><span style="color:#a6accd">192.168.99.28 k8s-master2 k8s-master2.dianjoy.com</span></span>
<span class="line"><span style="color:#a6accd">192.168.99.29 k8s-master3 k8s-master3.dianjoy.com</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">Events:  &lt;none&gt;</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>将Key手动指定为host-hosts</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@k8s-master1 ~]# kubectl create configmap file-config-v2 --from-file=host-hosts=/etc/hosts</span></span>
<span class="line"><span style="color:#a6accd">configmap/file-config-v2 created</span></span>
<span class="line"><span style="color:#a6accd">[root@k8s-master1 ~]# kubectl describe configmap file-config-v2</span></span>
<span class="line"><span style="color:#a6accd">Name:         file-config-v2</span></span>
<span class="line"><span style="color:#a6accd">Namespace:    default</span></span>
<span class="line"><span style="color:#a6accd">Labels:       &lt;none&gt;</span></span>
<span class="line"><span style="color:#a6accd">Annotations:  &lt;none&gt;</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">Data</span></span>
<span class="line"><span style="color:#a6accd">====</span></span>
<span class="line"><span style="color:#a6accd">host-hosts:</span></span>
<span class="line"><span style="color:#a6accd">----</span></span>
<span class="line"><span style="color:#a6accd">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span></span>
<span class="line"><span style="color:#a6accd">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span></span>
<span class="line"><span style="color:#a6accd">192.168.99.27 k8s-master1 k8s-master1.dianjoy.com </span></span>
<span class="line"><span style="color:#a6accd">192.168.99.28 k8s-master2 k8s-master2.dianjoy.com</span></span>
<span class="line"><span style="color:#a6accd">192.168.99.29 k8s-master3 k8s-master3.dianjoy.com</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">Events:  &lt;none&gt;</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h3 id="从目录创建configmap" tabindex="-1">从目录创建ConfigMap <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#从目录创建configmap" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>ConfigMap还支持通过目录创建，kubectl会为目录中的每个文件单独创建条目，需要注意的是如果目录下面包含子目录，会忽略这些子目录和子目录里面的内容。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@k8s-master1 ~]# kubectl create configmap dir-config --from-file=/etc/kubernetes</span></span>
<span class="line"><span style="color:#a6accd">configmap/dir-config created</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h3 id="混合选项创建configmap" tabindex="-1">混合选项创建ConfigMap <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#混合选项创建configmap" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>同时使用命令行、文件、目录创建ConfigMap也是支持的，只需要使用不同的选项即可。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@k8s-master1 ~]# kubectl create configmap mycp --from-literal=env=test \</span></span>
<span class="line"><span style="color:#a6accd"> --from-file=/etc/hosts \</span></span>
<span class="line"><span style="color:#a6accd"> --from-file=myhosts=/etc/hosts \</span></span>
<span class="line"><span style="color:#a6accd"> --from-file=/etc/kubernetes</span></span>
<span class="line"><span style="color:#a6accd">configmap/mycp created</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>ConfigMap的内容可以通过环境变量的形成传递给容器，也可通过和Volume的形式挂载到容器中。</p><h3 id="通过环境变量给容器传递configmap" tabindex="-1">通过环境变量给容器传递ConfigMap <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#通过环境变量给容器传递configmap" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>可以将ConfigMap中的键值对数据通过环境变量的形式传递到容器中，这样在配置容器的时候有一些数据可以使用环境变量，然后使用ConfigMap进行填充，这样就可以实现配置和Pod的分离。</p><h2 id="_15-2-使用secret管理敏感数据" tabindex="-1">15.2 使用Secret管理敏感数据 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_15-2-使用secret管理敏感数据" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h2><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">在应用启动过程中经常会有一些敏感信息需要存储，例如用户名和密码等，如果直接明文的方式保存会有安全风险。在Kubernetes中Secret这个资源对象类型用来保存敏感信息，例如密码、密钥、访问令牌、SSH Key等你认为需要保密的敏感信息。相对于将这些内容保存到容器镜像或者Pod的定义文件中，更加的灵活和安全。</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h3 id="配置pod使用harbor镜像" tabindex="-1">配置Pod使用Harbor镜像 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#配置pod使用harbor镜像" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>1．docker login得到 docker密码文件</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# docker login 192.168.56.11</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>2.对密码文件进行加密</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# cat /root/.docker/config.json |base64</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>3.创建harbor使用的Secret YAML文件：</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# vim harbor-secret.yaml </span></span>
<span class="line"><span style="color:#a6accd">apiVersion: v1</span></span>
<span class="line"><span style="color:#a6accd">kind: Secret</span></span>
<span class="line"><span style="color:#a6accd">metadata:</span></span>
<span class="line"><span style="color:#a6accd">  name: harbor-secret</span></span>
<span class="line"><span style="color:#a6accd">  namespace: default</span></span>
<span class="line"><span style="color:#a6accd">data:</span></span>
<span class="line"><span style="color:#a6accd">  .dockerconfigjson: 'ewoJImF1dGhzIjogewoJCSJyZWcuZ3JlYXRvcHMubmV0IjogewoJCQkiYXV0aCI6ICJZV1J0YVc0Nk1YRmhlbmh6ZHpJPSIKCQl9Cgl9Cn0='</span></span>
<span class="line"><span style="color:#a6accd">type: kubernetes.io/dockerconfigjson</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>4.创建Secret</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@jenkins k8s-deploy]# kubectl create -f reg-harbor.yaml </span></span>
<span class="line"><span style="color:#a6accd">secret "reg-harbor" created</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>5.创建pod并挂载资源</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">apiVersion: v1</span></span>
<span class="line"><span style="color:#a6accd">kind: Pod</span></span>
<span class="line"><span style="color:#a6accd">metadata:</span></span>
<span class="line"><span style="color:#a6accd">  name: sectest</span></span>
<span class="line"><span style="color:#a6accd">spec:</span></span>
<span class="line"><span style="color:#a6accd">  containers:</span></span>
<span class="line"><span style="color:#a6accd">  - name: sectest</span></span>
<span class="line"><span style="color:#a6accd">    image: 123.207.154.16/base/redis:alpine</span></span>
<span class="line"><span style="color:#a6accd">    ports:</span></span>
<span class="line"><span style="color:#a6accd">    - containerPort: 6379</span></span>
<span class="line"><span style="color:#a6accd">  imagePullSecrets:</span></span>
<span class="line"><span style="color:#a6accd">    - name: harbor-secret</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h1 id="使用helm管理kubernetes应用" tabindex="-1">使用Helm管理Kubernetes应用 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#使用helm管理kubernetes应用" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h1><p>通过前面的学习，掌握了将应用迁移至Kubernetes的步骤和技巧，过程比较艰辛。例如我们创建一个应用，涉及到Deployment、Service、Ingress、PV、PVC，如何有效的管理这些资源呢，Kubernetes给出了一个最佳实践就是Helm。 Helm是一个kubernetes应用的包管理工具，用来管理charts——预先配置好的安装包资源，有点类似于Ubuntu的APT和CentOS中的yum。 Helm chart是用来封装kubernetes原生应用程序的yaml文件，可以在你部署应用的时候自定义应用程序的一些metadata，便与应用程序的分发。</p><p>Helm和charts的主要作用：</p><ul><li>应用程序封装</li><li>版本管理</li><li>依赖检查</li><li>便于应用程序分发</li></ul><h2 id="helm部署" tabindex="-1">Helm部署 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#helm部署" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h2><h3 id="安装helm" tabindex="-1">安装Helm <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#安装helm" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>1.部署Helm客户端</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# cd /usr/local/src</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 src]# wget https://get.helm.sh/helm-v3.0.2-linux-amd64.tar.gz</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 src]# tar zxf helm-v3.0.2-linux-amd64.tar.gz</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 src]# mv linux-amd64/helm /usr/local/bin/</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>2.验证安装是否成功</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# helm version</span></span>
<span class="line"><span style="color:#a6accd">version.BuildInfo{Version:"v3.0.2", GitCommit:"19e47ee3283ae98139d98460de796c1be1e3975f", GitTreeState:"clean", GoVersion:"go1.13.5"}</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h3 id="使用helm部署第一个应用" tabindex="-1">使用Helm部署第一个应用 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#使用helm部署第一个应用" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>4.搜索Helm应用</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# helm search jenkins</span></span>
<span class="line"><span style="color:#a6accd">NAME              CHART VERSION    APP VERSION    DESCRIPTION                                       </span></span>
<span class="line"><span style="color:#a6accd">stable/jenkins    0.13.5           2.73           Open source continuous integration server. It s...</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>5.查看仓库</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# helm repo list</span></span>
<span class="line"><span style="color:#a6accd">NAME      URL                                                   </span></span>
<span class="line"><span style="color:#a6accd">stable    https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts</span></span>
<span class="line"><span style="color:#a6accd">local     http://127.0.0.1:8879/charts</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>6.安装第一个应用</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# helm install stable/jenkins</span></span>
<span class="line"><span style="color:#a6accd">NAME:   viable-seal</span></span>
<span class="line"><span style="color:#a6accd">LAST DEPLOYED: Thu Jul 26 19:21:07 2018</span></span>
<span class="line"><span style="color:#a6accd">NAMESPACE: default</span></span>
<span class="line"><span style="color:#a6accd">STATUS: DEPLOYED</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">RESOURCES:</span></span>
<span class="line"><span style="color:#a6accd">==&gt; v1/ConfigMap</span></span>
<span class="line"><span style="color:#a6accd">NAME                       DATA  AGE</span></span>
<span class="line"><span style="color:#a6accd">viable-seal-jenkins        3     1s</span></span>
<span class="line"><span style="color:#a6accd">viable-seal-jenkins-tests  1     1s</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">==&gt; v1/PersistentVolumeClaim</span></span>
<span class="line"><span style="color:#a6accd">NAME                 STATUS   VOLUME  CAPACITY  ACCESS MODES  STORAGECLASS  AGE</span></span>
<span class="line"><span style="color:#a6accd">viable-seal-jenkins  Pending  1s</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">==&gt; v1/Service</span></span>
<span class="line"><span style="color:#a6accd">NAME                       TYPE          CLUSTER-IP   EXTERNAL-IP  PORT(S)         AGE</span></span>
<span class="line"><span style="color:#a6accd">viable-seal-jenkins-agent  ClusterIP     10.1.154.54  &lt;none&gt;       50000/TCP       1s</span></span>
<span class="line"><span style="color:#a6accd">viable-seal-jenkins        LoadBalancer  10.1.63.24   &lt;pending&gt;    8080:20031/TCP  0s</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">==&gt; v1beta1/Deployment</span></span>
<span class="line"><span style="color:#a6accd">NAME                 DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE</span></span>
<span class="line"><span style="color:#a6accd">viable-seal-jenkins  1        1        1           0          0s</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">==&gt; v1/Pod(related)</span></span>
<span class="line"><span style="color:#a6accd">NAME                                  READY  STATUS   RESTARTS  AGE</span></span>
<span class="line"><span style="color:#a6accd">viable-seal-jenkins-7f5c7bd8d4-gc5hv  0/1    Pending  0         0s</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">==&gt; v1/Secret</span></span>
<span class="line"><span style="color:#a6accd">NAME                 TYPE    DATA  AGE</span></span>
<span class="line"><span style="color:#a6accd">viable-seal-jenkins  Opaque  2     1s</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">NOTES:</span></span>
<span class="line"><span style="color:#a6accd">1. Get your 'admin' user password by running:</span></span>
<span class="line"><span style="color:#a6accd">  printf $(kubectl get secret --namespace default viable-seal-jenkins -o jsonpath="{.data.jenkins-admin-password}" | base64 --decode);echo</span></span>
<span class="line"><span style="color:#a6accd">2. Get the Jenkins URL to visit by running these commands in the same shell:</span></span>
<span class="line"><span style="color:#a6accd">  NOTE: It may take a few minutes for the LoadBalancer IP to be available.</span></span>
<span class="line"><span style="color:#a6accd">        You can watch the status of by running 'kubectl get svc --namespace default -w viable-seal-jenkins'</span></span>
<span class="line"><span style="color:#a6accd">  export SERVICE_IP=$(kubectl get svc --namespace default viable-seal-jenkins --template "{{ range (index .status.loadBalancer.ingress 0) }}{{ . }}{{ end }}")</span></span>
<span class="line"><span style="color:#a6accd">  echo http://$SERVICE_IP:8080/login</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">3. Login with the password from step 1 and the username: admin</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">For more information on running Jenkins on Kubernetes, visit:</span></span>
<span class="line"><span style="color:#a6accd">https://cloud.google.com/solutions/jenkins-on-container-engine</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h1 id="_15-2-深入理解helm" tabindex="-1">15.2 深入理解Helm <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_15-2-深入理解helm" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h1><h3 id="helm组件" tabindex="-1">Helm组件 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#helm组件" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# tree ~/.helm/</span></span>
<span class="line"><span style="color:#a6accd">/root/.helm/</span></span>
<span class="line"><span style="color:#a6accd">├── cache</span></span>
<span class="line"><span style="color:#a6accd">│   └── archive</span></span>
<span class="line"><span style="color:#a6accd">│       └── jenkins-0.13.5.tgz</span></span>
<span class="line"><span style="color:#a6accd">├── plugins</span></span>
<span class="line"><span style="color:#a6accd">├── repository</span></span>
<span class="line"><span style="color:#a6accd">│   ├── cache</span></span>
<span class="line"><span style="color:#a6accd">│   │   ├── local-index.yaml -&gt; /root/.helm/repository/local/index.yaml</span></span>
<span class="line"><span style="color:#a6accd">│   │   └── stable-index.yaml</span></span>
<span class="line"><span style="color:#a6accd">│   ├── local</span></span>
<span class="line"><span style="color:#a6accd">│   │   └── index.yaml</span></span>
<span class="line"><span style="color:#a6accd">│   └── repositories.yaml</span></span>
<span class="line"><span style="color:#a6accd">└── starters</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">7 directories, 5 files</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>默认缓存的文件</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# cd .helm/cache/archive/</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 archive]# ls -l</span></span>
<span class="line"><span style="color:#a6accd">total 16</span></span>
<span class="line"><span style="color:#a6accd">-rw-r--r-- 1 root root 12650 Jul 26 19:21 jenkins-0.13.5.tgz</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 archive]# tar zxf jenkins-0.13.5.tgz</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 archive]# mv jenkins ~/.helm/repository/local/</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# cd ~/.helm/repository/local/jenkins/</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 jenkins]# tree</span></span>
<span class="line"><span style="color:#a6accd">.</span></span>
<span class="line"><span style="color:#a6accd">├── Chart.yaml</span></span>
<span class="line"><span style="color:#a6accd">├── OWNERS</span></span>
<span class="line"><span style="color:#a6accd">├── README.md</span></span>
<span class="line"><span style="color:#a6accd">├── templates</span></span>
<span class="line"><span style="color:#a6accd">│   ├── config.yaml</span></span>
<span class="line"><span style="color:#a6accd">│   ├── _helpers.tpl</span></span>
<span class="line"><span style="color:#a6accd">│   ├── home-pvc.yaml</span></span>
<span class="line"><span style="color:#a6accd">│   ├── jenkins-agent-svc.yaml</span></span>
<span class="line"><span style="color:#a6accd">│   ├── jenkins-master-deployment.yaml</span></span>
<span class="line"><span style="color:#a6accd">│   ├── jenkins-master-ingress.yaml</span></span>
<span class="line"><span style="color:#a6accd">│   ├── jenkins-master-networkpolicy.yaml</span></span>
<span class="line"><span style="color:#a6accd">│   ├── jenkins-master-svc.yaml</span></span>
<span class="line"><span style="color:#a6accd">│   ├── jenkins-test.yaml</span></span>
<span class="line"><span style="color:#a6accd">│   ├── jobs.yaml</span></span>
<span class="line"><span style="color:#a6accd">│   ├── NOTES.txt</span></span>
<span class="line"><span style="color:#a6accd">│   ├── rbac.yaml</span></span>
<span class="line"><span style="color:#a6accd">│   ├── secret.yaml</span></span>
<span class="line"><span style="color:#a6accd">│   ├── service-account.yaml</span></span>
<span class="line"><span style="color:#a6accd">│   └── test-config.yaml</span></span>
<span class="line"><span style="color:#a6accd">└── values.yaml</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">1 directory, 19 files</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h3 id="自定义jenkins的chart" tabindex="-1">自定义Jenkins的Chart <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#自定义jenkins的chart" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>修改为NodePort</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 jenkins]# vim values.yaml</span></span>
<span class="line"><span style="color:#a6accd">ServiceType: NodePort</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>检查</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# helm lint ~/.helm/repository/local/jenkins/</span></span>
<span class="line"><span style="color:#a6accd">==&gt; Linting /root/.helm/repository/local/jenkins/</span></span>
<span class="line"><span style="color:#a6accd">Lint OK</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">1 chart(s) linted, no failures</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>查看有哪些应用</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# helm list</span></span>
<span class="line"><span style="color:#a6accd">NAME           REVISION    UPDATED                     STATUS      CHART          NAMESPACE</span></span>
<span class="line"><span style="color:#a6accd">viable-seal    1           Thu Jul 26 19:21:07 2018    DEPLOYED    jenkins-0.13.5 default</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# helm delete --purge viable-seal</span></span>
<span class="line"><span style="color:#a6accd">release "viable-seal" deleted</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# helm install ~/.helm/repository/local/jenkins/ --name devops-jenkins </span></span>
<span class="line"><span style="color:#a6accd">NAME:   devops-jenkins</span></span>
<span class="line"><span style="color:#a6accd">LAST DEPLOYED: Thu Jul 26 19:36:10 2018</span></span>
<span class="line"><span style="color:#a6accd">NAMESPACE: default</span></span>
<span class="line"><span style="color:#a6accd">STATUS: DEPLOYED</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">RESOURCES:</span></span>
<span class="line"><span style="color:#a6accd">==&gt; v1/Secret</span></span>
<span class="line"><span style="color:#a6accd">NAME            TYPE    DATA  AGE</span></span>
<span class="line"><span style="color:#a6accd">devops-jenkins  Opaque  2     0s</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">==&gt; v1/ConfigMap</span></span>
<span class="line"><span style="color:#a6accd">NAME                  DATA  AGE</span></span>
<span class="line"><span style="color:#a6accd">devops-jenkins        3     0s</span></span>
<span class="line"><span style="color:#a6accd">devops-jenkins-tests  1     0s</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">==&gt; v1/PersistentVolumeClaim</span></span>
<span class="line"><span style="color:#a6accd">NAME            STATUS   VOLUME  CAPACITY  ACCESS MODES  STORAGECLASS  AGE</span></span>
<span class="line"><span style="color:#a6accd">devops-jenkins  Pending  0s</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">==&gt; v1/Service</span></span>
<span class="line"><span style="color:#a6accd">NAME                  TYPE       CLUSTER-IP   EXTERNAL-IP  PORT(S)         AGE</span></span>
<span class="line"><span style="color:#a6accd">devops-jenkins-agent  ClusterIP  10.1.74.175  &lt;none&gt;       50000/TCP       0s</span></span>
<span class="line"><span style="color:#a6accd">devops-jenkins        NodePort   10.1.3.112   &lt;none&gt;       8080:23558/TCP  0s</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">==&gt; v1beta1/Deployment</span></span>
<span class="line"><span style="color:#a6accd">NAME            DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE</span></span>
<span class="line"><span style="color:#a6accd">devops-jenkins  1        1        1           0          0s</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">==&gt; v1/Pod(related)</span></span>
<span class="line"><span style="color:#a6accd">NAME                            READY  STATUS   RESTARTS  AGE</span></span>
<span class="line"><span style="color:#a6accd">devops-jenkins-64d54b79c-pwjfb  0/1    Pending  0         0s</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">NOTES:</span></span>
<span class="line"><span style="color:#a6accd">1. Get your 'admin' user password by running:</span></span>
<span class="line"><span style="color:#a6accd">  printf $(kubectl get secret --namespace default devops-jenkins -o jsonpath="{.data.jenkins-admin-password}" | base64 --decode);echo</span></span>
<span class="line"><span style="color:#a6accd">2. Get the Jenkins URL to visit by running these commands in the same shell:</span></span>
<span class="line"><span style="color:#a6accd">  export NODE_PORT=$(kubectl get --namespace default -o jsonpath="{.spec.ports[0].nodePort}" services devops-jenkins)</span></span>
<span class="line"><span style="color:#a6accd">  export NODE_IP=$(kubectl get nodes --namespace default -o jsonpath="{.items[0].status.addresses[0].address}")</span></span>
<span class="line"><span style="color:#a6accd">  echo http://$NODE_IP:$NODE_PORT/login</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">3. Login with the password from step 1 and the username: admin</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">For more information on running Jenkins on Kubernetes, visit:</span></span>
<span class="line"><span style="color:#a6accd">https://cloud.google.com/solutions/jenkins-on-container-engine</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>查看状态</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# helm status devops-jenkins</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h2 id="创建自己的chart" tabindex="-1">创建自己的Chart <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#创建自己的chart" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h2><h3 id="创建自定义nginx的chart" tabindex="-1">创建自定义Nginx的Chart <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#创建自定义nginx的chart" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>1.创建自定义Chart Nginx的结构</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# helm create helm-nginx</span></span>
<span class="line"><span style="color:#a6accd">Creating helm-nginx</span></span>
<span class="line"><span style="color:#a6accd"> [root@linux-node1 ~]# tree helm-nginx/</span></span>
<span class="line"><span style="color:#a6accd">opencmdb/</span></span>
<span class="line"><span style="color:#a6accd">├── charts       #依赖的chart</span></span>
<span class="line"><span style="color:#a6accd">├── Chart.yaml   #本chart的信息</span></span>
<span class="line"><span style="color:#a6accd">├── templates    #模板目录</span></span>
<span class="line"><span style="color:#a6accd">│   ├── deployment.yaml</span></span>
<span class="line"><span style="color:#a6accd">│   ├── _helpers.tpl</span></span>
<span class="line"><span style="color:#a6accd">│   ├── ingress.yaml</span></span>
<span class="line"><span style="color:#a6accd">│   ├── NOTES.txt</span></span>
<span class="line"><span style="color:#a6accd">│   └── service.yaml</span></span>
<span class="line"><span style="color:#a6accd">└── values.yaml   #模板赋值</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>2.编辑Chart配置</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# cd helm-nginx/</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 helm-nginx]# vim values.yaml</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>3.验证Chart配置，最后面的点表示当前目录</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 helm-nginx]# helm install --dry-run --debug --name helm-nginx .</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>4.安装自定义Chart，最后面的点表示当前目录</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 helm-nginx]# helm install --name helm-nginx .</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h3 id="查看helm实例" tabindex="-1">查看Helm实例 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#查看helm实例" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# helm list</span></span>
<span class="line"><span style="color:#a6accd">NAME          REVISION    UPDATED                     STATUS      CHART                 NAMESPACE</span></span>
<span class="line"><span style="color:#a6accd">helm-nginx    1           Sun Sep 16 19:32:19 2018    DEPLOYED    helm-nginx-0.1.0      default  </span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# kubectl get pod</span></span>
<span class="line"><span style="color:#a6accd">NAME                          READY     STATUS    RESTARTS   AGE</span></span>
<span class="line"><span style="color:#a6accd">helm-nginx-6975f8dbcd-htvtd   1/1       Running   0          51s</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# kubectl get ingress</span></span>
<span class="line"><span style="color:#a6accd">NAME         HOSTS                ADDRESS   PORTS     AGE</span></span>
<span class="line"><span style="color:#a6accd">helm-nginx   www.helm-nginx.com             80        1m</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h1 id="_17-应用的日志采集与分析" tabindex="-1">17 应用的日志采集与分析 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_17-应用的日志采集与分析" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h1><h2 id="prometheus快速入门" tabindex="-1">Prometheus快速入门 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#prometheus快速入门" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h2><h3 id="prometheus架构介绍" tabindex="-1">Prometheus架构介绍 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#prometheus架构介绍" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>在学习Prometheus之前我们需要清晰的掌握其架构，Prometheus是由多个组件组成的的监控系统，主要有：Prometheus Server、Alertmanager、Pushgateway组成，这三个组件均为独立的应用服务，独立部署和运行，其中Prometheus Server中内置了Prometheus web UI。</p><p><img src="http://k8s.unixhot.com/kubernetes/media/2ada1ece66fcc81d704c2ba46f9dd7d3.png" alt="architecture"></p><p><strong>Prometheus Server：</strong></p><p>Promethedus Server是核心组件，负责数据的获取、存储、查询。Prometheus通过Pull的方式定期的从Jobs/Exporters中获取数据，并保存在内置的TSDB中；内置的Prometheus web UI可以让用户通过PromQL的方式进行数据的检索。</p><p><strong>Exporters：</strong></p><p>Exporters也是一个独立的组件，有官方提供的Exporters也有社区贡献的Exportes，它将监控采集的数据通过HTTP的方式暴露给Prometheus Server，Server定期获取数据。例如有一个Exporters叫做Node Exporter，它安装在受采集的主机上，为Server提供数据，有点类似于Zabbix监控系统中的Zabbix Agent。</p><p><strong>Prometheus web UI：</strong></p><p>Prometheus web UI是Server启动后内置的一个Web界面，通过该Web界面我们可以进行数据查询工作，不包含设置的相关功能。</p><p><img src="http://k8s.unixhot.com/kubernetes/media/cc11e0cf02ff729fb905ac3648af18f7.png" alt="img"></p><p><strong>PromQL：</strong></p><p>PromQL是Prometheus内置的自定义的查询语言，提供对Prometheus Server中的TSDB这个时间序列数据库进行数据查询，支持数据聚合和一些逻辑运算，是一个相对简单的查询语言，而且PromQL也提供了一些内置函数，帮助我们进行数据处理。</p><p><strong>Alertmanager：</strong></p><p>Alertmanager是Promethedus的告警管理组件，它支持基于PromQL来创建告警规则，类似于Zabbix中的告警表达式，对获取到的数据进行计算和比较，如果满足PromQL定义的规则条件，就会产生报警。</p><p><strong>Pushgateway：</strong></p><p>Pushgateway可以理解为数据的一个中转站，例如当Prometheus Server不能直接和Exporters进行通信的场景下。</p><h3 id="安装prometheus" tabindex="-1">安装Prometheus <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#安装prometheus" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>学习Prometheus的第一步就是先部署一个实验环境，官方提供了多种方式进行Prometheus安装：</p><ul><li>源码编译安装</li><li>下载预编译好的二进制文件</li><li>使用Docker部署</li><li>使用第三方工具：Ansible、SaltStack、Puppet、Chef。</li></ul><p>为了方便学习，首先我们使用二进制方式部署，可以在这里<a target="_blank" rel="noreferrer" href="https://prometheus.io/download/%E4%B8%8B%E8%BD%BD%E5%AF%B9%E5%BA%94%E7%9A%84%E9%A2%84%E7%BC%96%E8%AF%91%E7%9A%84%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%87%E4%BB%B6%E3%80%82"><!--[-->https://prometheus.io/download/下载对应的预编译的二进制文件。<!--]--><!----></a></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# cd /usr/local/src</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 src]# wget</span></span>
<span class="line"><span style="color:#a6accd">https://github.com/prometheus/prometheus/releases/download/v2.7.1/prometheus-2.7.1.linux-amd64.tar.gz</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 src]# tar zxf prometheus-2.7.1.linux-amd64.tar.gz</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 src]# mv prometheus-2.7.1.linux-amd64 /usr/local/</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 src]# ln -s /usr/local/prometheus-2.7.1.linux-amd64/</span></span>
<span class="line"><span style="color:#a6accd">/usr/local/prometheus</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>Prometheus配置</strong></p><p>Prometheus的配置文件在prometheus.yml中，直接启动也会到命令的当前目录下寻找该文件。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# cd /usr/local/prometheus</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 prometheus]# vim prometheus.yml</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd"># my global config</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">global:</span></span>
<span class="line"><span style="color:#a6accd">scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.</span></span>
<span class="line"><span style="color:#a6accd">evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.</span></span>
<span class="line"><span style="color:#a6accd"># scrape_timeout is set to the global default (10s).</span></span>
<span class="line"><span style="color:#a6accd"># Alertmanager configuration</span></span>
<span class="line"><span style="color:#a6accd">alerting:</span></span>
<span class="line"><span style="color:#a6accd">alertmanagers:</span></span>
<span class="line"><span style="color:#a6accd">- static_configs:</span></span>
<span class="line"><span style="color:#a6accd">- targets:</span></span>
<span class="line"><span style="color:#a6accd"># - alertmanager:9093</span></span>
<span class="line"><span style="color:#a6accd"># Load rules once and periodically evaluate them according to the global</span></span>
<span class="line"><span style="color:#a6accd">'evaluation_interval'.</span></span>
<span class="line"><span style="color:#a6accd">rule_files:</span></span>
<span class="line"><span style="color:#a6accd"># - "first_rules.yml"</span></span>
<span class="line"><span style="color:#a6accd"># - "second_rules.yml"</span></span>
<span class="line"><span style="color:#a6accd"># A scrape configuration containing exactly one endpoint to scrape:</span></span>
<span class="line"><span style="color:#a6accd"># Here it's Prometheus itself.</span></span>
<span class="line"><span style="color:#a6accd">scrape_configs:</span></span>
<span class="line"><span style="color:#a6accd"># The job name is added as a label `job=&lt;job_name&gt;` to any timeseries</span></span>
<span class="line"><span style="color:#a6accd">scraped from this config.</span></span>
<span class="line"><span style="color:#a6accd">- job_name: 'prometheus'</span></span>
<span class="line"><span style="color:#a6accd"># metrics_path defaults to '/metrics'</span></span>
<span class="line"><span style="color:#a6accd"># scheme defaults to 'http'.</span></span>
<span class="line"><span style="color:#a6accd">static_configs:</span></span>
<span class="line"><span style="color:#a6accd">- targets: ['localhost:9090']</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>启动Prometheus</strong></p><p>默认情况下Prometheus会把数据写在启动目录的./data目录下，可以通过启动参数指定目录：--storage.tsdb.path="data/"，更多参数可以通过—help查看</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 prometheus]# ./prometheus –help</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 prometheus]# ./prometheus</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">…</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">level=info ts=2019-02-12T08:04:03.799169159Z caller=main.go:620 msg="Starting</span></span>
<span class="line"><span style="color:#a6accd">TSDB ..."</span></span>
<span class="line"><span style="color:#a6accd">level=info ts=2019-02-12T08:04:03.835497463Z caller=main.go:635 msg="TSDB</span></span>
<span class="line"><span style="color:#a6accd">started"</span></span>
<span class="line"><span style="color:#a6accd">level=info ts=2019-02-12T08:04:03.835598421Z caller=main.go:695 msg="Loading</span></span>
<span class="line"><span style="color:#a6accd">configuration file" filename=prometheus.yml</span></span>
<span class="line"><span style="color:#a6accd">level=info ts=2019-02-12T08:04:03.83756508Z caller=main.go:722 msg="Completed</span></span>
<span class="line"><span style="color:#a6accd">loading of configuration file" filename=prometheus.yml</span></span>
<span class="line"><span style="color:#a6accd">level=info ts=2019-02-12T08:04:03.83760078Z caller=main.go:589 msg="Server is</span></span>
<span class="line"><span style="color:#a6accd">ready to receive web requests."</span></span>
<span class="line"><span style="color:#a6accd">level=info ts=2019-02-12T08:04:03.837641772Z caller=web.go:416 component=web</span></span>
<span class="line"><span style="color:#a6accd">msg="Start listening for connections" address=0.0.0.0:9090</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>默认会在前台启动，并监听9090端口，会自动创建data目录，并存放数据。注意如何服务器时间不正确会有警告提示，请保证服务器时间同步。</p><p><img src="http://k8s.unixhot.com/kubernetes/media/266a101825cbabc2782820895e161f59.png" alt="img"></p><p><strong>放置在后台运行</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# groupadd prometheus</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# useradd -g prometheus -d /var/lib/prometheus -s</span></span>
<span class="line"><span style="color:#a6accd">/sbin/nologin prometheus</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# vim /usr/lib/systemd/system/prometheus.service</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">[Unit]</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">Description=prometheus</span></span>
<span class="line"><span style="color:#a6accd">After=network.target</span></span>
<span class="line"><span style="color:#a6accd">[Service]</span></span>
<span class="line"><span style="color:#a6accd">Type=simple</span></span>
<span class="line"><span style="color:#a6accd">User=prometheus</span></span>
<span class="line"><span style="color:#a6accd">ExecStart=/usr/local/prometheus/prometheus</span></span>
<span class="line"><span style="color:#a6accd">--config.file=/usr/local/prometheus/prometheus.yml</span></span>
<span class="line"><span style="color:#a6accd">--storage.tsdb.path=/var/lib/prometheus</span></span>
<span class="line"><span style="color:#a6accd">Restart=on-failure</span></span>
<span class="line"><span style="color:#a6accd">[Install]</span></span>
<span class="line"><span style="color:#a6accd">WantedBy=multi-user.target</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>后台启动</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# systemctl enable prometheus</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# systemctl start prometheus</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>查看启动状态</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# netstat -ntlp | grep 9090</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">tcp6 0 0 :::9090 :::* LISTEN 61333/prometheus</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h3 id="使用node-exporter采集主机数据" tabindex="-1">使用Node Exporter采集主机数据 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#使用node-exporter采集主机数据" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# cd /usr/local/src</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 src]# wget</span></span>
<span class="line"><span style="color:#a6accd">&lt;https://github.com/prometheus/node_exporter/releases/download/v0.17.0/node_exporter-0.17.0.linux-amd64.tar.gz&gt;</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 src]# tar zxf node_exporter-0.17.0.linux-amd64.tar.gz</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 src]# mv node_exporter-0.17.0.linux-amd64 /usr/local/</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 src]# ln -s /usr/local/node_exporter-0.17.0.linux-amd64/</span></span>
<span class="line"><span style="color:#a6accd">/usr/local/node_exporter</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# vim /usr/lib/systemd/system/node_exporter.service</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">[Unit]</span></span>
<span class="line"><span style="color:#a6accd">Description=node_exporter</span></span>
<span class="line"><span style="color:#a6accd">After=network.target</span></span>
<span class="line"><span style="color:#a6accd">[Service]</span></span>
<span class="line"><span style="color:#a6accd">Type=simple</span></span>
<span class="line"><span style="color:#a6accd">User=prometheus</span></span>
<span class="line"><span style="color:#a6accd">ExecStart=/usr/local/prometheus/node_exporter/node_exporter</span></span>
<span class="line"><span style="color:#a6accd">Restart=on-failure</span></span>
<span class="line"><span style="color:#a6accd">[Install]</span></span>
<span class="line"><span style="color:#a6accd">WantedBy=multi-user.target</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>启动Node Exporter</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# systemctl enable node_exporter</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# systemctl start node_exporter</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>查看状态 [root@linux-node1 ~]# netstat -ntlp | grep 9100</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">tcp6 0 0 :::9100 :::* LISTEN 66239/node_exporter</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>默认情况下Node Exporter监听9100端口，通过/metrics暴露采集到的监控数据，Prometheus默认也从该地址获取数据。</p><p><img src="http://k8s.unixhot.com/kubernetes/media/1004a69a33423c72a2989005be5a790e.png" alt="img"></p><p><strong>配置Prometheus读取Node Exporter数据</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# vim /usr/local/prometheus/prometheus.yml</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">scrape_configs:</span></span>
<span class="line"><span style="color:#a6accd"># The job name is added as a label `job=&lt;job_name&gt;` to any timeseries</span></span>
<span class="line"><span style="color:#a6accd">scraped from this config.</span></span>
<span class="line"><span style="color:#a6accd">- job_name: 'prometheus'</span></span>
<span class="line"><span style="color:#a6accd"># metrics_path defaults to '/metrics'</span></span>
<span class="line"><span style="color:#a6accd"># scheme defaults to 'http'.</span></span>
<span class="line"><span style="color:#a6accd">static_configs:</span></span>
<span class="line"><span style="color:#a6accd">- targets: ['localhost:9090']</span></span>
<span class="line"><span style="color:#a6accd">- job_name: 'linux-node1'</span></span>
<span class="line"><span style="color:#a6accd">static_configs:</span></span>
<span class="line"><span style="color:#a6accd">- targets: ['192.168.56.11:9100']</span></span>
<span class="line"><span style="color:#a6accd">labels:</span></span>
<span class="line"><span style="color:#a6accd">instance: linux-node1</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>重启prometheus</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# systemctl restart prometheus</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>查看监控状态</strong></p><p>登录Prometheus的Web控制台，StatusTargets如果可以linux-node1并且状态是UP的状态即为配置成功。</p><p><img src="http://k8s.unixhot.com/kubernetes/media/8c3da60dbf5558dc649de1fd6ce43bf0.png" alt="img"></p><h3 id="使用prometheus-ui查看数据" tabindex="-1">使用Prometheus UI查看数据 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#使用prometheus-ui查看数据" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>现在Prometheus会定期的从<a target="_blank" rel="noreferrer" href="http://192.168.56.11:9100/metrics%E8%8E%B7%E5%8F%96%E6%95%B0%E6%8D%AE%EF%BC%8C%E5%B9%B6%E5%AD%98%E5%82%A8%EF%BC%8C%E6%88%91%E4%BB%AC%E5%8F%AF%E4%BB%A5%E4%BD%BF%E7%94%A8Prometheus"><!--[-->http://192.168.56.11:9100/metrics获取数据，并存储，我们可以使用Prometheus<!--]--><!----></a> UI来查看监控数据。</p><h3 id="使用grafana进行数据可视化" tabindex="-1">使用Grafana进行数据可视化 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#使用grafana进行数据可视化" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p><strong>1.安装Grafana</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# vim /etc/yum.repos.d/grafana.repo</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">[grafana]</span></span>
<span class="line"><span style="color:#a6accd">name=grafana</span></span>
<span class="line"><span style="color:#a6accd">baseurl=https://packages.grafana.com/oss/rpm</span></span>
<span class="line"><span style="color:#a6accd">repo_gpgcheck=1</span></span>
<span class="line"><span style="color:#a6accd">enabled=1</span></span>
<span class="line"><span style="color:#a6accd">gpgcheck=1</span></span>
<span class="line"><span style="color:#a6accd">gpgkey=https://packages.grafana.com/gpg.key</span></span>
<span class="line"><span style="color:#a6accd">sslverify=1</span></span>
<span class="line"><span style="color:#a6accd">sslcacert=/etc/pki/tls/certs/ca-bundle.crt</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# yum install -y grafana</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>2.配置Grafana</strong></p><p>Grafana的配置文件在/etc/grafana/grafana.ini，默认情况下Grafana监听3000端口</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# vim /etc/grafana/grafana.ini</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>3.启动Grafana</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# systemctl enable grafana-server</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# systemctl start grafana-server</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# netstat -ntlp | grep 3000</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">tcp6 0 0 :::3000 :::* LISTEN 81427/grafana-serve</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>4.访问Grafana</strong></p><p>访问<a target="_blank" rel="noreferrer" href="http://192.168.56.11:3000/"><!--[-->http://192.168.56.11:3000<!--]--><!----></a>，用户名和密码默认为admin/admin，第一次登陆会要求修改密码，请使用安全密码。</p><p><strong>5.增加Prometheus数据源</strong></p><p>点击</p><p><img src="http://k8s.unixhot.com/kubernetes/media/b681a9b528d2ff21ba66666ce2452e51.png" alt="img"></p><p>，然后选择</p><p><img src="http://k8s.unixhot.com/kubernetes/media/d3e83ac4f090a51c5b5e0c341b99dda5.png" alt="img"></p><p>。</p><p>配置URL为：<a target="_blank" rel="noreferrer" href="http://192.168.56.11:9090/"><!--[-->http://192.168.56.11:9090<!--]--><!----></a>，并点击Save&amp;Test。</p><p><img src="http://k8s.unixhot.com/kubernetes/media/e35d8aaebedd7e168ebd1b29b65b30bb.png" alt="img"></p><p><strong>6.设置Dashboard</strong></p><p>数据源设置完毕后，就可以设置Dashboard图形展示，可以手动添加，也可以直接下载别人配置好保持的Json文件直接导入即可。</p><p>下载地址：<a target="_blank" rel="noreferrer" href="https://grafana.com/dashboards/405%EF%BC%8C%E5%9C%A8%E5%8F%B3%E4%BE%A7%E6%9C%89Download"><!--[-->https://grafana.com/dashboards/405，在右侧有Download<!--]--><!----></a> Json按钮，下载该Json文件。</p><p><img src="http://k8s.unixhot.com/kubernetes/media/d9ab69b29a964a12df52512a7b128b5b.png" alt="img"></p><p>点击Home下的Import Dashboard</p><p><img src="http://k8s.unixhot.com/kubernetes/media/53ffc0e739ca7b9421f9568ae4cbf117.png" alt="img"></p><p>然后直接上传刚才下载的JSON文件。</p><p><img src="http://k8s.unixhot.com/kubernetes/media/500958891a82067b0c987d514239ffb0.png" alt="img"></p><p>导入完毕后，就可以在Grafana上查看对应节点的监控数据图表。你可以通过鼠标拖拽进行图表的自定义大小和位置的修改，效果如下：</p><p><img src="http://k8s.unixhot.com/kubernetes/media/698a6241faa0adc1af6c09cc369b259b.png" alt="img"></p><h1 id="第七部分-kubernetes高级进阶" tabindex="-1">第七部分 Kubernetes高级进阶 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#第七部分-kubernetes高级进阶" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h1><h1 id="kubernetes的权限控制rbac" tabindex="-1">Kubernetes的权限控制RBAC <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#kubernetes的权限控制rbac" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h1><p><strong>角色</strong></p><ul><li><p>Role: 角色，命名空间范围内的一个权限集合。</p></li><li><p>ClusterRole：集群角色，集群范围内的一个权限的集合，</p><p>Role和ClusterROle在Kubernetes中都被定义为集群内部的 API 资源，和我们前面学习过的 Pod、ConfigMap 这些类似，都是我们集群的资源对象，所以同样的可以使用我们前面的kubectl相关的命令来进行操作 Subject：主题，对应在集群中尝试操作的对象，集群中定义了3种类型的主题资源：</p></li></ul><p>User Account：用户，这是有外部独立服务进行管理的，管理员进行私钥的分配，用户可以使用 KeyStone或者 Goolge 帐号，甚至一个用户名和密码的文件列表也可以。对于用户的管理集群内部没有一个关联的资源对象，所以用户不能通过集群内部的 API 来进行管理 Group：组，这是用来关联多个账户的，集群中有一些默认创建的组，比如cluster-admin Service Account：服务帐号，通过Kubernetes API 来管理的一些用户帐号，和 namespace 进行关联的，适用于集群内部运行的应用程序，需要通过 API 来完成权限认证，所以在集群内部进行权限操作，我们都需要使用到 ServiceAccount，这也是我们这节课的重点 RoleBinding 和 ClusterRoleBinding：角色绑定和集群角色绑定，简单来说就是把声明的 Subject 和我们的 Role 进行绑定的过程(给某个用户绑定上操作的权限)，二者的区别也是作用范围的区别：RoleBinding 只会影响到当前 namespace 下面的资源操作权限，而 ClusterRoleBinding 会影响到所有的 namespace。</p><p><strong>创建用户凭证</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# openssl genrsa -out jenkins.key 2048</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# openssl req -new -key jenkins.key -out jenkins.csr -subj "/CN=jenkins/O=vmware"</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# openssl x509 -req -in jenkins.csr -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key -CAcreateserial -out jenkins.crt -days 365</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# kubectl config set-credentials jenkins --client-certificate=jenkins.crt  --client-key=jenkins.key</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# kubectl config set-context jenkins-context --cluster=kubernetes --namespace=jenkins --user=jenkins  </span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# kubectl get pods --context=jenkins-context</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>创建角色</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# vim jenkins-role.yml</span></span>
<span class="line"><span style="color:#a6accd">apiVersion: rbac.authorization.k8s.io/v1</span></span>
<span class="line"><span style="color:#a6accd">kind: Role</span></span>
<span class="line"><span style="color:#a6accd">metadata:</span></span>
<span class="line"><span style="color:#a6accd">  name: jenkins-role</span></span>
<span class="line"><span style="color:#a6accd">  namespace: jenkins</span></span>
<span class="line"><span style="color:#a6accd">rules:</span></span>
<span class="line"><span style="color:#a6accd">- apiGroups: ["", "extensions", "apps"]</span></span>
<span class="line"><span style="color:#a6accd">  resources: ["deployments", "replicasets", "pods"]</span></span>
<span class="line"><span style="color:#a6accd">  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>创建角色绑定</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# vim jenkins-role-binding.yml</span></span>
<span class="line"><span style="color:#a6accd">apiVersion: rbac.authorization.k8s.io/v1</span></span>
<span class="line"><span style="color:#a6accd">kind: RoleBinding</span></span>
<span class="line"><span style="color:#a6accd">metadata:</span></span>
<span class="line"><span style="color:#a6accd">  name: jenkins-rolebinding</span></span>
<span class="line"><span style="color:#a6accd">  namespace: jenkins</span></span>
<span class="line"><span style="color:#a6accd">subjects:</span></span>
<span class="line"><span style="color:#a6accd">- kind: User</span></span>
<span class="line"><span style="color:#a6accd">  name: jenkins</span></span>
<span class="line"><span style="color:#a6accd">  apiGroup: ""</span></span>
<span class="line"><span style="color:#a6accd">roleRef:</span></span>
<span class="line"><span style="color:#a6accd">  kind: Role</span></span>
<span class="line"><span style="color:#a6accd">  name: jenkins-role</span></span>
<span class="line"><span style="color:#a6accd">  apiGroup: ""</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h1 id="_22-深入理解pod调度" tabindex="-1">22 深入理解Pod调度 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_22-深入理解pod调度" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h1><h1 id="深入理解pod调度" tabindex="-1">深入理解Pod调度 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#深入理解pod调度" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h1><p>在前面的章节我们已经知道在Kubernetes中使用kube-scheduler进行Pod调度，它的目标是将Pod绑定到对应的Node上，经过一系列的条件和算法尽可能的让每个Pod都满意。kube-scheduler是Kubernetes默认的调度器。</p><p>kube-scheduler的代码位于<a target="_blank" rel="noreferrer" href="https://github.com/kubernetes/kubernetes/tree/master/pkg/scheduler"><!--[-->GitHub<!--]--><!----></a></p><p>可以将代码克隆到本地方便查看</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# git clone --depth 1 https://github.com/kubernetes/kubernetes.git</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>在algorithm下有调度算法，调度算法分为两个阶段：Predicates和priorities，首先对Node进行过滤看哪些Node符合调度要求，然后在符合调度要求的Node上进行优先级计算，判断调度到哪个Node最合适。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 algorithm]# pwd</span></span>
<span class="line"><span style="color:#a6accd">/root/kubernetes/pkg/scheduler/algorithm</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 algorithm]# ls -l</span></span>
<span class="line"><span style="color:#a6accd">total 20</span></span>
<span class="line"><span style="color:#a6accd">-rw-r--r-- 1 root root 1256 Dec 17 22:52 BUILD</span></span>
<span class="line"><span style="color:#a6accd">-rw-r--r-- 1 root root  735 Dec 17 22:52 doc.go</span></span>
<span class="line"><span style="color:#a6accd">drwxr-xr-x 2 root root  276 Dec 17 22:52 predicates</span></span>
<span class="line"><span style="color:#a6accd">drwxr-xr-x 3 root root 4096 Dec 17 22:52 priorities</span></span>
<span class="line"><span style="color:#a6accd">-rw-r--r-- 1 root root 3278 Dec 17 22:52 scheduler_interface.go</span></span>
<span class="line"><span style="color:#a6accd">-rw-r--r-- 1 root root 3383 Dec 17 22:52 types.go</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>官方文档详细的介绍了所有的步骤：<a target="_blank" rel="noreferrer" href="https://kubernetes.io/docs/concepts/scheduling/kube-scheduler/"><!--[-->https://kubernetes.io/docs/concepts/scheduling/kube-scheduler/<!--]--><!----></a></p><p><strong>设置调度器</strong></p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# kubectl get pod kube-proxy-5wbtf -n kube-system -o yaml | grep schedulerName</span></span>
<span class="line"><span style="color:#a6accd">  schedulerName: default-scheduler</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h2 id="taints（污点）" tabindex="-1">Taints（污点） <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#taints（污点）" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h2><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# kubectl describe node linux-node1.unixhot.com | grep Taints</span></span>
<span class="line"><span style="color:#a6accd">Taints:             node-role.kubernetes.io/master:NoSchedule</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>Taints的表现形式为</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">&lt;key&gt;=&lt;value&gt;:&lt;effect&gt;</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>effect的三种类型：</p><ul><li>NoSchedule: 如果Pod没有容忍该污点，不调度到该节点上。</li><li>PreferNoSchedule：尽量阻止Pod被调度到这个节点上，但是如果没有其它节点能够调度，可以调度到该节点。</li><li>NoExecute： NoScheduler和PreferNoSchedule只是在调度阶段起作用，但是NoExecute会影响正常运行的Pod，如果一个节点被打了NoExecute的污点，而运行在该节点的Pod没有容忍会直接被这个节点移除。</li></ul><p>查看Flannel为何能调度到Master节点</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# kubectl get po -n kube-system | grep flannel</span></span>
<span class="line"><span style="color:#a6accd">kube-flannel-ds-amd64-f2jrk                       1/1     Running   2          22h</span></span>
<span class="line"><span style="color:#a6accd">kube-flannel-ds-amd64-mh75v                       1/1     Running   2          22h</span></span>
<span class="line"><span style="color:#a6accd">kube-flannel-ds-amd64-n52zm                       1/1     Running   4          22h</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# kubectl describe pod kube-flannel-ds-amd64-f2jrk -n kube-system</span></span>
<span class="line"><span style="color:#a6accd">...</span></span>
<span class="line"><span style="color:#a6accd">Tolerations:     :NoSchedule</span></span>
<span class="line"><span style="color:#a6accd">                 node.kubernetes.io/disk-pressure:NoSchedule</span></span>
<span class="line"><span style="color:#a6accd">                 node.kubernetes.io/memory-pressure:NoSchedule</span></span>
<span class="line"><span style="color:#a6accd">                 node.kubernetes.io/network-unavailable:NoSchedule</span></span>
<span class="line"><span style="color:#a6accd">                 node.kubernetes.io/not-ready:NoExecute</span></span>
<span class="line"><span style="color:#a6accd">                 node.kubernetes.io/pid-pressure:NoSchedule</span></span>
<span class="line"><span style="color:#a6accd">                 node.kubernetes.io/unreachable:NoExecute</span></span>
<span class="line"><span style="color:#a6accd">                 node.kubernetes.io/unschedulable:NoSchedule</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h3 id="自定义污点" tabindex="-1">自定义污点 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#自定义污点" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@linux-node1 ~]# kubectl taint node linux-node2.example.com node-ytpe=gpu:NoSchedule       </span></span>
<span class="line"><span style="color:#a6accd">node/linux-node2.example.com tainted</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 example]# cat nginx-deployment-taint.yaml    </span></span>
<span class="line"><span style="color:#a6accd">apiVersion: apps/v1</span></span>
<span class="line"><span style="color:#a6accd">kind: Deployment</span></span>
<span class="line"><span style="color:#a6accd">metadata:</span></span>
<span class="line"><span style="color:#a6accd">  name: nginx-deployment</span></span>
<span class="line"><span style="color:#a6accd">  labels:</span></span>
<span class="line"><span style="color:#a6accd">    app: nginx</span></span>
<span class="line"><span style="color:#a6accd">spec:</span></span>
<span class="line"><span style="color:#a6accd">  replicas: 3</span></span>
<span class="line"><span style="color:#a6accd">  selector:</span></span>
<span class="line"><span style="color:#a6accd">    matchLabels:</span></span>
<span class="line"><span style="color:#a6accd">      app: nginx</span></span>
<span class="line"><span style="color:#a6accd">  template:</span></span>
<span class="line"><span style="color:#a6accd">    metadata:</span></span>
<span class="line"><span style="color:#a6accd">      labels:</span></span>
<span class="line"><span style="color:#a6accd">        app: nginx</span></span>
<span class="line"><span style="color:#a6accd">    spec:</span></span>
<span class="line"><span style="color:#a6accd">      containers:</span></span>
<span class="line"><span style="color:#a6accd">      - name: nginx</span></span>
<span class="line"><span style="color:#a6accd">        image: nginx:1.13.12</span></span>
<span class="line"><span style="color:#a6accd">        ports:</span></span>
<span class="line"><span style="color:#a6accd">        - containerPort: 80</span></span>
<span class="line"><span style="color:#a6accd">      tolerations:</span></span>
<span class="line"><span style="color:#a6accd">      - key: node-type</span></span>
<span class="line"><span style="color:#a6accd">        operator: Equal</span></span>
<span class="line"><span style="color:#a6accd">        value: gpu</span></span>
<span class="line"><span style="color:#a6accd">        effect: Noschedule</span></span>
<span class="line"><span style="color:#a6accd">[root@linux-node1 example]# kubectl get po -o wide</span></span>
<span class="line"><span style="color:#a6accd">NAME                                READY   STATUS    RESTARTS   AGE     IP          NODE                      NOMINATED NODE   READINESS GATES</span></span>
<span class="line"><span style="color:#a6accd">dns-test                            1/1     Running   1          6h15m   10.2.2.23   linux-node3.example.com   &lt;none&gt;           &lt;none&gt;</span></span>
<span class="line"><span style="color:#a6accd">nginx-deployment-77564d4546-2jkw9   1/1     Running   0          13s     10.2.2.26   linux-node3.example.com   &lt;none&gt;           &lt;none&gt;</span></span>
<span class="line"><span style="color:#a6accd">nginx-deployment-77564d4546-4hrbf   1/1     Running   0          13s     10.2.2.24   linux-node3.example.com   &lt;none&gt;           &lt;none&gt;</span></span>
<span class="line"><span style="color:#a6accd">nginx-deployment-77564d4546-s2r4h   1/1     Running   0          13s     10.2.2.25   linux-node3.example</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h2 id="亲缘性调度" tabindex="-1">亲缘性调度 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#亲缘性调度" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h2><h1 id="_23-kubernetes-api介绍" tabindex="-1">23 Kubernetes API介绍 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#_23-kubernetes-api介绍" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h1><p>查看集群状态</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@k8s-master1 ~]# kubectl cluster-info</span></span>
<span class="line"><span style="color:#a6accd">Kubernetes master is running at https://192.168.56.11:6443</span></span>
<span class="line"><span style="color:#a6accd">KubeDNS is running at https://192.168.56.11:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>直接访问Kubernetes API需要验证，无法直接访问。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@k8s-master1 ~]# curl -k https://192.168.56.11:6443</span></span>
<span class="line"><span style="color:#a6accd">{</span></span>
<span class="line"><span style="color:#a6accd">  "kind": "Status",</span></span>
<span class="line"><span style="color:#a6accd">  "apiVersion": "v1",</span></span>
<span class="line"><span style="color:#a6accd">  "metadata": {</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">  },</span></span>
<span class="line"><span style="color:#a6accd">  "status": "Failure",</span></span>
<span class="line"><span style="color:#a6accd">  "message": "forbidden: User \"system:anonymous\" cannot get path \"/\"",</span></span>
<span class="line"><span style="color:#a6accd">  "reason": "Forbidden",</span></span>
<span class="line"><span style="color:#a6accd">  "details": {</span></span>
<span class="line"><span style="color:#a6accd"></span></span>
<span class="line"><span style="color:#a6accd">  },</span></span>
<span class="line"><span style="color:#a6accd">  "code": 403</span></span>
<span class="line"><span style="color:#a6accd">}</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p><strong>通过Proxy访问Kubernetes API</strong></p><p>使用kubectl proxy可以在Master本地启动一个代理</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@k8s-master1 ~]# kubectl proxy</span></span>
<span class="line"><span style="color:#a6accd">Starting to serve on 127.0.0.1:8001</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>可以通过127.0.0.1:8001与API Server进行交互</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@k8s-master1 ~]# curl http://127.0.0.1:8001</span></span>
<span class="line"><span style="color:#a6accd">{</span></span>
<span class="line"><span style="color:#a6accd">  "paths": [</span></span>
<span class="line"><span style="color:#a6accd">    "/api",</span></span>
<span class="line"><span style="color:#a6accd">    "/api/v1",</span></span>
<span class="line"><span style="color:#a6accd">    "/apis",</span></span>
<span class="line"><span style="color:#a6accd">    "/apis/",</span></span>
<span class="line"><span style="color:#a6accd">    "/apis/admissionregistration.k8s.io",</span></span>
<span class="line"><span style="color:#a6accd">    "/apis/admissionregistration.k8s.io/v1beta1",</span></span>
<span class="line"><span style="color:#a6accd">    "/apis/apiextensions.k8s.io",</span></span>
<span class="line"><span style="color:#a6accd">    "/apis/apiextensions.k8s.io/v1beta1",</span></span>
<span class="line"><span style="color:#a6accd">    "/apis/apiregistration.k8s.io",</span></span>
<span class="line"><span style="color:#a6accd">    "/apis/apiregistration.k8s.io/v1",</span></span>
<span class="line"><span style="color:#a6accd">    "/apis/apiregistration.k8s.io/v1beta1",</span></span>
<span class="line"><span style="color:#a6accd">    "/apis/apps",</span></span>
<span class="line"><span style="color:#a6accd">...（省略其它输出）</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>可以通过修改监听地址，并关闭过滤，实现在其它地方登录和查看，这样就可以在本地浏览器访问API。切记不要再生产环境将代理地址暴露在外网。</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@k8s-master1 ~]# kubectl proxy --address=0.0.0.0 --disable-filter=true</span></span>
<span class="line"><span style="color:#a6accd">W1105 16:18:45.669591   16730 proxy.go:142] Request filter disabled, your proxy is vulnerable to XSRF attacks, please be cautious</span></span>
<span class="line"><span style="color:#a6accd">Starting to serve on [::]:8001</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><h3 id="使用swagger-ui进行api交互" tabindex="-1">使用Swagger UI进行API交互 <a aria-current="page" href="/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97#使用swagger-ui进行api交互" class="router-link-active router-link-exact-active header-anchor" aria-hidden="true"><!--[-->#<!--]--></a></h3><p>Kubernetes支持Swagger UI访问API，需要在API Server开启，如果已经根据本书使用kubeadm部署的集群，可以通过修改Pod的YAML文件，重建Pod来开启</p><p><strong>修改API Server的Pod定义文件</strong></p><p>在- kube-apiserver下面一行增加--enable-swagger-ui=true</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@k8s-master1 ~]# vim /etc/kubernetes/manifests/kube-apiserver.yaml </span></span>
<span class="line"><span style="color:#a6accd">apiVersion: v1</span></span>
<span class="line"><span style="color:#a6accd">kind: Pod</span></span>
<span class="line"><span style="color:#a6accd">metadata:</span></span>
<span class="line"><span style="color:#a6accd">  creationTimestamp: null</span></span>
<span class="line"><span style="color:#a6accd">  labels:</span></span>
<span class="line"><span style="color:#a6accd">    component: kube-apiserver</span></span>
<span class="line"><span style="color:#a6accd">    tier: control-plane</span></span>
<span class="line"><span style="color:#a6accd">  name: kube-apiserver</span></span>
<span class="line"><span style="color:#a6accd">  namespace: kube-system</span></span>
<span class="line"><span style="color:#a6accd">spec:</span></span>
<span class="line"><span style="color:#a6accd">  containers:</span></span>
<span class="line"><span style="color:#a6accd">  - command:</span></span>
<span class="line"><span style="color:#a6accd">    - kube-apiserver</span></span>
<span class="line"><span style="color:#a6accd">    - --enable-swagger-ui=true</span></span>
<span class="line"><span style="color:#a6accd">    - --advertise-address=192.168.56.11</span></span>
<span class="line"><span style="color:#a6accd">    - --allow-privileged=true</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>删除Pod，kubelet会通过该YAML重建Pod</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@k8s-master1 ~]# kubectl get pod -n kube-system | grep api</span></span>
<span class="line"><span style="color:#a6accd">kube-apiserver-linux-node1.unixhot.com            1/1     Running   0          55m</span></span>
<span class="line"><span style="color:#a6accd">[root@k8s-master1 ~]# kubectl delete pod kube-apiserver-linux-node1.unixhot.com -n kube-system</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>可以看到配置已经生效</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@k8s-master1 ~]# kubectl describe pod kube-apiserver-linux-node1.unixhot.com -n kube-system </span></span>
<span class="line"><span style="color:#a6accd">...</span></span>
<span class="line"><span style="color:#a6accd">    Command:</span></span>
<span class="line"><span style="color:#a6accd">      kube-apiserver</span></span>
<span class="line"><span style="color:#a6accd">      --enable-swagger-ui=true</span></span>
<span class="line"><span style="color:#a6accd">      --advertise-address=192.168.99.27</span></span>
<span class="line"><span style="color:#a6accd">....</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>部署一个Swagger UI服务查看API</p><div class="language-"><span class="copy"></span><pre class="shiki material-theme-palenight" tabindex="0"><code><span class="line"><span style="color:#a6accd">[root@k8s-master1 ~]# kubectl run swagger-ui --image=swaggerapi/swagger-ui:latest</span></span>
<span class="line"><span style="color:#a6accd">[root@k8s-master1 ~]# kubectl expose deployment swagger-ui --port=8080 --type=NodePort</span></span>
<span class="line"><span style="color:#a6accd">[root@k8s-master1 ~]# kubectl get service</span></span>
<span class="line"><span style="color:#a6accd">NAME         TYPE        CLUSTER-IP    EXTERNAL-IP     PORT(S)          AGE</span></span>
<span class="line"><span style="color:#a6accd">kubernetes   ClusterIP   10.1.0.1      &lt;none&gt;          443/TCP          43d</span></span>
<span class="line"><span style="color:#a6accd">swagger-ui   NodePort    10.1.205.94   &lt;none&gt;   8080:30410/TCP   34s</span></span>
<span class="line"><span style="color:#a6accd"></span></span></code></pre></div><p>因为我们部署的Swagger UI和API Server不在一个域名下，所以会有跨域的问题，Chrome浏览器需要提前安装Allow CROS插件解决</p><hr><hr><p>摘录自赵班长--------</p><!--]--><!--[--><!--]--><!--]--><div text="center"><!----></div><!----></article><!--]--><!--[--><!--[--><!--[--><div class="yun-sponsor-container flex-center flex-col" m="t-6"><button class="sponsor-button yun-icon-btn shadow hover:shadow-md" title="我很可爱，请给我钱！" text="red-400"><div i-ri-heart-line=""></div></button><div m="y-4" class="qrcode-container qrcode flex-center flex-col"><div class="sponsor-description" mb="4" text="sm">这是关于赞助的一些描述</div><div class="flex justify-around"><!--[--><a class="flex-center flex-col animate-iteration-1 animate-fade-in" href="https://cos.vlinux.cn/www-vlinux-cn-blog-img/WechatIMG203.jpeg" target="_blank" style="color:#00a3ee"><img class="sponsor-method-img" border="~ rounded" p="1" loading="lazy" src="https://cos.vlinux.cn/www-vlinux-cn-blog-img/WechatIMG203.jpeg" title="支付宝"><div text="xl" m="2" class="i-ri-alipay-line"></div></a><a class="flex-center flex-col animate-iteration-1 animate-fade-in" href="https://cos.vlinux.cn/www-vlinux-cn-blog-img/WechatIMG205.jpeg" target="_blank" style="color:#12b7f5"><img class="sponsor-method-img" border="~ rounded" p="1" loading="lazy" src="https://cos.vlinux.cn/www-vlinux-cn-blog-img/WechatIMG205.jpeg" title="QQ 支付"><div text="xl" m="2" class="i-ri-qq-line"></div></a><a class="flex-center flex-col animate-iteration-1 animate-fade-in" href="https://cos.vlinux.cn/www-vlinux-cn-blog-img/WechatIMG204.jpeg" target="_blank" style="color:#2dc100"><img class="sponsor-method-img" border="~ rounded" p="1" loading="lazy" src="https://cos.vlinux.cn/www-vlinux-cn-blog-img/WechatIMG204.jpeg" title="微信支付"><div text="xl" m="2" class="i-ri-wechat-pay-line"></div></a><!--]--></div></div></div><ul class="post-copyright" m="y-4"><li class="post-copyright-author"><strong>本文作者：</strong><span>卷饼</span></li><li class="post-copyright-link"><strong>本文链接：</strong><a href="https://www.vlinux.cn/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97">https://www.vlinux.cn/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97</a></li><li class="post-copyright-license"><strong>版权声明：</strong><span>本博客所有文章除特别声明外，均默认采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0 ">CC BY-NC-SA</a> 许可协议。</span></li></ul><!--]--><!--]--><!--]--></div><!--]--><!----></div><!--[--><!--]--><!--[--><div class="post-nav"><div class="post-nav-item"><a href="/posts/k8s(kubernetes)%E5%AE%9E%E8%B7%B5%E8%AE%A4%E7%9F%A5" class="post-nav-prev" title="K8S(kubernetes)实践认知"><div class="icon" i-ri-arrow-left-s-line=""></div><span class="title truncate" text="sm">K8S(kubernetes)实践认知</span></a></div><div class="post-nav-item"><a href="/posts/linux%E7%8E%AF%E5%A2%83%E5%88%9D%E5%A7%8B%E5%8C%96" class="post-nav-next" title="Linux环境初始化"><span class="title truncate" text="sm">Linux环境初始化</span><div class="icon" i-ri-arrow-right-s-line=""></div></a></div></div><!--]--><!--[--><!--]--><!--[--><div class="yun-card comment sm:p-6 lg:px-12 xl:px-16" w="full" p="4"><!----><!----><!--[--><!----><!--]--><!----></div><!--]--><!--[--><!--]--><footer class="va-footer p-4 text-$va-c-text-light" text="center sm"><div class="beian" m="y-2"><a href="https://beian.miit.gov.cn/" target="_blank" rel="noopener">豫ICP备20001100号-3</a></div><div class="copyright flex justify-center items-center gap-2" p="1"><span>©<!--[--> 2016 -<!--]--> 2023</span><a class="animate-pulse inline-flex" href="https://sponsors.yunyoujun.cn" target="_blank" title="Sponsor YunYouJun"><div class="i-ri-cloud-line"></div></a><span>卷饼</span></div><div class="powered" m="2"><span>由 <a href="https://github.com/YunYouJun/valaxy" target="_blank" rel="noopener">Valaxy</a> v0.14.28 驱动</span> | <span>主题 - <a href="https://github.com/YunYouJun/valaxy/tree/main/packages/valaxy-theme-yun" title="valaxy-theme-yun" target="_blank">Yun</a> v0.14.28</span></div><!--[--> 由 <a href="https://www.netdun.net" target="_blank" title="网盾星球">网盾星球</a> 提供 CDN 支持<!--]--></footer><!--[--><!--]--></div><!--]--><!--[--><!--[--><button class="xl:hidden toc-btn shadow fixed yun-icon-btn z-350" opacity="75" right="2" bottom="19"><div i-ri-file-list-line=""></div></button><!----><!--  --><aside class="va-card yun-aside" m="l-4" text="center"><div class="aside-container" flex="~ col" overflow="auto"><h2 m="t-6 b-2" font="serif black">文章目录</h2><div style="display:none" data-v-e1350763=""><div class="content" data-v-e1350763=""><div class="outline-title" data-v-e1350763="">On this page</div><div class="outline-marker" data-v-e1350763=""></div><nav aria-labelledby="doc-outline-aria-label" data-v-e1350763=""><span id="doc-outline-aria-label" class="visually-hidden" data-v-e1350763="">Table of Contents for current page</span><ul class="root va-toc relative z-1" data-v-e1350763="" data-v-e736e6e6=""><!--[--><!--]--></ul></nav></div></div><div class="flex-grow"></div><div class="custom-container"><!--[--><!--]--></div></div></aside><!--]--><!--]--></div></main><a href="#" class="back-to-top yun-icon-btn"><div w="8" h="8" i-ri-arrow-up-s-line=""></div><svg class="progress-circle-container" viewBox="0 0 100 100"><circle stroke-dasharray="301.59289474462014 301.59289474462014" stroke-dashoffset="301.59289474462014" class="progress-circle" cx="50" cy="50" r="48" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"></circle></svg></a><!--]--><!--]--></div><script>window.__INITIAL_STATE__='{"pinia":{"app":{"isSidebarOpen":false,"isRightSidebarOpen":false},"site":{}}}'</script><script type="application/ld+json" id="schema-org-graph" data-h-3437552="">{
  "@context": "https://schema.org",
  "@graph": [
    {
      "@id": "https://www.vlinux.cn/#identity",
      "@type": "Person",
      "name": "卷饼",
      "url": "https://www.vlinux.cn/",
      "image": {
        "@id": "https://www.vlinux.cn/#/schema/image/b8ad69b"
      },
      "sameAs": [
        "https://wpa.qq.com/msgrd?v=3&uin=38867033&site=qq&menu=yes&jumpflag=1",
        "https://github.com/vlinux",
        "https://repo.vlinux.cn/",
        "https://cos.vlinux.cn/www-vlinux-cn-blog-img/WechatIMG18.jpeg",
        "https://www.vlinux.cn/music/",
        "mailto:ilinux@88.com",
        "https://www.vlinux.cn/play"
      ]
    },
    {
      "@id": "https://www.vlinux.cn/#website",
      "@type": "WebSite",
      "dateModified": "2022-12-15T15:45:15+08:00",
      "datePublished": "2020-02-28T02:26:00+08:00",
      "inLanguage": "zh-CN",
      "name": "Docker+K8s实践指南",
      "url": "https://www.vlinux.cn/",
      "publisher": {
        "@id": "https://www.vlinux.cn/#identity"
      }
    },
    {
      "@id": "https://www.vlinux.cn/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97/#webpage",
      "@type": "WebPage",
      "description": "那你终将成为别人的一条裤衩",
      "name": "Docker+K8s实践指南",
      "url": "https://www.vlinux.cn/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97",
      "about": {
        "@id": "https://www.vlinux.cn/#identity"
      },
      "isPartOf": {
        "@id": "https://www.vlinux.cn/#website"
      },
      "potentialAction": [
        {
          "@type": "ReadAction",
          "target": [
            "https://www.vlinux.cn/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97"
          ]
        }
      ]
    },
    {
      "@id": "https://www.vlinux.cn/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97/#article",
      "description": "那你终将成为别人的一条裤衩",
      "headline": "Docker+K8s实践指南",
      "inLanguage": "zh-CN",
      "thumbnailUrl": "https://www.vlinux.cn/favicon.svg",
      "@type": [
        "Article",
        "BlogPosting"
      ],
      "author": {
        "@id": "https://www.vlinux.cn/#/schema/person/e914eae"
      },
      "image": {
        "@id": "https://www.vlinux.cn/#/schema/image/a0d155d"
      },
      "isPartOf": {
        "@id": "https://www.vlinux.cn/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97/#webpage"
      },
      "mainEntityOfPage": {
        "@id": "https://www.vlinux.cn/posts/docker+k8s%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97/#webpage"
      },
      "publisher": {
        "@id": "https://www.vlinux.cn/#identity"
      }
    },
    {
      "@id": "https://www.vlinux.cn/#/schema/person/e914eae",
      "@type": "Person",
      "name": "卷饼",
      "url": "https://valaxy.site"
    },
    {
      "@id": "https://www.vlinux.cn/#/schema/image/b8ad69b",
      "@type": "ImageObject",
      "contentUrl": "https://cos.vlinux.cn/vlinux-logo/user.jpg",
      "inLanguage": "zh-CN",
      "url": "https://cos.vlinux.cn/vlinux-logo/user.jpg"
    },
    {
      "@id": "https://www.vlinux.cn/#/schema/image/a0d155d",
      "@type": "ImageObject",
      "contentUrl": "https://www.vlinux.cn/favicon.svg",
      "inLanguage": "zh-CN",
      "url": "https://www.vlinux.cn/favicon.svg"
    }
  ]
}</script><link rel="stylesheet" href="/assets/ValaxyMain-3beb7542.css"><link rel="stylesheet" href="/assets/post-b195884f.css"><link rel="stylesheet" href="/assets/index-c2c57308.css"></body></html>